{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Intel\u00ae Security Libraries for Data Center (Intel\u00ae SecL-DC) Overview (Heading change for version selector change) Intel\u00ae Security Libraries for Data Center (Intel\u00ae SecL-DC) enables security use cases for data center using Intel\u00ae hardware security technologies. Hardware-based cloud security solutions provide a higher level of protection as compared to software-only security measures. There are many Intel platform security technologies, which can be used to secure customers' data. Customers have found adopting and deploying these technologies at a broad scale challenging, due to the lack of solution integration and deployment tools. Intel\u00ae Security Libraries for Data Centers (Intel\u00ae SecL - DC) was built to aid our customers in adopting and deploying Intel Security features, rooted in silicon, at scale. Intel\u00ae SecL-DC is an open-source remote attestation implementation comprising a set of building blocks that utilize Intel Security features to discover, attest, and enable critical foundation security and confidential computing use-cases. It applies the remote attestation fundamentals and standard specifications to maintain a platform data collection service, and an efficient verification engine to perform comprehensive trust evaluations. These trust evaluations can be used to govern different trust and security policies applied to any given workload. For more details please visit : https://01.org/intel-secl Architecture The below diagram depicts the high level architecture of the Intel\u00aeSecL-DC, Use Cases Foundational Security & Launch Time Protection Foundational and Workload Security refers to a collection of software security solutions provided by Intel SecL-DC that leverage Intel silicon to provide boot-time integrity attestation of platform components. Starting with a Hardware Root of Trust, a chain of measurements of system components that includes the system BIOS/UEFI and OS kernel is extended to a Trusted Platform Module for remote attestation against expected measurements. Use cases include auditing the integrity of Cloud platforms, Asset or Geolocation Tagging, Platform Integrity-aware Cloud orchestration, and VM and container encryption. This acts as a firm, hardware-rooted foundation upon which to build a Cloud platform with auditable integrity verification. Foundational and Workload Security Product Guide Foundational & Workload Security Quick Start Guide Foundational & Workload Security Swagger Documents SGX Attestation Infrastructure The SGX Attestation infrastructure provides an end to end support for registering SGX hosts and provisioning them with SGX material (PCK certificates) and SGX collateral (security patches information - TCB Information - and Certificate Revocation Lists - CRLs). The SGX Attestation infrastructure also provides support for generating SGX quotes for SGX enclaves hosted by workloads and verifying them by a remote attesting application. The remote attesting application can also use the SGX Attestation infrastructure to enforce enclave policies (like requiring a specific enclave signer). Optionally, the SGX Attestation Infrastructure allows to integrate with Cloud Orchestrators like Openstack and Kubernetes. The SGX Attestation infrastructure does not make any assumption on the user SGX workload and enclave policy. SGX Attestation Infrastructure and Secure Key Caching Product Guide SGX Attestation Infrastructure and Secure Key Caching Quick Start Guide SGX Attestation Infrastructure and Secure Key Caching Swagger Documents As a demonstration of SGX Attestation infrastructure artifacts, a sample SGX attestation app is available here. Secure Key Caching Secure Key Caching (SKC) leverages the SGX Attestation Infrastructure to support the Secure Key Caching (SKC) use case. SKC provides key protection at rest and in-use using Intel Software Guard Extensions (SGX). SGX implements the Trusted Execution Environment (TEE) paradigm. Using the SKC Client -- a set of libraries -- applications can retrieve keys from the Intel SecL-DC Key Broker Service (KBS) and load them to an SGX-protected memory (called SGX enclave) in the application memory space. KBS performs the SGX enclave attestation to ensure that the application will store the keys in a genuine SGX enclave. The attestation involves the KBS verification of a signed SGX quote generated by the SKC Client. The SGX quote contains the hash of the public key of an enclave generated RSA key pair. Application keys are wrapped with a Symmetric Wrapping Key (SWK) by KBS prior to transferring to the SGX enclave. The SWK is generated by KBS and wrapped with the enclave RSA public key, which ensures that the SWK is only known to KBS and the enclave . Consequently, application keys are protected from infrastructure admins, malicious applications and compromised HW/BIOS/OS/VMM. SKC does not require the refactoring of the application because it supports a standard PKCS#11 interface. SGX Attestation Infrastructure and Secure Key Caching Product Guide SGX Attestation Infrastructure and Secure Key Caching Quick Start Guide SGX Attestation Infrastructure and Secure Key Caching Swagger Documents License BSD 3-Clause License Contributing https://github.com/intel-secl/intel-secl/ Legalities","title":"Intel\u00ae Security Libraries for Data Center (Intel\u00ae SecL-DC)"},{"location":"#intel-security-libraries-for-data-center-intel-secl-dc","text":"","title":"Intel\u00ae Security Libraries for Data Center (Intel\u00ae SecL-DC)"},{"location":"#overview-heading-change-for-version-selector-change","text":"Intel\u00ae Security Libraries for Data Center (Intel\u00ae SecL-DC) enables security use cases for data center using Intel\u00ae hardware security technologies. Hardware-based cloud security solutions provide a higher level of protection as compared to software-only security measures. There are many Intel platform security technologies, which can be used to secure customers' data. Customers have found adopting and deploying these technologies at a broad scale challenging, due to the lack of solution integration and deployment tools. Intel\u00ae Security Libraries for Data Centers (Intel\u00ae SecL - DC) was built to aid our customers in adopting and deploying Intel Security features, rooted in silicon, at scale. Intel\u00ae SecL-DC is an open-source remote attestation implementation comprising a set of building blocks that utilize Intel Security features to discover, attest, and enable critical foundation security and confidential computing use-cases. It applies the remote attestation fundamentals and standard specifications to maintain a platform data collection service, and an efficient verification engine to perform comprehensive trust evaluations. These trust evaluations can be used to govern different trust and security policies applied to any given workload. For more details please visit : https://01.org/intel-secl","title":"Overview (Heading change for version selector change)"},{"location":"#architecture","text":"The below diagram depicts the high level architecture of the Intel\u00aeSecL-DC,","title":"Architecture"},{"location":"#use-cases","text":"","title":"Use Cases"},{"location":"#foundational-security-launch-time-protection","text":"Foundational and Workload Security refers to a collection of software security solutions provided by Intel SecL-DC that leverage Intel silicon to provide boot-time integrity attestation of platform components. Starting with a Hardware Root of Trust, a chain of measurements of system components that includes the system BIOS/UEFI and OS kernel is extended to a Trusted Platform Module for remote attestation against expected measurements. Use cases include auditing the integrity of Cloud platforms, Asset or Geolocation Tagging, Platform Integrity-aware Cloud orchestration, and VM and container encryption. This acts as a firm, hardware-rooted foundation upon which to build a Cloud platform with auditable integrity verification. Foundational and Workload Security Product Guide Foundational & Workload Security Quick Start Guide Foundational & Workload Security Swagger Documents","title":"Foundational Security &amp; Launch Time Protection"},{"location":"#sgx-attestation-infrastructure","text":"The SGX Attestation infrastructure provides an end to end support for registering SGX hosts and provisioning them with SGX material (PCK certificates) and SGX collateral (security patches information - TCB Information - and Certificate Revocation Lists - CRLs). The SGX Attestation infrastructure also provides support for generating SGX quotes for SGX enclaves hosted by workloads and verifying them by a remote attesting application. The remote attesting application can also use the SGX Attestation infrastructure to enforce enclave policies (like requiring a specific enclave signer). Optionally, the SGX Attestation Infrastructure allows to integrate with Cloud Orchestrators like Openstack and Kubernetes. The SGX Attestation infrastructure does not make any assumption on the user SGX workload and enclave policy. SGX Attestation Infrastructure and Secure Key Caching Product Guide SGX Attestation Infrastructure and Secure Key Caching Quick Start Guide SGX Attestation Infrastructure and Secure Key Caching Swagger Documents As a demonstration of SGX Attestation infrastructure artifacts, a sample SGX attestation app is available here.","title":"SGX Attestation Infrastructure"},{"location":"#secure-key-caching","text":"Secure Key Caching (SKC) leverages the SGX Attestation Infrastructure to support the Secure Key Caching (SKC) use case. SKC provides key protection at rest and in-use using Intel Software Guard Extensions (SGX). SGX implements the Trusted Execution Environment (TEE) paradigm. Using the SKC Client -- a set of libraries -- applications can retrieve keys from the Intel SecL-DC Key Broker Service (KBS) and load them to an SGX-protected memory (called SGX enclave) in the application memory space. KBS performs the SGX enclave attestation to ensure that the application will store the keys in a genuine SGX enclave. The attestation involves the KBS verification of a signed SGX quote generated by the SKC Client. The SGX quote contains the hash of the public key of an enclave generated RSA key pair. Application keys are wrapped with a Symmetric Wrapping Key (SWK) by KBS prior to transferring to the SGX enclave. The SWK is generated by KBS and wrapped with the enclave RSA public key, which ensures that the SWK is only known to KBS and the enclave . Consequently, application keys are protected from infrastructure admins, malicious applications and compromised HW/BIOS/OS/VMM. SKC does not require the refactoring of the application because it supports a standard PKCS#11 interface. SGX Attestation Infrastructure and Secure Key Caching Product Guide SGX Attestation Infrastructure and Secure Key Caching Quick Start Guide SGX Attestation Infrastructure and Secure Key Caching Swagger Documents","title":"Secure Key Caching"},{"location":"#license","text":"BSD 3-Clause License","title":"License"},{"location":"#contributing","text":"https://github.com/intel-secl/intel-secl/","title":"Contributing"},{"location":"#legalities","text":"","title":"Legalities"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/01/","text":"Product Guide July 2021 Revision 4.0 Notice: This document contains information on products in the design phase of development. The information here is subject to change without notice. Do not finalize a design with this information. Intel technologies\u2019 features and benefits depend on system configuration and may require enabled hardware, software, or service activation. Learn more at intel.com, or from the OEM or retailer. No computer system can be absolutely secure. Intel does not assume any liability for lost or stolen data or systems or any damages resulting from such losses. You may not use or facilitate the use of this document in connection with any infringement or other legal analysis concerning Intel products described herein. You agree to grant Intel a non-exclusive, royalty-free license to any patent claim thereafter drafted which includes subject matter disclosed herein. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document. The products described may contain design defects or errors known as errata which may cause the product to deviate from published specifications. Current characterized errata are available on request. This document contains information on products, services and/or processes in development. All information provided here is subject to change without notice. Contact your Intel representative to obtain the latest Intel product specifications and roadmaps. Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade. Warning Altering PC clock or memory frequency and/or voltage may (i) reduce system stability and use life of the system, memory and processor; (ii) cause the processor and other system components to fail; (iii) cause reductions in system performance; (iv) cause additional heat or other damage; and (v) affect system data integrity. Intel assumes no responsibility that the memory, included if used with altered clock frequencies and/or voltages, will be fit for any particular purpose. Check with memory manufacturer for warranty and additional details. Tests document performance of components on a particular test, in specific systems. Differences in hardware, software, or configuration will affect actual performance. Consult other sources of information to evaluate performance as you consider your purchase. For more complete information about performance and benchmark results, visit http://www.intel.com/performance . Cost reduction scenarios described are intended as examples of how a given Intel- based product, in the specified circumstances and configurations, may affect future costs and provide cost savings. Circumstances will vary. Intel does not guarantee any costs or cost reduction. Results have been estimated or simulated using internal Intel analysis or architecture simulation or modeling, and provided to you for informational purposes. Any differences in your system hardware, software or configuration may affect your actual performance. Intel does not control or audit third-party benchmark data or the web sites referenced in this document. You should visit the referenced web site and confirm whether referenced data are accurate. Intel is a sponsor and member of the Benchmark XPRT Development Community, and was the major developer of the XPRT family of benchmarks. Principled Technologies is the publisher of the XPRT family of benchmarks. You should consult other information and performance tests to assist you in fully evaluating your contemplated purchases. Copies of documents which have an order number and are referenced in this document may be obtained by calling 1-800-548-4725 or by visiting w ww.intel.com/design/literature.htm. Intel, the Intel logo, Intel TXT, and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries. *Other names and brands may be claimed as the property of others. Copyright \u00a9 2020, Intel Corporation. All Rights Reserved. Revision History Revision Number Description Date 1 Updated for all GA Failures May 2019 1.5 Updated for version 1.5 release July 2019 1.6 BETA Updated for 1.6 BETA release November 2019 1.6 Updated for version 1.6 release December 2019 2.0 Updated for version 2.0 release February 2020 2.1 Updated for version 2.1 release April 2020 2.2 Updated for version 2.2 release June 2020 3.0 Updated for version 3.0 release August 2020 3.1 Updated for version 3.1 release October 2020 3.2 Updated for version 3.2 release November 2020 3.3 Updated for version 3.3 release December 2020 3.3.1 Updated for version 3.3.1 release January 2021 3.4 Updated for version 3.4 release February 2021 3.5 Updated for version 3.5 release March 2021 3.6 Updated for version 3.6 release May 2021 4.0 Updated for version 4.0 release July 2021","title":"Product Guide"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/01/#product-guide","text":"","title":"Product Guide"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/01/#july-2021","text":"","title":"July 2021"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/01/#revision-40","text":"Notice: This document contains information on products in the design phase of development. The information here is subject to change without notice. Do not finalize a design with this information. Intel technologies\u2019 features and benefits depend on system configuration and may require enabled hardware, software, or service activation. Learn more at intel.com, or from the OEM or retailer. No computer system can be absolutely secure. Intel does not assume any liability for lost or stolen data or systems or any damages resulting from such losses. You may not use or facilitate the use of this document in connection with any infringement or other legal analysis concerning Intel products described herein. You agree to grant Intel a non-exclusive, royalty-free license to any patent claim thereafter drafted which includes subject matter disclosed herein. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document. The products described may contain design defects or errors known as errata which may cause the product to deviate from published specifications. Current characterized errata are available on request. This document contains information on products, services and/or processes in development. All information provided here is subject to change without notice. Contact your Intel representative to obtain the latest Intel product specifications and roadmaps. Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade. Warning Altering PC clock or memory frequency and/or voltage may (i) reduce system stability and use life of the system, memory and processor; (ii) cause the processor and other system components to fail; (iii) cause reductions in system performance; (iv) cause additional heat or other damage; and (v) affect system data integrity. Intel assumes no responsibility that the memory, included if used with altered clock frequencies and/or voltages, will be fit for any particular purpose. Check with memory manufacturer for warranty and additional details. Tests document performance of components on a particular test, in specific systems. Differences in hardware, software, or configuration will affect actual performance. Consult other sources of information to evaluate performance as you consider your purchase. For more complete information about performance and benchmark results, visit http://www.intel.com/performance . Cost reduction scenarios described are intended as examples of how a given Intel- based product, in the specified circumstances and configurations, may affect future costs and provide cost savings. Circumstances will vary. Intel does not guarantee any costs or cost reduction. Results have been estimated or simulated using internal Intel analysis or architecture simulation or modeling, and provided to you for informational purposes. Any differences in your system hardware, software or configuration may affect your actual performance. Intel does not control or audit third-party benchmark data or the web sites referenced in this document. You should visit the referenced web site and confirm whether referenced data are accurate. Intel is a sponsor and member of the Benchmark XPRT Development Community, and was the major developer of the XPRT family of benchmarks. Principled Technologies is the publisher of the XPRT family of benchmarks. You should consult other information and performance tests to assist you in fully evaluating your contemplated purchases. Copies of documents which have an order number and are referenced in this document may be obtained by calling 1-800-548-4725 or by visiting w ww.intel.com/design/literature.htm. Intel, the Intel logo, Intel TXT, and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries. *Other names and brands may be claimed as the property of others. Copyright \u00a9 2020, Intel Corporation. All Rights Reserved.","title":"Revision 4.0"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/01/#revision-history","text":"Revision Number Description Date 1 Updated for all GA Failures May 2019 1.5 Updated for version 1.5 release July 2019 1.6 BETA Updated for 1.6 BETA release November 2019 1.6 Updated for version 1.6 release December 2019 2.0 Updated for version 2.0 release February 2020 2.1 Updated for version 2.1 release April 2020 2.2 Updated for version 2.2 release June 2020 3.0 Updated for version 3.0 release August 2020 3.1 Updated for version 3.1 release October 2020 3.2 Updated for version 3.2 release November 2020 3.3 Updated for version 3.3 release December 2020 3.3.1 Updated for version 3.3.1 release January 2021 3.4 Updated for version 3.4 release February 2021 3.5 Updated for version 3.5 release March 2021 3.6 Updated for version 3.6 release May 2021 4.0 Updated for version 4.0 release July 2021","title":"Revision History"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/","text":"Intel\u00ae Security Libraries Container Deployment Prerequisites for Containerized Deployment using Kubernetes Supported Operating Systems RHEL 8.3 Ubuntu 18.04 Kubernetes Distributions The included Intel SecL container deployment scripts can deploy using the following Kubernetes distributions: Single-node: microk8s (1.17.17) A \"single-node\" deployment will deploy all of the Intel SecL services into a single microk8s deployment on a single bare metal container host. Because all resources are deployed onto a single worker, only local resources are required. This deployment type is best used for POCs and demos due to the requirement of a physical server. Multi-node: kubeadm (1.17.17) A \"multi-node\" deployment will deploy the Intel SecL services using kubeadm as a Kubernetes pod, using NFS shared storage. Any worker nodes that will be attested must be bare metal physical servers; agents will be deployed as privileged DaemonSets. Container Runtime CRIO-1.17.5 on RHEL 8.3 CRIO-1.17.5 on Ubuntu 18.04 Storage Single-node: hostPath storage for all services and agents Multi-node: NFS storage for all services, hostPath storage for agents Building from Source Prerequisites The following steps need to be performed on a RHEL 8.3 Build machine (VM/Physical Node) System Tools and Utilities # RedHat Enterprise Linux 8.3 dnf install -y git wget tar python3 gcc gcc-c++ zip make yum-utils openssl-devel dnf install -y https://dl.fedoraproject.org/pub/fedora/linux/releases/32/Everything/x86_64/os/Packages/m/makeself-2.4.0-5.fc32.noarch.rpm ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip # Ubuntu-18.04 apt update apt remove -y gcc gcc-7 apt install -y python3-problem-report git wget tar python3 gcc-8 make makeself openssl libssl-dev libgpg-error-dev cp /usr/bin/gcc-8 /usr/bin/gcc ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip Repo tool tmpdir = $( mktemp -d ) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir /repo /usr/local/bin rm -rf $tmpdir Golang wget https://dl.google.com/go/go1.14.4.linux-amd64.tar.gz tar -xzf go1.14.4.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT = /usr/local/go export PATH = $GOROOT /bin: $PATH rm -rf go1.14.4.linux-amd64.tar.gz Docker # RedHat Enterprise Linux-8.3 dnf module enable -y container-tools dnf install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo dnf install -y docker-ce-19.03.13 docker-ce-cli-19.03.13 systemctl enable docker systemctl start docker # Ubuntu-18.04 apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null apt-get update apt-get install docker-ce = 5 :19.03.13~3-0~ubuntu-bionic docker-ce-cli = 5 :19.03.13~3-0~ubuntu-bionic containerd.io systemctl enable docker systemctl start docker Apply the below steps only if running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" systemctl daemon-reload systemctl restart docker Building OCI Container images and K8s Manifests Foundational Security Sync the repos mkdir -p /root/intel-secl/build/fs && cd /root/intel-secl/build/fs repo init -u https://github.com/intel-secl/build-manifest.git -m manifest/fs.xml -b refs/tags/v4.0.0 repo sync Run the pre-requisites setup script cd utils/build/foundational-security/ chmod +x fs-prereq.sh ./fs-prereq.sh -s Install skopeo # RHEL 8.x dnf install -y skopeo # Ubuntu 18.04 add-apt-repository ppa:projectatomic/ppa apt-get update apt-get install skopeo Build Single-node: A \"single-node\" deployment will deploy all of the Intel SecL services into a single microk8s deployment on a single bare metal container host. Because all resources are deployed onto a single worker, only local resources are required. This deployment type is best used for POCs and demos due to the requirement of a physical server Single node cluster with microk8s - make k8s-aio Multi-node: kubeadm (1.17.17) A \"multi-node\" deployment will deploy the Intel SecL services using kubeadm as a Kubernetes pod, using NFS shared storage. Multi node cluster with kubeadm - make k8s After the build process is complete, the container images, Kubernetes manifests and deployment scripts can be found in the following folder: /root/intel-secl/build/fs/k8s/ Workload Confidentiality Workload Confidentiality can be used with either the CRIO container runtime. Container Confidentiality with CRIO Runtime Sync the repos mkdir -p /root/intel-secl/build/cc-crio && cd /root/intel-secl/build/cc-crio repo init -u https://github.com/intel-secl/build-manifest.git -m manifest/cc-crio.xml -b refs/tags/v4.0.0 repo sync Run the pre-requisites script cd utils/build/workload-security chmod +x ws-prereq.sh ./ws-prereq.sh -c Build cd /root/intel-secl/build/cc-crio #Single node cluster with microk8s make k8s-aio #Multi node cluster with kubeadm make k8s The container images, Kubernetes manifests and deployment scripts will output to the following folder: /root/intel-secl/build/cc-crio/k8s/ Intel\u00ae SecL-DC Foundational Security K8s Deployment This section details deployment of Intel SecL services as a Kubernetes deployment, using a privileged DaemonSet for the Trust Agent and Workload Agent. Pre-requisites Install openssl on the Kubernetes control node Ensure a container registry is running locally or remotely. Note For the single-node microk8s deployment, a registry can be brought up by using microk8s addons. More details can be found in the Microk8s documentation. This is not mandatory; if a remote registry already exists, it can be used. Note For multi-node kubeadm deployment, a container registry is required. Push all container images to container registry. # Without TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/<image-name>:<image-tag> --dest-tls-verify = false # With TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/image-name>:<image-tag> Note For microk8s deployments, when the container registry is enabled locally, the OCI container images need to be copied to the node where registry is enabled and then the above command can be run. This is not required if a remote registry is available. On each worker node registered to the Kubernetes control plane, perform the following prerequisite steps: Foundational Security Tboot-1.10.1 or later to be installed for non SUEFI servers. Tboot installation Details Only for Ubuntu, run the following command $ modprobe msr Workload Security Container Confidentiality with CRIO runtime Tboot-1.10.1 or later to be installed for non SUEFI servers. Tboot installation Details Copy container-runtime directory to each of the physical servers Run the install-prereqs-crio.sh script on the physical servers from container-runtime Reboot the server For Ubuntu worker nodes only, run the following command $ modprobe msr Deploy Single-Node Deployment A single-node deployment utilizes Microk8s to deploy all services onto a single physical server. This deployment type is best suited to POCs and demos. Pre-requisites Setup microk8s must be installed on a physical server with a supported combination of platform integrity security technologies enabled. Copy all manifests and OCI container images as required to the Kubernetes control node Ensure a container registry is available The Kubernetes cluster admin can configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: TXT-ENABLED or node.type: SUEFI-ENABLED respectively for TXT/SUEFI enabled servers can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the Kubernetes manifests. This can be edited in k8s/manifests/ta/daemonset.yml , k8s/manifests/wla/daemonset.yml #Label node for TXT kubectl label node <node-name> node.type = TXT-ENABLED #Label node for SUEFI kubectl label node <node-name> node.type = SUEFI-ENABLED In a microk8s cluster, the --allow-privileged=true flag needs to be added to the kube-apiserver under /var/snap/microk8s/current/args/kube-apiserver and restart kube-apiserver with systemctl restart snap.microk8s.daemon-apiserver to allow running of privileged containers like TRUST-AGENT and WORKLOAD-AGENT For container confidentiality use cases, ensure a backend KMIP-2.0 compliant server like pykmip is up and running. Manifests Update all the Kubernetes manifests with the image names to be pulled from the registry The tolerations and node-affinity for the isecl-scheduler and isecl-controller need to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to microk8s.io/cluster based on the Kubernetes distribution ( kubeadm and microk8s, respectively) Deploy steps The bootstrap script will facilitate the deployment of all Intel SecL Foundational Security components based on the use cases to be enabled. Update .env file #Kubernetes Distribution - microk8s K8S_DISTRIBUTION = microk8s K8S_CONTROL_PLANE_IP = K8S_CONTROL_PLANE_HOSTNAME = # cms CMS_BASE_URL = https://cms-svc.isecl.svc.cluster.local:8445/cms/v1 CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> CMS_K8S_ENDPOINT_URL = https://<k8s control-plane IP>:30445/cms/v1 # authservice AAS_API_URL = https://aas-svc.isecl.svc.cluster.local:8444/aas/v1 AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_DB_HOSTNAME = aasdb-svc.isecl.svc.cluster.local AAS_DB_PORT = \"5432\" AAS_DB_NAME = aasdb AAS_DB_SSLMODE = verify-full AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> #NATS_ACCOUNT_NAME=ISecL-account # Workload Service WLS_SERVICE_USERNAME = admin@wls WLS_SERVICE_PASSWORD = wlsAdminPass WLS_DB_USERNAME = wlsdbuser WLS_DB_PASSWORD = wlsdbpassword WLS_DB_HOSTNAME = wlsdb-svc.isecl.svc.cluster.local WLS_DB_NAME = wlsdb WLS_DB_PORT = \"5432\" WLS_API_URL = https://wls-svc.isecl.svc.cluster.local:5000/wls/v1 WLS_CERT_SAN_LIST = wls-svc.isecl.svc.cluster.local # Host Verification Service HVS_SERVICE_USERNAME = admin@hvs HVS_SERVICE_PASSWORD = hvsAdminPass HVS_DB_USERNAME = hvsdbuser HVS_DB_PASSWORD = hvsdbpassword HVS_DB_HOSTNAME = hvsdb-svc.isecl.svc.cluster.local HVS_DB_NAME = hvsdb HVS_CERT_SAN_LIST = hvs-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> HVS_DB_PORT = \"5432\" HVS_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2/ #Nats Servers configuration for TA and HVS #NATS_SERVERS=nats://<K8s control-plane IP/Hostname>:30222 # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> # For microk8s # K8S_API_SERVER_CERT=/var/snap/microk8s/current/certs/server.crt K8S_API_SERVER_CERT = /var/snap/microk8s/current/certs/server.crt # This is valid for multinode deployment, should be populated once ihub is deployed successfully IHUB_PUB_KEY_PATH = HVS_BASE_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2 # TrustAgent # e.g TA_CERT_SAN_LIST=*.example.com,192.168.1.* TA_CERT_SAN_LIST = TPM_OWNER_SECRET = # Workload Agent WLA_SERVICE_USERNAME = wlauser@wls WLA_SERVICE_PASSWORD = wlaAdminPass # KBS ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_HOSTNAME = <KMIP IP/Hostname> KMIP_SERVER_IP = KMIP_SERVER_PORT = # Retrieve the following KMIP server\u2019s client certificate, client key and root ca certificate from the KMIP server. # This key and certificates will be available in KMIP server, /etc/pykmip is the default path copy them to this system manifests/kbs/kmip-secrets path KMIP_CLIENT_CERT_NAME = client_certificate.pem KMIP_CLIENT_KEY_NAME = client_key.pem KMIP_ROOT_CERT_NAME = root_certificate.pem # ISecl Scheduler # For microk8s # K8S_CA_KEY=/var/snap/microk8s/current/certs/ca.key # K8S_CA_CERT=/var/snap/microk8s/current/certs/ca.crt K8S_CA_KEY = /var/snap/microk8s/current/certs/ca.key K8S_CA_CERT = /var/snap/microk8s/current/certs/ca.crt # populate users.env ISECL_INSTALL_COMPONENTS = \"AAS,HVS,WLS,IHUB,KBS,WLA,TA,WPM\" #NATS_CERT_SAN_LIST= #NATS_TLS_COMMON_NAME= GLOBAL_ADMIN_USERNAME = GLOBAL_ADMIN_PASSWORD = INSTALL_ADMIN_USERNAME = INSTALL_ADMIN_PASSWORD = WPM_SERVICE_USERNAME = WPM_SERVICE_PASSWORD = CUSTOM_CLAIMS_COMPONENTS = CCC_ADMIN_USERNAME = CCC_ADMIN_PASSWORD = Run scripts on Kubernetes Controller Node These bootstrap scripts are sample scripts to allow for a quick start of Intel SecL services and agents. #Pre-reqs.sh ./pre-requisites.sh #isecl-bootstrap-db-services #Reference #./isecl-bootstrap-db-services.sh: option requires an argument -- h #Usage: ./isecl-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, Workload Service and Host verification Service # purge Delete Database Services for Authservice, Workload Service and Host verification Service ./isecl-bootstrap-db-services.sh up #isecl-bootstrap #Reference #Usage: Usage: ./isecl-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap ISecL K8s environment for specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete ISecL K8s environment for specified agent/service/usecase [will not delete data, config, logs] # purge Delete ISecL K8s environment with data, config, logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of tagent, wlagent # service Can be one of cms, authservice, hvs, ihub, wls, kbs, isecl-controller, isecl-scheduler # usecase Can be one of foundation-security, workload-security, isecl-orchestration-k8s ./isecl-bootstrap.sh up <all/usecase of choice> Perform the following steps for isecl-scheduler #Copy scheduler-policy.json cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions/ #Edit the kube-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service Multi-Node Deployment A multi-node deployment will deploy Intel SecL control plane services as a pod on a kubeadm Kubernetes cluster, using a DaemonSet to deploy agent components to worker nodes. Pre-requisites Setup kubeadm must be deployed Copy all manifests and OCI container images as required to the Kubernetes control node Intel SecL container images must be pushed to a container registry The Kubernetes cluster admin can configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: TXT-ENABLED or node.type: SUEFI-ENABLED respectively for TXT/SUEFI enabled servers can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the K8s manifests. This can be edited in k8s/manifests/ta/daemonset.yml , k8s/manifests/wla/daemonset.yml #Label node for TXT kubectl label node <node-name> node.type = TXT-ENABLED #Label node for SUEFI kubectl label node <node-name> node.type = SUEFI-ENABLED The NFS storage class is used in Kubernetes for data persistence. An NFS server with appropriate directory structure and permissions is required. Intel recommends creation of a separate user ID with permissions for all Intel SecL directories. Below are some samples for reference Snapshot showing directory structure for which user needs to create on NFS volumes manually or using custom scripts. Snapshot showing ownership and permissions for directories for which user needs to manually grant the ownership. Snapshot for configuring PV and PVC , user need to provide the NFS server IP or hostname and paths for each of the service directories. Sample manifest for creating config-pv for cms service --- apiVersion : v1 kind : PersistentVolume metadata : name : cms-config-pv spec : capacity : storage : 128Mi volumeMode : Filesystem accessModes : - ReadWriteMany persistentVolumeReclaimPolicy : Retain storageClassName : nfs nfs : path : /<NFS-vol-base-path>/isecl/cms/config server : <NFS Server IP/Hostname> Sample manifest for creating config-pvc for cms service --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : cms-config-pvc namespace : isecl spec : storageClassName : nfs accessModes : - ReadWriteMany resources : requests : storage : 128Mi Note The user id specified in security context in deployment.yml for a given service and owner of the service related directories in NFS must be same For container confidentiality use cases, a backend KMIP-2.0 compliant server like pykmip must be availabe Manifests Update Kubernetes manifests with the image names to be pulled from the registry The tolerations and node-affinity for the isecl-scheduler and isecl-controller nees to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to node-role.kubernetes.io/master All NFS PV yaml files must be updated with the path: /<NFS-vol-path> and server: <NFS Server IP/Hostname> under each service manifest file for config , logs , db-data Deploy steps Update .env file #Kubernetes Distribution - kubeadm K8S_DISTRIBUTION = kubeadm K8S_CONTROL_PLANE_IP = K8S_CONTROL_PLANE_HOSTNAME = # cms CMS_BASE_URL = https://cms-svc.isecl.svc.cluster.local:8445/cms/v1 CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> CMS_K8S_ENDPOINT_URL = https://<k8s control-plane IP>:30445/cms/v1 # authservice AAS_API_URL = https://aas-svc.isecl.svc.cluster.local:8444/aas/v1 AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_DB_HOSTNAME = aasdb-svc.isecl.svc.cluster.local AAS_DB_PORT = \"5432\" AAS_DB_NAME = aasdb AAS_DB_SSLMODE = verify-full AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> #NATS_ACCOUNT_NAME=ISecL-account # Workload Service WLS_SERVICE_USERNAME = admin@wls WLS_SERVICE_PASSWORD = wlsAdminPass WLS_DB_USERNAME = wlsdbuser WLS_DB_PASSWORD = wlsdbpassword WLS_DB_HOSTNAME = wlsdb-svc.isecl.svc.cluster.local WLS_DB_NAME = wlsdb WLS_DB_PORT = \"5432\" WLS_API_URL = https://wls-svc.isecl.svc.cluster.local:5000/wls/v1 WLS_CERT_SAN_LIST = wls-svc.isecl.svc.cluster.local # Host Verification Service HVS_SERVICE_USERNAME = admin@hvs HVS_SERVICE_PASSWORD = hvsAdminPass HVS_DB_USERNAME = hvsdbuser HVS_DB_PASSWORD = hvsdbpassword HVS_DB_HOSTNAME = hvsdb-svc.isecl.svc.cluster.local HVS_DB_NAME = hvsdb HVS_CERT_SAN_LIST = hvs-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> HVS_DB_PORT = \"5432\" HVS_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2/ #Nats Servers configuration for TA and HVS #NATS_SERVERS=nats://<K8s control-plane IP/Hostname>:30222 # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> # For Kubeadm # K8S_API_SERVER_CERT=/etc/kubernetes/pki/apiserver.crt K8S_API_SERVER_CERT = /etc/kubernetes/pki/apiserver.crt # This is valid for multinode deployment, should be populated once ihub is deployed successfully IHUB_PUB_KEY_PATH = HVS_BASE_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2 # TrustAgent # e.g TA_CERT_SAN_LIST=*.example.com,192.168.1.* TA_CERT_SAN_LIST = TPM_OWNER_SECRET = # Workload Agent WLA_SERVICE_USERNAME = wlauser@wls WLA_SERVICE_PASSWORD = wlaAdminPass # KBS ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_HOSTNAME = <KMIP IP/Hostname> KMIP_SERVER_IP = KMIP_SERVER_PORT = # Retrieve the following KMIP server\u2019s client certificate, client key and root ca certificate from the KMIP server. # This key and certificates will be available in KMIP server, /etc/pykmip is the default path copy them to this system manifests/kbs/kmip-secrets path KMIP_CLIENT_CERT_NAME = client_certificate.pem KMIP_CLIENT_KEY_NAME = client_key.pem KMIP_ROOT_CERT_NAME = root_certificate.pem # ISecl Scheduler # For Kubeadm # K8S_CA_KEY=/etc/kubernetes/pki/ca.key # K8S_CA_CERT=/etc/kubernetes/pki/ca.crt K8S_CA_KEY = /etc/kubernetes/pki/ca.key K8S_CA_CERT = /etc/kubernetes/pki/ca.crt # populate users.env ISECL_INSTALL_COMPONENTS = \"AAS,HVS,WLS,IHUB,KBS,WLA,TA,WPM\" #NATS_CERT_SAN_LIST= #NATS_TLS_COMMON_NAME= GLOBAL_ADMIN_USERNAME = GLOBAL_ADMIN_PASSWORD = INSTALL_ADMIN_USERNAME = INSTALL_ADMIN_PASSWORD = WPM_SERVICE_USERNAME = WPM_SERVICE_PASSWORD = CUSTOM_CLAIMS_COMPONENTS = CCC_ADMIN_USERNAME = CCC_ADMIN_PASSWORD = Run scripts on Kubernetes Controller Node The following bootstrap scripts are sample scripts to allow for a quick start of Intel SecL services and agents. #Pre-reqs.sh ./pre-requisites.sh #isecl-bootstrap-db-services #Reference #./isecl-bootstrap-db-services.sh: option requires an argument -- h #Usage: ./isecl-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, Workload Service and Host verification Service # purge Delete Database Services for Authservice, Workload Service and Host verification Service ./isecl-bootstrap-db-services.sh up #isecl-bootstrap #Reference #Usage: Usage: ./isecl-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap ISecL K8s environment for specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete ISecL K8s environment for specified agent/service/usecase [will not delete data, config, logs] # purge Delete ISecL K8s environment with data, config, logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of tagent, wlagent # service Can be one of cms, authservice, hvs, ihub, wls, kbs, isecl-controller, isecl-scheduler # usecase Can be one of foundation-security, workload-security, isecl-orchestration-k8s ./isecl-bootstrap.sh up <all/usecase of choice> Copy the ihub_public_key.pem from NFS path - <mnt>/isecl/ihub/config/ihub_public_key.pem to K8s Master Update the isecl-k8s.env for IHUB_PUB_KEY_PATH Bring up the isecl-k8s-scheduler ./isecl-bootstrap.sh up isecl-scheduler Create and update scheduler-policy.json path mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions Configure kube-scheduler to establish communication with isecl-scheduler. Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec : containers : - command : - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers : volumeMounts : - mountPath : /opt/isecl-k8s-extensions/ name : extendedsched readOnly : true volumes : - hostPath : path : /opt/isecl-k8s-extensions/ type : name : extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml Restart kubelet which restart all the k8s services including kube-scheduler systemctl restart kubelet Default Service and Agent Mount Paths Single Node Deployments Single node Deployments use hostPath mounting pod(container) files directly on the host. #Certificate-Management-Service Config : /etc/cms Logs : /var/log/cms #Authentication Authorization Service Config : /etc/authservice Logs : /var/log/authservice Pg-data : /usr/local/kube/data/authservice/pgdata #Host Attestation Service Config : /etc/hvs Logs : /var/log/hvs Pg-data : /usr/local/kube/data/hvs #Integration-Hub Config : /etc/ihub Log : /var/log/ihub #Workload Service Config : /etc/workload-service Logs : /var/log/workload-service Pg-data : /usr/local/kube/data/workload-service #Key-Broker-Service Config : /etc/kbs Log : /var/log/kbs Opt : /opt/kbs #Trust Agent: Config : /opt/trustagent/configuration Logs : /var/log/trustagent/ tpmrm : /dev/tpmrm0 txt-stat : /usr/sbin/txt-stat ta-hostname-path : /etc/hostname ta-hosts-path : /etc/hosts #Workload Agent: Config : /etc/workload-agent/ Logs : /var/log/workload-agent TA Config : /opt/trustagent/configuration WLA-Socket : /var/run/workload-agent Multi Node Deployments Multi node Deployments use Kubernetes persistent volume and persistent volume claims for mounting pod(container) files on NFS volumes for all services. Agents will use hostPath for persistent storage. F #Certificate-Management-Service Config : <NFS-vol-base-path>/isecl/cms/config Logs : <NFS-vol-base-path>/isecl/cms/logs #Authentication Authorization Service Config : <NFS-vol-base-path>/isecl/aas/config Logs : <NFS-vol-base-path>/isecl/aas/logs Pg-data : <NFS-vol-base-path>/isecl/aas/db #Host Attestation Service Config : <NFS-vol-base-path>/isecl/hvs/config Logs : <NFS-vol-base-path>/isecl/hvs/logs Pg-data : <NFS-vol-base-path>/usr/local/kube/data/hvs #Integration-Hub Config : <NFS-vol-base-path>/isecl/ihub/config Log : <NFS-vol-base-path>/isecl/ihub/logs #Workload Service Config : <NFS-vol-base-path>/isecl/wls/config Logs : <NFS-vol-base-path>/isecl/wls/log Pg-data : <NFS-vol-base-path>/usr/local/kube/data/wls #Key-Broker-Service Config : <NFS-vol-base-path>/isecl/kbs/config Log : <NFS-vol-base-path>/isecl/kbs/logs Opt : <NFS-vol-base-path>/isecl/kbs/kbs/opt #Trust Agent: Config : /opt/trustagent/configuration Logs : /var/log/trustagent/ tpmrm : /dev/tpmrm0 txt-stat : /usr/sbin/txt-stat ta-hostname-path : /etc/hostname ta-hosts-path : /etc/hosts #Workload Agent: Config : /etc/workload-agent/ Logs : /var/log/workload-agent WLA-Socket : /var/run/workload-agent Default Service Ports For both single-node and multi-node deployments, the following ports are used: CMS : 30445 AAS : 30444 HVS : 30443 WLS : 30447 IHUB : None KBS : 30448 K8s-scheduler : 30888 K8s-controller : None TA : 31443 WLA : None","title":"Intel\u00ae Security Libraries Container Deployment"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#intel-security-libraries-container-deployment","text":"","title":"Intel\u00ae Security Libraries Container Deployment"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#prerequisites-for-containerized-deployment-using-kubernetes","text":"","title":"Prerequisites for Containerized Deployment using Kubernetes"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#supported-operating-systems","text":"RHEL 8.3 Ubuntu 18.04","title":"Supported Operating Systems"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#kubernetes-distributions","text":"The included Intel SecL container deployment scripts can deploy using the following Kubernetes distributions: Single-node: microk8s (1.17.17) A \"single-node\" deployment will deploy all of the Intel SecL services into a single microk8s deployment on a single bare metal container host. Because all resources are deployed onto a single worker, only local resources are required. This deployment type is best used for POCs and demos due to the requirement of a physical server. Multi-node: kubeadm (1.17.17) A \"multi-node\" deployment will deploy the Intel SecL services using kubeadm as a Kubernetes pod, using NFS shared storage. Any worker nodes that will be attested must be bare metal physical servers; agents will be deployed as privileged DaemonSets.","title":"Kubernetes Distributions"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#container-runtime","text":"CRIO-1.17.5 on RHEL 8.3 CRIO-1.17.5 on Ubuntu 18.04","title":"Container Runtime"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#storage","text":"Single-node: hostPath storage for all services and agents Multi-node: NFS storage for all services, hostPath storage for agents","title":"Storage"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#building-from-source","text":"","title":"Building from Source"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#prerequisites","text":"The following steps need to be performed on a RHEL 8.3 Build machine (VM/Physical Node)","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#system-tools-and-utilities","text":"# RedHat Enterprise Linux 8.3 dnf install -y git wget tar python3 gcc gcc-c++ zip make yum-utils openssl-devel dnf install -y https://dl.fedoraproject.org/pub/fedora/linux/releases/32/Everything/x86_64/os/Packages/m/makeself-2.4.0-5.fc32.noarch.rpm ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip # Ubuntu-18.04 apt update apt remove -y gcc gcc-7 apt install -y python3-problem-report git wget tar python3 gcc-8 make makeself openssl libssl-dev libgpg-error-dev cp /usr/bin/gcc-8 /usr/bin/gcc ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip","title":"System Tools and Utilities"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#repo-tool","text":"tmpdir = $( mktemp -d ) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir /repo /usr/local/bin rm -rf $tmpdir","title":"Repo tool"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#golang","text":"wget https://dl.google.com/go/go1.14.4.linux-amd64.tar.gz tar -xzf go1.14.4.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT = /usr/local/go export PATH = $GOROOT /bin: $PATH rm -rf go1.14.4.linux-amd64.tar.gz","title":"Golang"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#docker","text":"# RedHat Enterprise Linux-8.3 dnf module enable -y container-tools dnf install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo dnf install -y docker-ce-19.03.13 docker-ce-cli-19.03.13 systemctl enable docker systemctl start docker # Ubuntu-18.04 apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null apt-get update apt-get install docker-ce = 5 :19.03.13~3-0~ubuntu-bionic docker-ce-cli = 5 :19.03.13~3-0~ubuntu-bionic containerd.io systemctl enable docker systemctl start docker Apply the below steps only if running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" systemctl daemon-reload systemctl restart docker","title":"Docker"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#building-oci-container-images-and-k8s-manifests","text":"","title":"Building OCI Container images and K8s Manifests"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#foundational-security","text":"Sync the repos mkdir -p /root/intel-secl/build/fs && cd /root/intel-secl/build/fs repo init -u https://github.com/intel-secl/build-manifest.git -m manifest/fs.xml -b refs/tags/v4.0.0 repo sync Run the pre-requisites setup script cd utils/build/foundational-security/ chmod +x fs-prereq.sh ./fs-prereq.sh -s Install skopeo # RHEL 8.x dnf install -y skopeo # Ubuntu 18.04 add-apt-repository ppa:projectatomic/ppa apt-get update apt-get install skopeo Build Single-node: A \"single-node\" deployment will deploy all of the Intel SecL services into a single microk8s deployment on a single bare metal container host. Because all resources are deployed onto a single worker, only local resources are required. This deployment type is best used for POCs and demos due to the requirement of a physical server Single node cluster with microk8s - make k8s-aio Multi-node: kubeadm (1.17.17) A \"multi-node\" deployment will deploy the Intel SecL services using kubeadm as a Kubernetes pod, using NFS shared storage. Multi node cluster with kubeadm - make k8s After the build process is complete, the container images, Kubernetes manifests and deployment scripts can be found in the following folder: /root/intel-secl/build/fs/k8s/","title":"Foundational Security"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#workload-confidentiality","text":"Workload Confidentiality can be used with either the CRIO container runtime.","title":"Workload Confidentiality"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#container-confidentiality-with-crio-runtime","text":"Sync the repos mkdir -p /root/intel-secl/build/cc-crio && cd /root/intel-secl/build/cc-crio repo init -u https://github.com/intel-secl/build-manifest.git -m manifest/cc-crio.xml -b refs/tags/v4.0.0 repo sync Run the pre-requisites script cd utils/build/workload-security chmod +x ws-prereq.sh ./ws-prereq.sh -c Build cd /root/intel-secl/build/cc-crio #Single node cluster with microk8s make k8s-aio #Multi node cluster with kubeadm make k8s The container images, Kubernetes manifests and deployment scripts will output to the following folder: /root/intel-secl/build/cc-crio/k8s/","title":"Container Confidentiality with CRIO Runtime"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#intel-secl-dc-foundational-security-k8s-deployment","text":"This section details deployment of Intel SecL services as a Kubernetes deployment, using a privileged DaemonSet for the Trust Agent and Workload Agent.","title":"Intel\u00ae SecL-DC Foundational Security K8s Deployment"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#pre-requisites","text":"Install openssl on the Kubernetes control node Ensure a container registry is running locally or remotely. Note For the single-node microk8s deployment, a registry can be brought up by using microk8s addons. More details can be found in the Microk8s documentation. This is not mandatory; if a remote registry already exists, it can be used. Note For multi-node kubeadm deployment, a container registry is required. Push all container images to container registry. # Without TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/<image-name>:<image-tag> --dest-tls-verify = false # With TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/image-name>:<image-tag> Note For microk8s deployments, when the container registry is enabled locally, the OCI container images need to be copied to the node where registry is enabled and then the above command can be run. This is not required if a remote registry is available. On each worker node registered to the Kubernetes control plane, perform the following prerequisite steps:","title":"Pre-requisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#foundational-security_1","text":"Tboot-1.10.1 or later to be installed for non SUEFI servers. Tboot installation Details Only for Ubuntu, run the following command $ modprobe msr","title":"Foundational Security"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#workload-security","text":"","title":"Workload Security"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#container-confidentiality-with-crio-runtime_1","text":"Tboot-1.10.1 or later to be installed for non SUEFI servers. Tboot installation Details Copy container-runtime directory to each of the physical servers Run the install-prereqs-crio.sh script on the physical servers from container-runtime Reboot the server For Ubuntu worker nodes only, run the following command $ modprobe msr","title":"Container Confidentiality with CRIO runtime"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#deploy","text":"","title":"Deploy"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#single-node-deployment","text":"A single-node deployment utilizes Microk8s to deploy all services onto a single physical server. This deployment type is best suited to POCs and demos.","title":"Single-Node Deployment"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#pre-requisites_1","text":"","title":"Pre-requisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#setup","text":"microk8s must be installed on a physical server with a supported combination of platform integrity security technologies enabled. Copy all manifests and OCI container images as required to the Kubernetes control node Ensure a container registry is available The Kubernetes cluster admin can configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: TXT-ENABLED or node.type: SUEFI-ENABLED respectively for TXT/SUEFI enabled servers can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the Kubernetes manifests. This can be edited in k8s/manifests/ta/daemonset.yml , k8s/manifests/wla/daemonset.yml #Label node for TXT kubectl label node <node-name> node.type = TXT-ENABLED #Label node for SUEFI kubectl label node <node-name> node.type = SUEFI-ENABLED In a microk8s cluster, the --allow-privileged=true flag needs to be added to the kube-apiserver under /var/snap/microk8s/current/args/kube-apiserver and restart kube-apiserver with systemctl restart snap.microk8s.daemon-apiserver to allow running of privileged containers like TRUST-AGENT and WORKLOAD-AGENT For container confidentiality use cases, ensure a backend KMIP-2.0 compliant server like pykmip is up and running.","title":"Setup"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#manifests","text":"Update all the Kubernetes manifests with the image names to be pulled from the registry The tolerations and node-affinity for the isecl-scheduler and isecl-controller need to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to microk8s.io/cluster based on the Kubernetes distribution ( kubeadm and microk8s, respectively)","title":"Manifests"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#deploy-steps","text":"The bootstrap script will facilitate the deployment of all Intel SecL Foundational Security components based on the use cases to be enabled.","title":"Deploy steps"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#update-env-file","text":"#Kubernetes Distribution - microk8s K8S_DISTRIBUTION = microk8s K8S_CONTROL_PLANE_IP = K8S_CONTROL_PLANE_HOSTNAME = # cms CMS_BASE_URL = https://cms-svc.isecl.svc.cluster.local:8445/cms/v1 CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> CMS_K8S_ENDPOINT_URL = https://<k8s control-plane IP>:30445/cms/v1 # authservice AAS_API_URL = https://aas-svc.isecl.svc.cluster.local:8444/aas/v1 AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_DB_HOSTNAME = aasdb-svc.isecl.svc.cluster.local AAS_DB_PORT = \"5432\" AAS_DB_NAME = aasdb AAS_DB_SSLMODE = verify-full AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> #NATS_ACCOUNT_NAME=ISecL-account # Workload Service WLS_SERVICE_USERNAME = admin@wls WLS_SERVICE_PASSWORD = wlsAdminPass WLS_DB_USERNAME = wlsdbuser WLS_DB_PASSWORD = wlsdbpassword WLS_DB_HOSTNAME = wlsdb-svc.isecl.svc.cluster.local WLS_DB_NAME = wlsdb WLS_DB_PORT = \"5432\" WLS_API_URL = https://wls-svc.isecl.svc.cluster.local:5000/wls/v1 WLS_CERT_SAN_LIST = wls-svc.isecl.svc.cluster.local # Host Verification Service HVS_SERVICE_USERNAME = admin@hvs HVS_SERVICE_PASSWORD = hvsAdminPass HVS_DB_USERNAME = hvsdbuser HVS_DB_PASSWORD = hvsdbpassword HVS_DB_HOSTNAME = hvsdb-svc.isecl.svc.cluster.local HVS_DB_NAME = hvsdb HVS_CERT_SAN_LIST = hvs-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> HVS_DB_PORT = \"5432\" HVS_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2/ #Nats Servers configuration for TA and HVS #NATS_SERVERS=nats://<K8s control-plane IP/Hostname>:30222 # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> # For microk8s # K8S_API_SERVER_CERT=/var/snap/microk8s/current/certs/server.crt K8S_API_SERVER_CERT = /var/snap/microk8s/current/certs/server.crt # This is valid for multinode deployment, should be populated once ihub is deployed successfully IHUB_PUB_KEY_PATH = HVS_BASE_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2 # TrustAgent # e.g TA_CERT_SAN_LIST=*.example.com,192.168.1.* TA_CERT_SAN_LIST = TPM_OWNER_SECRET = # Workload Agent WLA_SERVICE_USERNAME = wlauser@wls WLA_SERVICE_PASSWORD = wlaAdminPass # KBS ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_HOSTNAME = <KMIP IP/Hostname> KMIP_SERVER_IP = KMIP_SERVER_PORT = # Retrieve the following KMIP server\u2019s client certificate, client key and root ca certificate from the KMIP server. # This key and certificates will be available in KMIP server, /etc/pykmip is the default path copy them to this system manifests/kbs/kmip-secrets path KMIP_CLIENT_CERT_NAME = client_certificate.pem KMIP_CLIENT_KEY_NAME = client_key.pem KMIP_ROOT_CERT_NAME = root_certificate.pem # ISecl Scheduler # For microk8s # K8S_CA_KEY=/var/snap/microk8s/current/certs/ca.key # K8S_CA_CERT=/var/snap/microk8s/current/certs/ca.crt K8S_CA_KEY = /var/snap/microk8s/current/certs/ca.key K8S_CA_CERT = /var/snap/microk8s/current/certs/ca.crt # populate users.env ISECL_INSTALL_COMPONENTS = \"AAS,HVS,WLS,IHUB,KBS,WLA,TA,WPM\" #NATS_CERT_SAN_LIST= #NATS_TLS_COMMON_NAME= GLOBAL_ADMIN_USERNAME = GLOBAL_ADMIN_PASSWORD = INSTALL_ADMIN_USERNAME = INSTALL_ADMIN_PASSWORD = WPM_SERVICE_USERNAME = WPM_SERVICE_PASSWORD = CUSTOM_CLAIMS_COMPONENTS = CCC_ADMIN_USERNAME = CCC_ADMIN_PASSWORD =","title":"Update .env file"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#run-scripts-on-kubernetes-controller-node","text":"These bootstrap scripts are sample scripts to allow for a quick start of Intel SecL services and agents. #Pre-reqs.sh ./pre-requisites.sh #isecl-bootstrap-db-services #Reference #./isecl-bootstrap-db-services.sh: option requires an argument -- h #Usage: ./isecl-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, Workload Service and Host verification Service # purge Delete Database Services for Authservice, Workload Service and Host verification Service ./isecl-bootstrap-db-services.sh up #isecl-bootstrap #Reference #Usage: Usage: ./isecl-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap ISecL K8s environment for specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete ISecL K8s environment for specified agent/service/usecase [will not delete data, config, logs] # purge Delete ISecL K8s environment with data, config, logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of tagent, wlagent # service Can be one of cms, authservice, hvs, ihub, wls, kbs, isecl-controller, isecl-scheduler # usecase Can be one of foundation-security, workload-security, isecl-orchestration-k8s ./isecl-bootstrap.sh up <all/usecase of choice> Perform the following steps for isecl-scheduler #Copy scheduler-policy.json cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions/ #Edit the kube-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service","title":"Run scripts on Kubernetes Controller Node"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#multi-node-deployment","text":"A multi-node deployment will deploy Intel SecL control plane services as a pod on a kubeadm Kubernetes cluster, using a DaemonSet to deploy agent components to worker nodes.","title":"Multi-Node Deployment"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#pre-requisites_2","text":"","title":"Pre-requisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#setup_1","text":"kubeadm must be deployed Copy all manifests and OCI container images as required to the Kubernetes control node Intel SecL container images must be pushed to a container registry The Kubernetes cluster admin can configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: TXT-ENABLED or node.type: SUEFI-ENABLED respectively for TXT/SUEFI enabled servers can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the K8s manifests. This can be edited in k8s/manifests/ta/daemonset.yml , k8s/manifests/wla/daemonset.yml #Label node for TXT kubectl label node <node-name> node.type = TXT-ENABLED #Label node for SUEFI kubectl label node <node-name> node.type = SUEFI-ENABLED The NFS storage class is used in Kubernetes for data persistence. An NFS server with appropriate directory structure and permissions is required. Intel recommends creation of a separate user ID with permissions for all Intel SecL directories. Below are some samples for reference Snapshot showing directory structure for which user needs to create on NFS volumes manually or using custom scripts. Snapshot showing ownership and permissions for directories for which user needs to manually grant the ownership. Snapshot for configuring PV and PVC , user need to provide the NFS server IP or hostname and paths for each of the service directories. Sample manifest for creating config-pv for cms service --- apiVersion : v1 kind : PersistentVolume metadata : name : cms-config-pv spec : capacity : storage : 128Mi volumeMode : Filesystem accessModes : - ReadWriteMany persistentVolumeReclaimPolicy : Retain storageClassName : nfs nfs : path : /<NFS-vol-base-path>/isecl/cms/config server : <NFS Server IP/Hostname> Sample manifest for creating config-pvc for cms service --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : cms-config-pvc namespace : isecl spec : storageClassName : nfs accessModes : - ReadWriteMany resources : requests : storage : 128Mi Note The user id specified in security context in deployment.yml for a given service and owner of the service related directories in NFS must be same For container confidentiality use cases, a backend KMIP-2.0 compliant server like pykmip must be availabe","title":"Setup"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#manifests_1","text":"Update Kubernetes manifests with the image names to be pulled from the registry The tolerations and node-affinity for the isecl-scheduler and isecl-controller nees to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to node-role.kubernetes.io/master All NFS PV yaml files must be updated with the path: /<NFS-vol-path> and server: <NFS Server IP/Hostname> under each service manifest file for config , logs , db-data","title":"Manifests"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#deploy-steps_1","text":"","title":"Deploy steps"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#update-env-file_1","text":"#Kubernetes Distribution - kubeadm K8S_DISTRIBUTION = kubeadm K8S_CONTROL_PLANE_IP = K8S_CONTROL_PLANE_HOSTNAME = # cms CMS_BASE_URL = https://cms-svc.isecl.svc.cluster.local:8445/cms/v1 CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> CMS_K8S_ENDPOINT_URL = https://<k8s control-plane IP>:30445/cms/v1 # authservice AAS_API_URL = https://aas-svc.isecl.svc.cluster.local:8444/aas/v1 AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_DB_HOSTNAME = aasdb-svc.isecl.svc.cluster.local AAS_DB_PORT = \"5432\" AAS_DB_NAME = aasdb AAS_DB_SSLMODE = verify-full AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> #NATS_ACCOUNT_NAME=ISecL-account # Workload Service WLS_SERVICE_USERNAME = admin@wls WLS_SERVICE_PASSWORD = wlsAdminPass WLS_DB_USERNAME = wlsdbuser WLS_DB_PASSWORD = wlsdbpassword WLS_DB_HOSTNAME = wlsdb-svc.isecl.svc.cluster.local WLS_DB_NAME = wlsdb WLS_DB_PORT = \"5432\" WLS_API_URL = https://wls-svc.isecl.svc.cluster.local:5000/wls/v1 WLS_CERT_SAN_LIST = wls-svc.isecl.svc.cluster.local # Host Verification Service HVS_SERVICE_USERNAME = admin@hvs HVS_SERVICE_PASSWORD = hvsAdminPass HVS_DB_USERNAME = hvsdbuser HVS_DB_PASSWORD = hvsdbpassword HVS_DB_HOSTNAME = hvsdb-svc.isecl.svc.cluster.local HVS_DB_NAME = hvsdb HVS_CERT_SAN_LIST = hvs-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> HVS_DB_PORT = \"5432\" HVS_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2/ #Nats Servers configuration for TA and HVS #NATS_SERVERS=nats://<K8s control-plane IP/Hostname>:30222 # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> # For Kubeadm # K8S_API_SERVER_CERT=/etc/kubernetes/pki/apiserver.crt K8S_API_SERVER_CERT = /etc/kubernetes/pki/apiserver.crt # This is valid for multinode deployment, should be populated once ihub is deployed successfully IHUB_PUB_KEY_PATH = HVS_BASE_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2 # TrustAgent # e.g TA_CERT_SAN_LIST=*.example.com,192.168.1.* TA_CERT_SAN_LIST = TPM_OWNER_SECRET = # Workload Agent WLA_SERVICE_USERNAME = wlauser@wls WLA_SERVICE_PASSWORD = wlaAdminPass # KBS ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_HOSTNAME = <KMIP IP/Hostname> KMIP_SERVER_IP = KMIP_SERVER_PORT = # Retrieve the following KMIP server\u2019s client certificate, client key and root ca certificate from the KMIP server. # This key and certificates will be available in KMIP server, /etc/pykmip is the default path copy them to this system manifests/kbs/kmip-secrets path KMIP_CLIENT_CERT_NAME = client_certificate.pem KMIP_CLIENT_KEY_NAME = client_key.pem KMIP_ROOT_CERT_NAME = root_certificate.pem # ISecl Scheduler # For Kubeadm # K8S_CA_KEY=/etc/kubernetes/pki/ca.key # K8S_CA_CERT=/etc/kubernetes/pki/ca.crt K8S_CA_KEY = /etc/kubernetes/pki/ca.key K8S_CA_CERT = /etc/kubernetes/pki/ca.crt # populate users.env ISECL_INSTALL_COMPONENTS = \"AAS,HVS,WLS,IHUB,KBS,WLA,TA,WPM\" #NATS_CERT_SAN_LIST= #NATS_TLS_COMMON_NAME= GLOBAL_ADMIN_USERNAME = GLOBAL_ADMIN_PASSWORD = INSTALL_ADMIN_USERNAME = INSTALL_ADMIN_PASSWORD = WPM_SERVICE_USERNAME = WPM_SERVICE_PASSWORD = CUSTOM_CLAIMS_COMPONENTS = CCC_ADMIN_USERNAME = CCC_ADMIN_PASSWORD =","title":"Update .env file"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#run-scripts-on-kubernetes-controller-node_1","text":"The following bootstrap scripts are sample scripts to allow for a quick start of Intel SecL services and agents. #Pre-reqs.sh ./pre-requisites.sh #isecl-bootstrap-db-services #Reference #./isecl-bootstrap-db-services.sh: option requires an argument -- h #Usage: ./isecl-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, Workload Service and Host verification Service # purge Delete Database Services for Authservice, Workload Service and Host verification Service ./isecl-bootstrap-db-services.sh up #isecl-bootstrap #Reference #Usage: Usage: ./isecl-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap ISecL K8s environment for specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete ISecL K8s environment for specified agent/service/usecase [will not delete data, config, logs] # purge Delete ISecL K8s environment with data, config, logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of tagent, wlagent # service Can be one of cms, authservice, hvs, ihub, wls, kbs, isecl-controller, isecl-scheduler # usecase Can be one of foundation-security, workload-security, isecl-orchestration-k8s ./isecl-bootstrap.sh up <all/usecase of choice> Copy the ihub_public_key.pem from NFS path - <mnt>/isecl/ihub/config/ihub_public_key.pem to K8s Master Update the isecl-k8s.env for IHUB_PUB_KEY_PATH Bring up the isecl-k8s-scheduler ./isecl-bootstrap.sh up isecl-scheduler Create and update scheduler-policy.json path mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions Configure kube-scheduler to establish communication with isecl-scheduler. Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec : containers : - command : - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers : volumeMounts : - mountPath : /opt/isecl-k8s-extensions/ name : extendedsched readOnly : true volumes : - hostPath : path : /opt/isecl-k8s-extensions/ type : name : extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml Restart kubelet which restart all the k8s services including kube-scheduler systemctl restart kubelet","title":"Run scripts on Kubernetes Controller Node"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#default-service-and-agent-mount-paths","text":"","title":"Default Service and Agent Mount Paths"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#single-node-deployments","text":"Single node Deployments use hostPath mounting pod(container) files directly on the host. #Certificate-Management-Service Config : /etc/cms Logs : /var/log/cms #Authentication Authorization Service Config : /etc/authservice Logs : /var/log/authservice Pg-data : /usr/local/kube/data/authservice/pgdata #Host Attestation Service Config : /etc/hvs Logs : /var/log/hvs Pg-data : /usr/local/kube/data/hvs #Integration-Hub Config : /etc/ihub Log : /var/log/ihub #Workload Service Config : /etc/workload-service Logs : /var/log/workload-service Pg-data : /usr/local/kube/data/workload-service #Key-Broker-Service Config : /etc/kbs Log : /var/log/kbs Opt : /opt/kbs #Trust Agent: Config : /opt/trustagent/configuration Logs : /var/log/trustagent/ tpmrm : /dev/tpmrm0 txt-stat : /usr/sbin/txt-stat ta-hostname-path : /etc/hostname ta-hosts-path : /etc/hosts #Workload Agent: Config : /etc/workload-agent/ Logs : /var/log/workload-agent TA Config : /opt/trustagent/configuration WLA-Socket : /var/run/workload-agent","title":"Single Node Deployments"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#multi-node-deployments","text":"Multi node Deployments use Kubernetes persistent volume and persistent volume claims for mounting pod(container) files on NFS volumes for all services. Agents will use hostPath for persistent storage. F #Certificate-Management-Service Config : <NFS-vol-base-path>/isecl/cms/config Logs : <NFS-vol-base-path>/isecl/cms/logs #Authentication Authorization Service Config : <NFS-vol-base-path>/isecl/aas/config Logs : <NFS-vol-base-path>/isecl/aas/logs Pg-data : <NFS-vol-base-path>/isecl/aas/db #Host Attestation Service Config : <NFS-vol-base-path>/isecl/hvs/config Logs : <NFS-vol-base-path>/isecl/hvs/logs Pg-data : <NFS-vol-base-path>/usr/local/kube/data/hvs #Integration-Hub Config : <NFS-vol-base-path>/isecl/ihub/config Log : <NFS-vol-base-path>/isecl/ihub/logs #Workload Service Config : <NFS-vol-base-path>/isecl/wls/config Logs : <NFS-vol-base-path>/isecl/wls/log Pg-data : <NFS-vol-base-path>/usr/local/kube/data/wls #Key-Broker-Service Config : <NFS-vol-base-path>/isecl/kbs/config Log : <NFS-vol-base-path>/isecl/kbs/logs Opt : <NFS-vol-base-path>/isecl/kbs/kbs/opt #Trust Agent: Config : /opt/trustagent/configuration Logs : /var/log/trustagent/ tpmrm : /dev/tpmrm0 txt-stat : /usr/sbin/txt-stat ta-hostname-path : /etc/hostname ta-hosts-path : /etc/hosts #Workload Agent: Config : /etc/workload-agent/ Logs : /var/log/workload-agent WLA-Socket : /var/run/workload-agent","title":"Multi Node Deployments"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/25Intel%C2%AE%20Security%20Libraries%20Container%20Deployment/#default-service-ports","text":"For both single-node and multi-node deployments, the following ports are used: CMS : 30445 AAS : 30444 HVS : 30443 WLS : 30447 IHUB : None KBS : 30448 K8s-scheduler : 30888 K8s-controller : None TA : 31443 WLA : None","title":"Default Service Ports"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/","text":"Authentication Beginning in the Intel\u00ae SecL-DC 1.6 release, authentication is centrally managed by the Authentication and Authorization Service (AAS). This service uses a Bearer Token authentication method, which replaces the previous HTTP BASIC authentication. This service also centralizes the creation of roles and users, allowing much easier management of users, passwords, and permissions across all Intel\u00ae SecL-DC services. To make an API request to an Intel\u00ae SecL-DC service, an authentication token is now required. API requests must now include an Authorization header with an appropriate token: Authorization: Bearer $TOKEN The token is issued by the AAS and will expire after a set amount of time. This token may be used with any Intel\u00ae SecL-DC service, and will carry the appropriate permissions for the role(s) assigned to the account the token was generated for. Create Token To request a new token from the AAS: POST https://<AAS IP or hostname>:8444/aas/v1/token { \"username\" : \"<username>\", \"password\" : \"<password>\" } The response will be a token that can be used in the Authorization header for other requests. The length of time for which the token will be valid is configured on the AAS using the key AAS\\_JWT\\_TOKEN\\_DURATION\\_MINS (in the installation answer file during installation) or aas.jwt.token.duration.mins (configured on the AAS after installation). In both cases the value is the length of time in minutes that issued tokens will remain valid before expiring. User Management Users in Intel\u00ae SecL-DC are no longer restrained to a specific service, as they are now centrally managed by the Authentication and Authorization Service. Any user may now be assigned roles for any service, allowing user accounts to be fully defined by the tasks needed. Username and Password requirements Passwords have the following constraints: cannot be empty - i.e must at least have one character maximum length of 255 characters Usernames have the following requirements: Format: username[@host_name[domain]] [@host_name[domain]] is optional username shall be minimum of 2 and maximum of 255 characters username allowed characters are alphanumeric, ., -, _ - but cannot start with -. Domain name must meet requirements of a host name or fully qualified internet host name Examples admin, admin_wls, admin@wls, admin@wls.intel.com , wls-admin@intel.com Create User POST h tt ps : //<IP or hos tna me o f AAS> : 8444 /aas/v 1 /users Au t horiza t io n : Bearer < t oke n > { \"username\" : \"<username>\" , \"password\" : \"<password>\" } Search Users by Username GET h tt ps : //<IP or hos tna me o f AAS> : 8444 /aas/v 1 /users? na me=<value> Change User Password PATCH h tt ps : //<IP or hos tna me o f AAS> : 8444 /aas/v 1 /users/cha n gepassword Au t horiza t io n : Bearer < t oke n > { \"username\" : \"<username>\" , \"old_password\" : \"<old_password>\" , \"new_password\" : \"<new_password>\" , \"password_confirm\" : \"<new_password>\" } Delete User DELETE h tt ps : //<IP or hos tna me o f AAS> : 8444 /aas/v 1 /users/<User ID> Au t horiza t io n : Bearer < t oke n > Roles and Permissions Permissions in Intel\u00ae SecL-DC are managed by Roles. Roles are a set of predefined permissions applicable to a specific service. Any number of Roles may be applied to a User. While new Roles can be created, each Intel\u00ae SecL service defines permissions that are applicable to specific predetermined Roles. This means that only pre-defined Roles will actually have any permissions. Role creation is intended to allow Intel\u00ae SecL-DC services to define their permissions while allowing role and user management to be centrally managed on the AAS. When a new service is installed, it will use the Role creation functions to define roles applicable for that service in the AAS. Create Role POST h tt ps : //<AAS IP or Hos tna me> : 8444 /aas/v 1 /roles Au t horiza t io n : Bearer < t oke n > { \"service\" : \"<Service name>\" , \"name\" : \"<Role Name>\" , \"permissions\" : [ <array o f permissio ns > ] } service field contains a minimum of 1 and maximum of 20 characters. Allowed characters are alphanumeric plus the special charecters -, _, @, ., , name field contains a minimum of 1 and maximum of 40 characters. Allowed characters are alphanumeric plus the special characters -, _, @, ., , service and name fields are mandatory context field is optional and can contain up to 512 characters. Allowed characters are alphanumeric plus -, _, @, ., ,,=,;,:,* permissions field is optional and allow up to a maximum of 512 characters. The Permissions array must a comma-separated list of permissions formatted as resource:action: Permissions required to execute specific API requests are listed with the API resource and method definitions in the API documentation. Search Roles GET h tt ps : //<AAS IP or Hos tna me> : 8444 /aas/v 1 /roles?<parame ter >=<value> Au t horiza t io n : Bearer < t oke n > Search parameters supported: Service=< na me o f service> Name=<role na me> Co nte x t =<co nte x t > co nte x t Co nta i ns =<par t ial \"context\" s tr i n g> allCo nte x ts =< true or false > f il ter = false Delete Role DELETE h tt ps : //<AAS IP or Hos tna me> : 8444 /aas/v 1 /roles/<role ID> Au t horiza t io n : Bearer < t oke n > Assign Role to User POST h tt ps : //<AAS IP or Hos tna me> : 8444 /aas/v 1 /users/<user ID>/roles Au t horiza t io n : Bearer < t oke n > { \"role_ids\" : [ \"<comma-separated list of role IDs>\" ] } List Roles Assigned to User GET h tt ps : //<AAS IP or Hos tna me\\> : 8444 /aas/v 1 /users/<user ID>/roles Au t horiza t io n : Bearer < t oke n > Remove Role from User DELETE h tt ps : //<AAS IP or Hos tna me> : 8444 /aas/v 1 /users/<userID>/roles/<role ID> Au t horiza t io n : Bearer < t oke n > Role Definitions The following roles are created during installation (or by the CreateUsers script) and exist by default. Role Name Permissions Utility TA:Administrator TA:*:* Used by the Verification Service to access Trust Agent APIs, including retrieval of TPM quotes, provisioning Asset Tags and SOFTWARE Flavors, etc. HVS:ReportSearcher HVS: [reports:search:*\"] Used by the Integration Hub to retrieve attestation reports from the Verification Service KBS:Keymanager KBS: [\"keys:create:*\", \"keys:transfer:*\"] Used by the WPM to create and retrieve symmetric encryption keys to encrypt workload images WLS:FlavorsImageRetrieval WLS: image_flavors:retrieve:* Used by the Workload Agent during Workload Confidentiality flows to retrieve the image Flavor HVS: ReportCreator HVS: [\"reports:create:*\"] Used by the Workload Service to create new attestation reports on the Verification Service as part of Workload Confidentiality key retrievals. Administrator *:*:* Global administrator role used for the initial administrator account. This role has all permissions across all services, including permissions to create new roles and users. AAS: Administrator *:*:* Administrator role for the AAS only. Has all permissions for AAS resources, including the ability to create or delete users and roles. AAS: RoleManager AAS: [roles:create:*, roles:retrieve:*, roles:search:*, roles:delete:*] AAS role that allows all actions for Roles, but cannot create or delete Users or assign Roles to Users. AAS: UserManager AAS: [users:create:*, users:retrieve:*, users:store:*, users:search:*, users:delete:*] AAS role with all permissions for Users, but has no ability to create Roles or assign Roles to Users. AAS: UserRoleManager AAS: [user_roles:create:*, user_roles:retrieve:*, user_roles:search:*, user_roles:delete:*, AAS role with permissions to assign Roles to Users, but cannot create delete or modify Users or Roles. HVS: AttestationRegister HVS: [host_tls_policies:create:*, hosts:create:*, hosts:store:*, hosts:search:*, host_unique_flavors:create:*, flavors:search:*, tpm_passwords:retrieve:*, tpm_passwords:create:*, host_aiks:certify:* Role used for Trust Agent provisioning. Used to create the installation token provided during installation. HVS: Certifier HVS: host_signing_key_certificates:create:* Used for installation of the Workload Agent","title":"Authentication"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#authentication","text":"Beginning in the Intel\u00ae SecL-DC 1.6 release, authentication is centrally managed by the Authentication and Authorization Service (AAS). This service uses a Bearer Token authentication method, which replaces the previous HTTP BASIC authentication. This service also centralizes the creation of roles and users, allowing much easier management of users, passwords, and permissions across all Intel\u00ae SecL-DC services. To make an API request to an Intel\u00ae SecL-DC service, an authentication token is now required. API requests must now include an Authorization header with an appropriate token: Authorization: Bearer $TOKEN The token is issued by the AAS and will expire after a set amount of time. This token may be used with any Intel\u00ae SecL-DC service, and will carry the appropriate permissions for the role(s) assigned to the account the token was generated for.","title":"Authentication"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#create-token","text":"To request a new token from the AAS: POST https://<AAS IP or hostname>:8444/aas/v1/token { \"username\" : \"<username>\", \"password\" : \"<password>\" } The response will be a token that can be used in the Authorization header for other requests. The length of time for which the token will be valid is configured on the AAS using the key AAS\\_JWT\\_TOKEN\\_DURATION\\_MINS (in the installation answer file during installation) or aas.jwt.token.duration.mins (configured on the AAS after installation). In both cases the value is the length of time in minutes that issued tokens will remain valid before expiring.","title":"Create Token"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#user-management","text":"Users in Intel\u00ae SecL-DC are no longer restrained to a specific service, as they are now centrally managed by the Authentication and Authorization Service. Any user may now be assigned roles for any service, allowing user accounts to be fully defined by the tasks needed.","title":"User Management"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#username-and-password-requirements","text":"Passwords have the following constraints: cannot be empty - i.e must at least have one character maximum length of 255 characters Usernames have the following requirements: Format: username[@host_name[domain]] [@host_name[domain]] is optional username shall be minimum of 2 and maximum of 255 characters username allowed characters are alphanumeric, ., -, _ - but cannot start with -. Domain name must meet requirements of a host name or fully qualified internet host name Examples admin, admin_wls, admin@wls, admin@wls.intel.com , wls-admin@intel.com","title":"Username and Password requirements"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#create-user","text":"POST h tt ps : //<IP or hos tna me o f AAS> : 8444 /aas/v 1 /users Au t horiza t io n : Bearer < t oke n > { \"username\" : \"<username>\" , \"password\" : \"<password>\" }","title":"Create User"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#search-users-by-username","text":"GET h tt ps : //<IP or hos tna me o f AAS> : 8444 /aas/v 1 /users? na me=<value>","title":"Search Users by Username"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#change-user-password","text":"PATCH h tt ps : //<IP or hos tna me o f AAS> : 8444 /aas/v 1 /users/cha n gepassword Au t horiza t io n : Bearer < t oke n > { \"username\" : \"<username>\" , \"old_password\" : \"<old_password>\" , \"new_password\" : \"<new_password>\" , \"password_confirm\" : \"<new_password>\" }","title":"Change User Password"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#delete-user","text":"DELETE h tt ps : //<IP or hos tna me o f AAS> : 8444 /aas/v 1 /users/<User ID> Au t horiza t io n : Bearer < t oke n >","title":"Delete User"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#roles-and-permissions","text":"Permissions in Intel\u00ae SecL-DC are managed by Roles. Roles are a set of predefined permissions applicable to a specific service. Any number of Roles may be applied to a User. While new Roles can be created, each Intel\u00ae SecL service defines permissions that are applicable to specific predetermined Roles. This means that only pre-defined Roles will actually have any permissions. Role creation is intended to allow Intel\u00ae SecL-DC services to define their permissions while allowing role and user management to be centrally managed on the AAS. When a new service is installed, it will use the Role creation functions to define roles applicable for that service in the AAS.","title":"Roles and Permissions"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#create-role","text":"POST h tt ps : //<AAS IP or Hos tna me> : 8444 /aas/v 1 /roles Au t horiza t io n : Bearer < t oke n > { \"service\" : \"<Service name>\" , \"name\" : \"<Role Name>\" , \"permissions\" : [ <array o f permissio ns > ] } service field contains a minimum of 1 and maximum of 20 characters. Allowed characters are alphanumeric plus the special charecters -, _, @, ., , name field contains a minimum of 1 and maximum of 40 characters. Allowed characters are alphanumeric plus the special characters -, _, @, ., , service and name fields are mandatory context field is optional and can contain up to 512 characters. Allowed characters are alphanumeric plus -, _, @, ., ,,=,;,:,* permissions field is optional and allow up to a maximum of 512 characters. The Permissions array must a comma-separated list of permissions formatted as resource:action: Permissions required to execute specific API requests are listed with the API resource and method definitions in the API documentation.","title":"Create Role"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#search-roles","text":"GET h tt ps : //<AAS IP or Hos tna me> : 8444 /aas/v 1 /roles?<parame ter >=<value> Au t horiza t io n : Bearer < t oke n > Search parameters supported: Service=< na me o f service> Name=<role na me> Co nte x t =<co nte x t > co nte x t Co nta i ns =<par t ial \"context\" s tr i n g> allCo nte x ts =< true or false > f il ter = false","title":"Search Roles"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#delete-role","text":"DELETE h tt ps : //<AAS IP or Hos tna me> : 8444 /aas/v 1 /roles/<role ID> Au t horiza t io n : Bearer < t oke n >","title":"Delete Role"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#assign-role-to-user","text":"POST h tt ps : //<AAS IP or Hos tna me> : 8444 /aas/v 1 /users/<user ID>/roles Au t horiza t io n : Bearer < t oke n > { \"role_ids\" : [ \"<comma-separated list of role IDs>\" ] }","title":"Assign Role to User"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#list-roles-assigned-to-user","text":"GET h tt ps : //<AAS IP or Hos tna me\\> : 8444 /aas/v 1 /users/<user ID>/roles Au t horiza t io n : Bearer < t oke n >","title":"List Roles Assigned to User"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#remove-role-from-user","text":"DELETE h tt ps : //<AAS IP or Hos tna me> : 8444 /aas/v 1 /users/<userID>/roles/<role ID> Au t horiza t io n : Bearer < t oke n >","title":"Remove Role from User"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/26Authentication/#role-definitions","text":"The following roles are created during installation (or by the CreateUsers script) and exist by default. Role Name Permissions Utility TA:Administrator TA:*:* Used by the Verification Service to access Trust Agent APIs, including retrieval of TPM quotes, provisioning Asset Tags and SOFTWARE Flavors, etc. HVS:ReportSearcher HVS: [reports:search:*\"] Used by the Integration Hub to retrieve attestation reports from the Verification Service KBS:Keymanager KBS: [\"keys:create:*\", \"keys:transfer:*\"] Used by the WPM to create and retrieve symmetric encryption keys to encrypt workload images WLS:FlavorsImageRetrieval WLS: image_flavors:retrieve:* Used by the Workload Agent during Workload Confidentiality flows to retrieve the image Flavor HVS: ReportCreator HVS: [\"reports:create:*\"] Used by the Workload Service to create new attestation reports on the Verification Service as part of Workload Confidentiality key retrievals. Administrator *:*:* Global administrator role used for the initial administrator account. This role has all permissions across all services, including permissions to create new roles and users. AAS: Administrator *:*:* Administrator role for the AAS only. Has all permissions for AAS resources, including the ability to create or delete users and roles. AAS: RoleManager AAS: [roles:create:*, roles:retrieve:*, roles:search:*, roles:delete:*] AAS role that allows all actions for Roles, but cannot create or delete Users or assign Roles to Users. AAS: UserManager AAS: [users:create:*, users:retrieve:*, users:store:*, users:search:*, users:delete:*] AAS role with all permissions for Users, but has no ability to create Roles or assign Roles to Users. AAS: UserRoleManager AAS: [user_roles:create:*, user_roles:retrieve:*, user_roles:search:*, user_roles:delete:*, AAS role with permissions to assign Roles to Users, but cannot create delete or modify Users or Roles. HVS: AttestationRegister HVS: [host_tls_policies:create:*, hosts:create:*, hosts:store:*, hosts:search:*, host_unique_flavors:create:*, flavors:search:*, tpm_passwords:retrieve:*, tpm_passwords:create:*, host_aiks:certify:* Role used for Trust Agent provisioning. Used to create the installation token provided during installation. HVS: Certifier HVS: host_signing_key_certificates:create:* Used for installation of the Workload Agent","title":"Role Definitions"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/27Connection%20Strings/","text":"Connection Strings Connection Strings define a remote API resource endpoint that will be used to communicate with the registered host for retrieving TPM quotes and other host information. Connection Strings differ based on the type of host. Trust Agent The Trust Agent connection string connects directly to the Trust Agent on a given host. The Verification Service will use a service account with the needed Trust Agent permissions to connect to the Trust Agent. In previous Intel\u00ae SecL versions, each Trust Agent had its own unique user access controls. Starting in the 1.6 release, all authentication has been centralized with the new Authentication and Authorization Service, eliminating the need for credentials to be provided for connection strings connecting to Trust Agent resources. By default, the Trust Agent uses \"HTTP\" mode, which uses the following connection string: intel:https://<HostNameOrIp>:1443 The Trust Agent can also be used in NATS mode, which uses a slightly different connection string: intel:nats://<unique host identifier, configured at Trust Agent installation> The unique host identifier is a unique ID used by NATS to differentiate services when passing messages. Any unique string is acceptable, but good examples can be the host's FQDN or hardware UUID. VMware ESXi Importing VMware TLS Certificates Before connecting to vCenter to register hosts or clusters, the vCenter TLS certificate needs to be imported to the Verification Service. This must be done for each vCenter server that the Verification Service will connect to, for importing Flavors or registering hosts. Download the root CA certs from vCenter: wget --no-proxy \"*\" https://<vCenter IP or hostname>/certs/download.zip --no-check-certificate This downloads all the root CA certificates for you into download.zip file. unzip download.zip All of the certificates will be stored under <pwd>/certs/ . Certs will be in PEM format. Upload the certificates to the HVS POST h tt ps : //% 3 CIP% 3E : 8443 /hvs/v 2 /ca - cer t i f ica tes { \"name\" : \"<cert name>\" , \"type\" : \"root\" , \"certificate\" : \"MIIELTCCAxW...\" } Note Please make sure that the certificate does not contain any other characters other than the base64 characters like that of \\n or -----BEGIN CERTIFICATE----- etc. After upload is successful, restart the HVS hvs restart Registering a VMware ESXi Host The VMware ESXi connection string is actually directed to vCenter, not the actual ESXi host. Many ESXi hosts managed by the same vCenter server will use the same connection string. The username and password specified are vCenter credentials, and the vCenter \"Validate Session\" privilege is required for access. vmware:https://<vCenterHostNameOrIp>:443/sdk ; h = <hostname of ESXi host> ; u = <username> ; p = <password>","title":"Connection Strings"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/27Connection%20Strings/#connection-strings","text":"Connection Strings define a remote API resource endpoint that will be used to communicate with the registered host for retrieving TPM quotes and other host information. Connection Strings differ based on the type of host.","title":"Connection Strings"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/27Connection%20Strings/#trust-agent","text":"The Trust Agent connection string connects directly to the Trust Agent on a given host. The Verification Service will use a service account with the needed Trust Agent permissions to connect to the Trust Agent. In previous Intel\u00ae SecL versions, each Trust Agent had its own unique user access controls. Starting in the 1.6 release, all authentication has been centralized with the new Authentication and Authorization Service, eliminating the need for credentials to be provided for connection strings connecting to Trust Agent resources. By default, the Trust Agent uses \"HTTP\" mode, which uses the following connection string: intel:https://<HostNameOrIp>:1443 The Trust Agent can also be used in NATS mode, which uses a slightly different connection string: intel:nats://<unique host identifier, configured at Trust Agent installation> The unique host identifier is a unique ID used by NATS to differentiate services when passing messages. Any unique string is acceptable, but good examples can be the host's FQDN or hardware UUID.","title":"Trust Agent"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/27Connection%20Strings/#vmware-esxi","text":"","title":"VMware ESXi"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/27Connection%20Strings/#importing-vmware-tls-certificates","text":"Before connecting to vCenter to register hosts or clusters, the vCenter TLS certificate needs to be imported to the Verification Service. This must be done for each vCenter server that the Verification Service will connect to, for importing Flavors or registering hosts. Download the root CA certs from vCenter: wget --no-proxy \"*\" https://<vCenter IP or hostname>/certs/download.zip --no-check-certificate This downloads all the root CA certificates for you into download.zip file. unzip download.zip All of the certificates will be stored under <pwd>/certs/ . Certs will be in PEM format. Upload the certificates to the HVS POST h tt ps : //% 3 CIP% 3E : 8443 /hvs/v 2 /ca - cer t i f ica tes { \"name\" : \"<cert name>\" , \"type\" : \"root\" , \"certificate\" : \"MIIELTCCAxW...\" } Note Please make sure that the certificate does not contain any other characters other than the base64 characters like that of \\n or -----BEGIN CERTIFICATE----- etc. After upload is successful, restart the HVS hvs restart","title":"Importing VMware TLS Certificates"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/27Connection%20Strings/#registering-a-vmware-esxi-host","text":"The VMware ESXi connection string is actually directed to vCenter, not the actual ESXi host. Many ESXi hosts managed by the same vCenter server will use the same connection string. The username and password specified are vCenter credentials, and the vCenter \"Validate Session\" privilege is required for access. vmware:https://<vCenterHostNameOrIp>:443/sdk ; h = <hostname of ESXi host> ; u = <username> ; p = <password>","title":"Registering a VMware ESXi Host"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/","text":"Platform Integrity Attestation Platform attestation is the cornerstone use case for ISecL. Platform attestation involves taking measurements of system components during system boot, and then cryptographically verifying that the actual measurements taken matched a set of expected or approved values, ensuring that the measured components were in an acceptable or \" trusted \" state at the time of the last system boot. ISecL leverages the Trusted Compute Group specification for a trusted boot process, extending measurements of platform components to registers in a Trusted Platform Module, and securely generating quotes of those measurements from the TPM for remote comparison to expected values (attestation). This section includes basic REST API examples for these workflows. See the Javadoc for more detailed documentation on REST APIs supported by ISecL. Typical workflows in the datacenter might include: Creating a set of acceptable flavors for attestation with automatic flavor matching that represent the known-good measurements for acceptable BIOS and OS versions in the datacenter Registering hosts for attestation with automatic flavor matching Upgrading hosts in the datacenter to a new BIOS or OS version Removing hosts from the Verification Service Removing flavors Provisioning asset tags to hosts Invalidating asset tags Retrieving current attestation reports Retrieving current host state information Remediating an untrusted attestation Host Registration Registration creates a host record with connectivity details and other host information in the Verification Service database. This host record will be used by the Verification Service to retrieve TPM attestation quotes from the Trust Agent to generate an attestation report. Trust Agent Registration via Trust Agent Command Line The Trust Agent can register the host with a Verification Service by running the following command: tagent setup create-host Note The following environment variables must be exported for this command: HVS_URL = https://<hvs_IP/hostname>:8443/hvs/v2 CURRENT_IP = <trustagent_IP> BEARER_TOKEN = <authentication token> Note Because VMWare ESXi hosts do not use a Trust Agent, this method is not applicable for registration of ESXi hosts. Registration via Verification Service API Any Trust Agent or VMware ESXi host/cluster can be registered using a Verification Service API request. Registration can be performed with or without a set of existing Flavors. Rules for Flavor matching can be set by using the Flavor Group in the request; if no Flavor Group is specified, the automatic Flavor Group will be used. See the Flavor Management section for additional details on Flavors, Flavor Groups, and Flavor matching. Sample Call POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 /hos ts Au t horiza t io n : Bearer < t oke n > { \"host_name\" : \"<hostname of host to be registered>\" , \"connection_string\" : \"<connection string>\" , \"flavorgroup_name\" : \"\" , \"description\" : \"<description>\" } Requires the permission hosts:create Sample Call for ESXi Cluster Registration POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 /hos ts Au t horiza t io n : Bearer < t oke n > { \" esxi_clusters\" : [ { \"connection_string\" : \"<password>\" , \"cluster_name\" : \"<cluster name>\" } ] } Requires the permission esxi_clusters:create Flavor Creation for Automatic Flavor Matching Flavor creation is the process of adding one or more sets of acceptable measurements to the Verification Service database. These measurements correspond to specific system components, and are used as the basis of comparison to generate trust attestations. Flavors can be created manually, or can be imported from an example host. Flavors are automatically matched to hosts based on the Flavorgroup used by the host and the Flavors, and the Flavor Match Policies of the Flavorgroup. The ISecL Verification Service creates a default Flavorgroups during installation called \" automatic \" This Flavorgroup is configured to be used as a pool of all acceptable Flavors in a given environment, and will automatically match the appropriate Flavor parts to the correct host. This Flavorgroup is used by default and is expected to be useful for the majority of deployments. If no Flavorgroup is specified when creating a Flavor, it will be placed in the \" automatic \" Flavorgroup. Flavors are also divided into Flavor parts, which correspond to the PLATFORM , OS , HOST_UNIQUE , SOFTWARE , and ASSET_TAG measurements. These can be created and maintained separately (so that users can manage acceptable OS and BIOS versions, rather than entire host configurations). By default, if not specified, the Verification Service will import Flavors as separate Flavor parts, as appropriate for the host type. By using individual Flavor parts, individual versions of OS or PLATFORM measurements can be managed and automatically mapped. Whenever a host changes states (Untrusted, Connected, etc.) the Verification Service will attempt to match appropriate Flavors to that host. If a Flavor is removed or added, all appropriate hosts will be updated to use the new Flavor, or to no longer use the deleted Flavor. Hosts that are currently using a BIOS where that BIOS versions\u2019 PLATFORM Flavor was deleted will now appear Untrusted, for example. This can be used to easily flag as Untrusted hosts that are using software that has been End-Of-Lifed, or perhaps an OS kernel with a known security vulnerability. Note See the Flavor Management section for additional details on how flavors can be managed, and how the Flavor matching engine works. The sample workflow provided here is intended to be an introduction only. Importing a Flavor from a Sample Host POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"<connection string>\" , \"partial_flavor_types\" : [ \"PLATFORM\" , \"OS\" , \"HOST_UNIQUE\" ], \"flavorgroup_names\" : [] } Requires the permission flavors:create Note The HOST_UNIQUE Flavor parts, used by Red Hat Enterprise Linux and VMWare ESXi host types, MUST be created for each registered host of that type, and should in general be imported from that host. This means that importing the HOST_UNIQUE flavor should always be done for each host registered. To import ONLY the HOST_UNIQUE Flavor part from a host: POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"<connection string>\" , \"partial_flavor_types\" : [ \"HOST_UNIQUE\" ], fla vorgroup_ na mes\": [] } Requires the permission flavors:create Creating a Flavor Manually Flavors can be directly created (rather than importing from a sample host) if the required information is known. If no Flavorgroup is specified, the Flavor will be placed in the automatic group. Note that the label is a required field and must be unique. POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"\" , \"flavor_collection\" : { \"flavors\" : [ { \"meta\" : { \"vendor\" : \"INTEL\" , \"description\" : { \"flavor_part\" : \"PLATFORM\" , \"label\" : \"Intel Corporation_SE5C610.86B.01.01.1008.031920151331_TPM2.0\" , \"bios_name\" : \"Intel Corporation\" , \"bios_version\" : \"SE5C620.86B.00.01.0004.071220170215\" , \"tpm_version\" : \"2.0\" } }, \"hardware\" : { \"processor_info\" : \"\u2026\" , \"processor_flags\" : \"\u2026\" , \"feature\" : { \"tpm\" : { \"enabled\" : true , \"pcr_banks\" : [ \"SHA1\" , \"SHA256\" ] }, \"txt\" : { \"enabled\" : true } } }, \"pcrs\" : { \"SHA1\" : { \"pcr_0\" : { \"value\" : \"d2ed125942726641a7260c4f92beb67d531a0def\" }, \"pcr_17\" : { \"value\" : \"1ec12004b371e3afd43d04155abde7476a3794fa\" , \"event\" : ... } Requires the permission flavors:create Creating the Default SOFTWARE Flavor (Linux Only) As part of the new Application Integrity feature added in Intel\u00ae SecL-DC version 1.5, a new default SOFTWARE Flavor part is provided so that the Linux Trust Agent itself can be measured and included in the attestation process. The default SOFTWARE Flavor includes a manifest for the static files and folders in the Trust Agent. The manifest is automatically deployed to each Linux Trust Agent during the provisioning step. Note The Linux Trust Agent must be rebooted after the Provisioning step is completed (typically Provisioning happens during installation, based on whether all of the required variables are set in the trustagent.env file). Rebooting allows the default SOFTWARE Flavor manifest to be measured and extended to the TPM PCRs. If the reboot is not performed, the system will require a SOFTWARE Flavor, but the measurements will not exist, and the system will appear Untrusted. If an un-rebooted host is used to create the SOFTWARE Flavor, the Flavor will be created based on measurements that do not exist, and will fail. The SOFTWARE Flavor part should be created separately from the other Flavor parts. Only one default SOFTWARE Flavor needs to be created for each version of the Linux Trust Agent. If the SOFTWARE Flavor for the same Trust Agent version is imported multiple times, subsequent imports will fail as the Flavor already exists. To import the SOFTWARE Flavor part from a host: POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"<connection string>\" , \"partial_flavor_types\" : [ \"SOFTWARE\" ], fla vorgroup_ na mes\": [] } Requires the permission flavors:create Creating and Provisioning Asset Tags Asset Tags represent a set of key/value pairs that can be associated with a host in hardware. This enables usages around restricting workflows to specific hosts based on tags, which could include location information, compliance tags, etc. ISecL creates Asset Tags by creating a certificate containing the list of key/value pairs to be tagged to the host, with the host\u2019s hardware UUID as the certificate subject. A hash of this certificate is then written to an NVRAM index in the host\u2019s TPM. This value is included in TPM quotes, and can be attested using an Asset Tag flavor that matches up the expected value and the actual key/value pairs. Creating Asset Tag Certificates Asset Tag certificates can be created with a single REST API call, with any number of key/value pairs. Note that one certificate must be created for each host to be tagged, even if they will all be tagged with identical key/value pairs. POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / ta g - cer t i f ica tes Au t horiza t io n : Bearer < t oke n > { \"hardware_uuid\" : \"<hardware UUID of host to be tagged>\" , \"selection_content\" : [ { \"name\" : \"<key>\" , \"value\" : \"<value>\" }, { \"name\" : \"<key>\" , \"value\" : \"<value>\" }, { \"name\" : \"<key>\" , \"value\" : \"<value>\" } ] } Deploying Asset Tags Red Hat Enterprise Linux Asset Tags can be provisioned to a Windows or RHEL host via a REST API request on the Verification Service that will in turn make a request to the Trust Agent on the host to be tagged. POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 /rpc/deploy - ta g - cer t i f ica te Au t horiza t io n : Bearer < t oke n > { \"certificate_id\" : \"<certificate ID>\" } VMWare Since VMWare ESXi hosts do not use a Trust Agent, the process for writing Asset Tags to a VMWare host is different from RHEL. A new interface has been added to ESXi via a new esxcli command starting in vSphere 6.5 Update 2 that allows the Asset Tag information to be written to the TPM via a command-line command. The older process is also described below. The high-level workflow for using Asset Tags with VMWare ESXi is: Create the Asset Tag Certificate for the host. Calculate the Certificate Hash value. Provision the Certificate Hash value to the host TPM and reboot Create the Asset Tag Flavor. Note Asset Tag is currently not supported for VMWare hosts using TPM 2.0. Calculate the Certificate Hash Value Only the hash value of the Asset Tag Certificate can be provisioned to the TPM, due to the low size of the NVRAM. Retrieve the Asset Tag Certificate. The Asset Tag Certificate can be retrieved either from the response when the Asset Tag certificate is created, or by using a GET API request to retrieve the certificate: GET h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / ta g - cer t i f ica tes ?subjec t EqualTo=<HardwareUUID> Au t horiza t io n : Bearer < t oke n > Copy only the certificate value (this will be the certificate in encoded format) and write the data to a file on a Linux system. Remove any line breaks and save the file. Assuming the filename used is tag-cert use the following to generate the correct hash: cat tag-cert | base64 --decode | openssl dgst -sha1 | awk -F \" \" '{print $2}' This hash value will be what is actually written to the TPM NVRAM. Provision the Certificate Hash to the Host TPM Due to a new feature added in vSphere 6.5 Update 2, the process for provisioning Asset Tags on VMWare ESXi hosts has been significantly improved. Both the old and new process for provisioning Asset Tags is documented below. Intel recommends using vSphere 6.5 Update 2 or later due to the significant difference in the process. vSphere 6.5 Update 2 or Later Starting in ESXi 6.5u2, you can now use SSH to write Asset Tags directly with no need for TPM clears, reboots, PXE, or BIOS access. SSH to the ESXi host using root credentials. Then use the command: esxcli hardware tpm tag set -d <hash> You can use the following command to verify that the tag was written: esxcli hardware tpm tag get Reboot the host. After rebooting, the TPM PCR 22 will have the measured value of the hash. vSphere 6.5 Update 1 or Older There is no direct interface from VMWare vCenter or ESXi previous to vSphere 6.5 Update 2 that will write the Tag information to the host TPM. Writing Asset Tag information to a TPM requires TPM ownership; VMWare ESXi takes TPM ownership with a secret password at boot time. This means that the process for writing Asset Tags to a VMWare host requires: Clear TPM ownership. This can be done via the system BIOS, or using One Touch Activation through the IPMI interface (if enabled by the server OEM). Reactivate TPM/TXT. This can be done via the system BIOS, or using One Touch Activation through the IPMI interface (if enabled by the server OEM). Booting to an OS that has the ability to issue TPM commands Typically the provisioning OS used is Ubuntu or RHEL, booted temporarily using PXE. Writing the Tag information The TPM index 0x40000010 must be defined, and the hash of the Asset Tag certificate must be written to that index. Clear TPM ownership. This can be done via the system BIOS, or using One Touch Activation through the IPMI interface (if enabled by the server OEM). Reactivate TPM/TXT This can be done via the system BIOS, or using One Touch Activation through the IPMI interface (if enabled by the server OEM). Boot back to VMWare ESXi. When the system is rebooted to ESXi, the Trusted Boot process will extend the value to PCR22, and this value can be used during attestation. Creating the Asset Tag Flavor (VMWare ESXi Only) While for RHEL and Windows hosts the Asset Tag Flavor is automatically created during the Tag Provisioning step, for VMWare ESXi hosts the Flavor must be created by importing it from the host after the Tag has been provisioned. POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"<VMWare vCenter connection string>\" , \"partial_flavor_types\" : [ \"ASSET_TAG\" ] } Once the Asset Tag Flavor is imported, the host can be attested including Asset Tags as normal. Retrieving Current Attestation Reports GET https://verification.service.com:8443/hvs/v2/reports?latestPerHost=true Authorization: Bearer <token> Retrieving Current Host State Information GET https://verification.service.com:8443/hvs/v2/host-status?latestPerHost=true Authorization: Bearer <token> Upgrading Hosts in the Datacenter to a New BIOS or OS Version Software and firmware updates are a common occurrence in the datacenter. Automatic Flavor matching makes this process relatively simple: Create a new Flavor for the new version. This may be manually created or imported directly from a sample host that has already received the upgrade. Be sure to create new Flavors for each TPM version represented in your datacenter. POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"<connection string>\" , \"partial_flavor_types\" : [ \"PLATFORM\" , \"OS\" , \"HOST_UNIQUE\" ], fla vorgroup_ na mes\": [] } Update the hosts to the new software or firmware version as normal. On the next attestation attempt, the Verification Service will automatically match the updated hosts to the new Flavor. (Optional) If desired, delete the Flavor for the older version after the update is completed. This will cause any hosts that are still using the old version to attest as Untrusted. Which can easily flag hosts that missed the upgrade for remediation. DELETE https://verification.service.com:8443/hvs/v2/flavors/<flavorId> Authorization: Bearer <token> Removing Hosts From the Verification Service Hosts can be deleted at any time. Reports for that host will remain in the Verification Service database for audit purposes. DELETE https://verification.service.com:8443/hvs/v2/hosts/<hostId> Authorization: Bearer <token> The hostId can be retrieved either at the time the host is created, or by searching hosts using the host\u2019s hostname. Removing Flavors Flavors can be deleted; this will cause any hosts that match the deleted Flavor to evaluate as Untrusted. This can be done if, for example, an old BIOS version needs to be retired and should no longer exist in the datacenter. By deleting the PLATFORM Flavor, hosts with the old BIOS version will attest as Untrusted, flagging them for easy remediation. DELETE https://verification.service.com:8443/hvs/v2/flavors/<flavorId> Invalidating Asset Tags Asset Tags can be deleted in two ways. Deleting the ASSET_TAG Flavor part will retain the Asset Tag certificate in the database, but will cause the host using this Tag to no longer use the Asset Tag for attestation (the Tag result will be disregarded and no tags will be exposed in the attestation Reports). DELETE https://verification.service.com:8443/hvs/v2/flavors/<assetTagflavorId> Authorization: Bearer <token> Deleting the actual Asset Tag certificate will remove the certificate from the database, but will not actually affect attestation results (the authority for attestation results is the Flavor). DELETE https://verification.service.com:8443/hvs/v2/tag-certificates/<assetTagCertificateId> Authorization: Bearer <token> Remediating an Untrusted attestation Hosts can become Untrusted for a wide variety of causes. The first clue to finding the root cause for an Untrusted attestation is the attestation Report itself \u2013 the Report will show Trust results for the PLATFORM , OS , HOST_UNIQUE , and ASSET_TAG Flavor parts individually, along with the OVERALL trust. If the Report shows that the PLATFORM Flavor part trust is \u201cfalse\u201d for example, it means that the PLATFORM measurements did not match any Flavors in the host\u2019s Flavorgroup. Untrusted attestation Reports will contain faults that describe the specific attestation rules that were not satisfied. This often shows enough information to describe the cause of the Untrusted status. A fault like RequiredButNotDefined means that a Flavor part is required by the Flavorgroup policy, but no Flavors for that Flavor part exist in the Flavorgroup (for example, generally Flavorgroups should always require a PLATFORM Flavor part; if no PLATFORM Flavors are in the Flavorgroup, hosts in the Flavorgroup will attest with this fault). Other faults include: \"PcrMatchesConstant\" - describes a rule that evaluates whether a TPM PCR has a specific value \"PcrEventLogIntegrity\" - the module event log is replayed during attestation to verify that the resulting measurement matches the actual value in the module PCR. If the replay does not match, it indicates the event log cannot itself be trusted. \"AikCertificateTrusted\" \u2013 This rule evaluates whether the TPM quote was signed by the TPM associated with this host. As part of host registration, the public half of the Attestation Identity Keypair is captured, and this public key is used to verify the signature on TPM quotes from that host. See the Appendix for a full list of the rules evaluated during Attestation. The Flavor matching engine will use the most-similar Flavor for the attestation Report in the case of an Untrusted result. The fault will explain in a general sense what rule the host attestation violated. To remediate, the rule will need to be satisfied. This could mean creating a new Flavor to match the actual observed values, or it could mean that the host has been tampered with and should have its BIOS flashed or OS reloaded. Attestation Reporting Attestation results are delivered in the form of Host Reports. A Report can delivered in several different formats, which can change the type of data returned. The preferred format for Host Reports is a SAML attestation. A SAML-formatted report includes a chain or signatures that provides auditability for the Report. The SAML attestation will include the base trust status of the host, as well as the overall trust for each individual Flavor used in the attestation. The Report will also contain host information, such as TPM version, Operating System name and version, BIOS version, etc. The SAML Report will not, however, contain individual measurements and comparisons of values. This format of the Report is ideal for securely communicating the trust status of a host and for audit history. Attestation Reports can also be retrieved in json or xml format. These formats will not include the signature chain provided in the SAML format, but will contain the actual measurement values and expected Flavor values used for comparison. These reports are typically used for remediation, because they will show specifically why a given Host attested as Untrusted. The format for a Report is determined by the Accept header in the request. Attestations are automatically generated in the Verification Service by a repeating scheduled background process. This process looks for Attestation Reports that are close to expiration, and triggers a new Attestation Report. By default, Attestation Reports are valid for 90 minutes, and the background refresh process will trigger a new attestation when a Report is found to be within 3 minutes of expiration. A user can either retrieve the most recent currently valid Attestation Report for a given host, or may trigger a new Attestation Report to be generated. Typically, it is best to retrieve an existing Report for performance reasons. Generating a new Attestation Report requires the generation of a new TPM quote from the TPM of the host being attested; TPM performance differs greatly between vendors, and a quote can take anywhere between 2-7 seconds to generate. Sample Call \u2013 Generating a New Attestation Report POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 /repor ts Au t horiza t io n : Bearer < t oke n > { \"host_name\" : \"host-1\" } Requires the permission reports:create Sample Call \u2013 Retrieving an Existing Attestation Report GET https://verification.service.com:8443/hvs/v2/reports?hostName=HostName.server.com Authorization: Bearer <token> Below are the supported criteria options in order of precedence. If no host filter criteria is specified, then results are returned for all active hosts. id - unique UUID of the report entry in the database hostId - unique UUID of the host entry in the database hostName - name of the host hostHardwareId - hardware UUID of the host hostStatus - current state of the host, which supports the following options: CONNECTED - host is in connected state QUEUE - host is in queue to be processed CONNECTION_FAILURE - connection failure UNAUTHORIZED - unauthorized AIK_NOT_PROVISIONED - AIK certificate is not provisioned EC_NOT_PRESENT - endorsement certificate is not present MEASURED_LAUNCH_FAILURE - TXT measured launch failure TPM_OWNERSHIP_FAILURE - TPM ownership failureTPM_NOT_PRESENT - TPM is not present UNSUPPORTED_TPM - unsupported TPM version UNKNOWN - unknown host state Requires the permissions reports:search Other search criteria may also be used. By default, the most recent currently valid attestation is returned. However, different query parameters can be used to retrieve all attestations for a specific host over the last 30 days, for example. Integration Intel\u00ae SecL can be integrated with scheduler services (or potentially other services) to provide additional security controls. For example, by integrating Intel\u00ae SecL with the OpenStack scheduler service, the OpenStack placement service can incorporate the Intel\u00ae SecL security attributes into VM scheduling. The Integration Hub The Integration Hub acts as the integration point between the Verification Service and a third party service. The primary purpose of the Hub is to collect and maintain up-to-date attestation information, and to \u201cpush\u201d that information to the external service. The secondary purpose is to allow for multitenancy, the Verification Service does not allow for permissions to be applied for specific hosts, so a user with the \u201cattestation\u201d role can access all attestations for all hosts. By using separate Integration Hub instances for each Cloud environment (or \"tenant\"), the Hub will push attestations only for the associated hosts to a given tenant\u2019s integration endpoints. For example, Tenant A is using hosts 1-10 for an OpenStack environment. Tenant B is using hosts 11-15 for a Kubernetes environment. Two Hub instances must be configured, one managing tenant A's OpenStack cluster and a second instance managing Tenant B's Kubernetes environment. Each integration Hub will automatically retrieve the list of hosts used by its configured orchestration endpoint, retrieve the attestation reports only for those hosts, and push the attestation attribute information to each configured endpoint. Neither tenant will have access to the Verification Service, and will not be able to see attestation or other host details regarding infrastructure used by other tenants. Different integration endpoints can be added to the Integration Hub through a plugin architecture. By default, the Integration Hub includes plugins for OpenStack and Kubernetes (Kubernetes deployments require the additional installation of two Intel\u00ae SecL-DC Custom Resource Definitions on the Kube Control Plane). Integration with OpenStack Starting in the Rocky release, OpenStack can now use \u201cTraits\u201d to provide qualitative data about Nova Compute hosts, and to establish Trait requirements for VM instances. The updated scheduler will place VMs requiring a given Trait on Nova Compute nodes that meet the Trait requirements. Intel SecL-DC uses the Integration Hub to continually push platform integrity and Asset Tag information to the OpenStack Traits resources. This means the OpenStack scheduler natively supports workload scheduling incorporating Intel SecL-DC security attributes, including attestation report Trust status and Asset Tags. The OpenStack Placement Service will automatically attempt to place images with Trait requirements on compute nodes that have those Traits. Note This control only applies to instances launched using the OpenStack scheduler, and the Traits functions will not affect manually-launched instances where a specific Compute Node is defined (since this does not use the scheduler at all). Intel SecL-DC uses existing OpenStack interfaces and does not modify OpenStack code. The datacenter owner or OpenStack administrator is responsible for the security of the OpenStack workload scheduling process in general, and Intel recommends following published OpenStack security best practices. Prerequisites Verification Service must be installed and running. OpenStack* Rocky (or later) Nova, Glance, Horizon, and Keystone services must be installed and running The Integration Hub must be installed and running. Setting Image Traits Image Traits define the policy for which Traits are required for that Image to be launched on a Nova Compute node. By setting these Traits to \u201crequired,\u201d the OpenStack scheduler will require these same Traits to be present on a Nova Compute node in order to launch instances of the image. To set the Image Traits for Intel SecL-DC, a specific naming convention is used. This naming convention will match the Traits that the Integration Hub will automatically push to OpenStack. Two types of Traits are currently supported \u2013 one Trait is used to require that the Compute Node be Trusted in the Attestation Report, and the other Trait is used to require specific Asset Tag key/value pairs. To require a Trusted Attestation Report: CUSTOM_ISECL_TRUSTED=required The naming convention for Asset Tags is more flexible, and any number of these Traits can be used simultaneously. Note All of the Traits must be present on the Compute Node for the scheduler to allow instances to land, so be sure not to set mutually exclusive Asset Tag values. CUSTOM_ISECL_AT_TAG_<key>_<value>=required` For example, to define a Trait that will require an Asset Tag where State = CA use the following: CUSTOM_ISECL_AT_TAG_STATE_CA= required These Traits can be set using CLI commands for OpenStack Glance: openstack image set --property trait:CUSTOM_ISECL_AT_STATE__CA = required <image_name> openstack image set --property trait:CUSTOM_ISECL_TRUSTED = required <image_name> To remove a Trait so that it is no longer required for an Image: openstack image unset --property trait:CUSTOM_ISECL_AT_STATE__CA <image_name> openstack image unset --property trait:CUSTOM_ISECL_TRUSTED <image_name> Configuring the Integration Hub for Use with OpenStack The Integration Hub must be configured with the API URLs and credentials for the OpenStack instance it will integrate with. This can be done during installation using the \"OPENSTACK_...\" variables shown in the ihub.env answer file sample (see the Installing the Integration Hub section). However, this configuration can also be performed after installation using CLI commands: ihub setup openstack --endpoint-url=\"http://openstack:5000/v3\" --endpoint-user=\"username\" --endpoint-pass=\"password\" Restart the Integration Hub after configuring the endpoint. Note that \"endpoint name\" should be replaced with any user-friendly name for the OpenStack instance you would prefer. Scheduling Instances Once Trait requirements are set for Images and the Integration Hub is configured to push attributes to OpenStack, instances can be launched in OpenStack as normal. As long as the OpenStack Nova scheduler is used to schedule the workloads, only compliant Compute Nodes will be scheduled to run instances of controlled Images. Note This control only applies to instances launched using the OpenStack scheduler, and the Traits functions will not affect manually-launched instances where a specific Compute Node is defined (since this does not use the scheduler at all). Intel SecL-DC uses existing OpenStack interfaces and does not modify OpenStack code. The datacenter owner or OpenStack administrator is responsible for the security of the OpenStack workload scheduling process in general, and Intel recommends following published OpenStack security best practices. Integration with Kubernetes Through the use of Custom Resource Definitions for the Kubernetes Control Plane, Intel\u00ae Security Libraries can make Kubernetes aware of Intel\u00ae SecL security attributes and make them available for pod orchestration. In this way, a security-sensitive pod can be launched only on Trusted physical worker nodes, or on physical worker nodes that match specified Asset Tag values. Note This control only applies to pods launched using the Kubernetes scheduler, and these scheduling controls will not affect manually-launched instances where a specific worker node is defined (since this does not use the scheduler at all). Intel SecL-DC uses existing Kubernetes interfaces and does not modify Kubernetes code, using only the standard Custom Resource Definition mechanism to add this functionality to the Kubernetes Control Plane. The datacenter owner or Kubernetes administrator is responsible for the security of the Kubernetes workload scheduling process in general, and Intel recommends following published Kubernetes security best practices. Prerequisites Verification Service must be installed and running. Kubernetes Control Plane Node must be installed and running The supported Kubernetes versions are from 1.14.8 - 1.17.3 and the integration is validated with 1.14.8 and 1.17.3 Kubernetes Worker Nodes must be configured as physical hosts and attached to the Control Plane Node Installing the Intel\u00ae SecL Custom Resource Definitions Intel\u00ae SecL uses Custom Resource Definitions to add the ability to base orchestration decisions on Intel\u00ae SecL security attributes to Kubernetes. These CRDs allow Kubernetes administrators to configure pods to require specific security attributes so that the Kubernetes Control Plane Node will schedule those pods only on Worker Nodes that match the specified attributes. Two CRDs are required for integration with Intel\u00ae SecL \u2013 an extension for the Control Plane nodes, and a scheduler extension. A single installer will deploy both of these CRDs. The extensions are deployed as a Kubernetes deployment in the isecl namespace. Note Please refer detail steps given for 3.15 Installing the Intel\u00ae SecL Kubernetes Extensions and Integration Hub section. Configuring Pods to Require Intel\u00ae SecL Attributes (Optional) Verify that the worker nodes have had their Intel\u00ae SecL security attributes populated: kubectl get nodes --show-labels The output should show the Trust staus and any Asset Tags applied to all of the registered Worker Nodes. Add the following to any Pod creation files: spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : isecl.trusted operator : In values : - \"true\" - key : isecl.TAG_Country operator : In values : - CA - US - key : isecl.TAG_Customer operator : In values : - Coke - Pepsi - key : isecl.TAG_State operator : In values : - CA The isecl.trusted key defines the requirement for a Trusted host. Only one of these keys should be used. The isecl.TAG_ keys indicate Asset Tags; if the workload should only launch on hosts with the COUNTRY=US Asset Tag, the pod should be launched with the matchExpression key isecl.TAG_COUNTRY with the value US . All of the matchExpression definitions must be true for a given worker node to launch the pod \u2013 in the example above, the host must be attested as Trusted with Asset Tags Country=US , Customer=Customer1 and State=CA . If the worker node has additional Asset Tags beyond the ones required, the pod will still be able to be launched on that node. However, if one of the specified Tags is missing or has a different value, that worker node will not be used for that pod. Tainting Untrusted Worker Nodes Optionally, the Intel\u00ae SecL Kubernetes CRDs can be configured to flag worker nodes as tainted to prevent any pods from launching on them. This restriction is applied regardless of whether the pod has a specific trust policy \u2013 if a worker node is flagged as tainted no pods will be launched on that worker. This setting is disabled by default. To enable this setting: Edit the isecl-controller.yaml file under /opt/isecl-k8s-extensions/yamls/isecl-controller.yaml and set TAINT_UNTRUSTED_NODES=true Run kubectl apply -f /opt/isecl-k8s-extensions/yamls/isecl-controller.yaml Worker nodes that attest as untrusted will be tainted with the NoExecute flag and unable to launch pods. If a worker was previously considered tainted and the untrusted state is resolved, the Intel\u00ae SecL CRDs will remove the tainted flag and the worker will be able to launch pods again.","title":"Platform Integrity Attestation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#platform-integrity-attestation","text":"Platform attestation is the cornerstone use case for ISecL. Platform attestation involves taking measurements of system components during system boot, and then cryptographically verifying that the actual measurements taken matched a set of expected or approved values, ensuring that the measured components were in an acceptable or \" trusted \" state at the time of the last system boot. ISecL leverages the Trusted Compute Group specification for a trusted boot process, extending measurements of platform components to registers in a Trusted Platform Module, and securely generating quotes of those measurements from the TPM for remote comparison to expected values (attestation). This section includes basic REST API examples for these workflows. See the Javadoc for more detailed documentation on REST APIs supported by ISecL. Typical workflows in the datacenter might include: Creating a set of acceptable flavors for attestation with automatic flavor matching that represent the known-good measurements for acceptable BIOS and OS versions in the datacenter Registering hosts for attestation with automatic flavor matching Upgrading hosts in the datacenter to a new BIOS or OS version Removing hosts from the Verification Service Removing flavors Provisioning asset tags to hosts Invalidating asset tags Retrieving current attestation reports Retrieving current host state information Remediating an untrusted attestation","title":"Platform Integrity Attestation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#host-registration","text":"Registration creates a host record with connectivity details and other host information in the Verification Service database. This host record will be used by the Verification Service to retrieve TPM attestation quotes from the Trust Agent to generate an attestation report.","title":"Host Registration"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#trust-agent","text":"","title":"Trust Agent"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#registration-via-trust-agent-command-line","text":"The Trust Agent can register the host with a Verification Service by running the following command: tagent setup create-host Note The following environment variables must be exported for this command: HVS_URL = https://<hvs_IP/hostname>:8443/hvs/v2 CURRENT_IP = <trustagent_IP> BEARER_TOKEN = <authentication token> Note Because VMWare ESXi hosts do not use a Trust Agent, this method is not applicable for registration of ESXi hosts.","title":"Registration via Trust Agent Command Line"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#registration-via-verification-service-api","text":"Any Trust Agent or VMware ESXi host/cluster can be registered using a Verification Service API request. Registration can be performed with or without a set of existing Flavors. Rules for Flavor matching can be set by using the Flavor Group in the request; if no Flavor Group is specified, the automatic Flavor Group will be used. See the Flavor Management section for additional details on Flavors, Flavor Groups, and Flavor matching.","title":"Registration via Verification Service API"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#sample-call","text":"POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 /hos ts Au t horiza t io n : Bearer < t oke n > { \"host_name\" : \"<hostname of host to be registered>\" , \"connection_string\" : \"<connection string>\" , \"flavorgroup_name\" : \"\" , \"description\" : \"<description>\" } Requires the permission hosts:create","title":"Sample Call"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#sample-call-for-esxi-cluster-registration","text":"POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 /hos ts Au t horiza t io n : Bearer < t oke n > { \" esxi_clusters\" : [ { \"connection_string\" : \"<password>\" , \"cluster_name\" : \"<cluster name>\" } ] } Requires the permission esxi_clusters:create","title":"Sample Call for ESXi Cluster Registration"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#flavor-creation-for-automatic-flavor-matching","text":"Flavor creation is the process of adding one or more sets of acceptable measurements to the Verification Service database. These measurements correspond to specific system components, and are used as the basis of comparison to generate trust attestations. Flavors can be created manually, or can be imported from an example host. Flavors are automatically matched to hosts based on the Flavorgroup used by the host and the Flavors, and the Flavor Match Policies of the Flavorgroup. The ISecL Verification Service creates a default Flavorgroups during installation called \" automatic \" This Flavorgroup is configured to be used as a pool of all acceptable Flavors in a given environment, and will automatically match the appropriate Flavor parts to the correct host. This Flavorgroup is used by default and is expected to be useful for the majority of deployments. If no Flavorgroup is specified when creating a Flavor, it will be placed in the \" automatic \" Flavorgroup. Flavors are also divided into Flavor parts, which correspond to the PLATFORM , OS , HOST_UNIQUE , SOFTWARE , and ASSET_TAG measurements. These can be created and maintained separately (so that users can manage acceptable OS and BIOS versions, rather than entire host configurations). By default, if not specified, the Verification Service will import Flavors as separate Flavor parts, as appropriate for the host type. By using individual Flavor parts, individual versions of OS or PLATFORM measurements can be managed and automatically mapped. Whenever a host changes states (Untrusted, Connected, etc.) the Verification Service will attempt to match appropriate Flavors to that host. If a Flavor is removed or added, all appropriate hosts will be updated to use the new Flavor, or to no longer use the deleted Flavor. Hosts that are currently using a BIOS where that BIOS versions\u2019 PLATFORM Flavor was deleted will now appear Untrusted, for example. This can be used to easily flag as Untrusted hosts that are using software that has been End-Of-Lifed, or perhaps an OS kernel with a known security vulnerability. Note See the Flavor Management section for additional details on how flavors can be managed, and how the Flavor matching engine works. The sample workflow provided here is intended to be an introduction only.","title":"Flavor Creation for Automatic Flavor Matching"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#importing-a-flavor-from-a-sample-host","text":"POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"<connection string>\" , \"partial_flavor_types\" : [ \"PLATFORM\" , \"OS\" , \"HOST_UNIQUE\" ], \"flavorgroup_names\" : [] } Requires the permission flavors:create Note The HOST_UNIQUE Flavor parts, used by Red Hat Enterprise Linux and VMWare ESXi host types, MUST be created for each registered host of that type, and should in general be imported from that host. This means that importing the HOST_UNIQUE flavor should always be done for each host registered. To import ONLY the HOST_UNIQUE Flavor part from a host: POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"<connection string>\" , \"partial_flavor_types\" : [ \"HOST_UNIQUE\" ], fla vorgroup_ na mes\": [] } Requires the permission flavors:create","title":"Importing a Flavor from a Sample Host"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#creating-a-flavor-manually","text":"Flavors can be directly created (rather than importing from a sample host) if the required information is known. If no Flavorgroup is specified, the Flavor will be placed in the automatic group. Note that the label is a required field and must be unique. POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"\" , \"flavor_collection\" : { \"flavors\" : [ { \"meta\" : { \"vendor\" : \"INTEL\" , \"description\" : { \"flavor_part\" : \"PLATFORM\" , \"label\" : \"Intel Corporation_SE5C610.86B.01.01.1008.031920151331_TPM2.0\" , \"bios_name\" : \"Intel Corporation\" , \"bios_version\" : \"SE5C620.86B.00.01.0004.071220170215\" , \"tpm_version\" : \"2.0\" } }, \"hardware\" : { \"processor_info\" : \"\u2026\" , \"processor_flags\" : \"\u2026\" , \"feature\" : { \"tpm\" : { \"enabled\" : true , \"pcr_banks\" : [ \"SHA1\" , \"SHA256\" ] }, \"txt\" : { \"enabled\" : true } } }, \"pcrs\" : { \"SHA1\" : { \"pcr_0\" : { \"value\" : \"d2ed125942726641a7260c4f92beb67d531a0def\" }, \"pcr_17\" : { \"value\" : \"1ec12004b371e3afd43d04155abde7476a3794fa\" , \"event\" : ... } Requires the permission flavors:create","title":"Creating a Flavor Manually"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#creating-the-default-software-flavor-linux-only","text":"As part of the new Application Integrity feature added in Intel\u00ae SecL-DC version 1.5, a new default SOFTWARE Flavor part is provided so that the Linux Trust Agent itself can be measured and included in the attestation process. The default SOFTWARE Flavor includes a manifest for the static files and folders in the Trust Agent. The manifest is automatically deployed to each Linux Trust Agent during the provisioning step. Note The Linux Trust Agent must be rebooted after the Provisioning step is completed (typically Provisioning happens during installation, based on whether all of the required variables are set in the trustagent.env file). Rebooting allows the default SOFTWARE Flavor manifest to be measured and extended to the TPM PCRs. If the reboot is not performed, the system will require a SOFTWARE Flavor, but the measurements will not exist, and the system will appear Untrusted. If an un-rebooted host is used to create the SOFTWARE Flavor, the Flavor will be created based on measurements that do not exist, and will fail. The SOFTWARE Flavor part should be created separately from the other Flavor parts. Only one default SOFTWARE Flavor needs to be created for each version of the Linux Trust Agent. If the SOFTWARE Flavor for the same Trust Agent version is imported multiple times, subsequent imports will fail as the Flavor already exists. To import the SOFTWARE Flavor part from a host: POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"<connection string>\" , \"partial_flavor_types\" : [ \"SOFTWARE\" ], fla vorgroup_ na mes\": [] } Requires the permission flavors:create","title":"Creating the Default SOFTWARE Flavor (Linux Only)"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#creating-and-provisioning-asset-tags","text":"Asset Tags represent a set of key/value pairs that can be associated with a host in hardware. This enables usages around restricting workflows to specific hosts based on tags, which could include location information, compliance tags, etc. ISecL creates Asset Tags by creating a certificate containing the list of key/value pairs to be tagged to the host, with the host\u2019s hardware UUID as the certificate subject. A hash of this certificate is then written to an NVRAM index in the host\u2019s TPM. This value is included in TPM quotes, and can be attested using an Asset Tag flavor that matches up the expected value and the actual key/value pairs.","title":"Creating and Provisioning Asset Tags"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#creating-asset-tag-certificates","text":"Asset Tag certificates can be created with a single REST API call, with any number of key/value pairs. Note that one certificate must be created for each host to be tagged, even if they will all be tagged with identical key/value pairs. POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / ta g - cer t i f ica tes Au t horiza t io n : Bearer < t oke n > { \"hardware_uuid\" : \"<hardware UUID of host to be tagged>\" , \"selection_content\" : [ { \"name\" : \"<key>\" , \"value\" : \"<value>\" }, { \"name\" : \"<key>\" , \"value\" : \"<value>\" }, { \"name\" : \"<key>\" , \"value\" : \"<value>\" } ] }","title":"Creating Asset Tag Certificates"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#deploying-asset-tags","text":"","title":"Deploying Asset Tags"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#red-hat-enterprise-linux","text":"Asset Tags can be provisioned to a Windows or RHEL host via a REST API request on the Verification Service that will in turn make a request to the Trust Agent on the host to be tagged. POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 /rpc/deploy - ta g - cer t i f ica te Au t horiza t io n : Bearer < t oke n > { \"certificate_id\" : \"<certificate ID>\" }","title":"Red Hat Enterprise Linux"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#vmware","text":"Since VMWare ESXi hosts do not use a Trust Agent, the process for writing Asset Tags to a VMWare host is different from RHEL. A new interface has been added to ESXi via a new esxcli command starting in vSphere 6.5 Update 2 that allows the Asset Tag information to be written to the TPM via a command-line command. The older process is also described below. The high-level workflow for using Asset Tags with VMWare ESXi is: Create the Asset Tag Certificate for the host. Calculate the Certificate Hash value. Provision the Certificate Hash value to the host TPM and reboot Create the Asset Tag Flavor. Note Asset Tag is currently not supported for VMWare hosts using TPM 2.0.","title":"VMWare"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#calculate-the-certificate-hash-value","text":"Only the hash value of the Asset Tag Certificate can be provisioned to the TPM, due to the low size of the NVRAM. Retrieve the Asset Tag Certificate. The Asset Tag Certificate can be retrieved either from the response when the Asset Tag certificate is created, or by using a GET API request to retrieve the certificate: GET h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / ta g - cer t i f ica tes ?subjec t EqualTo=<HardwareUUID> Au t horiza t io n : Bearer < t oke n > Copy only the certificate value (this will be the certificate in encoded format) and write the data to a file on a Linux system. Remove any line breaks and save the file. Assuming the filename used is tag-cert use the following to generate the correct hash: cat tag-cert | base64 --decode | openssl dgst -sha1 | awk -F \" \" '{print $2}' This hash value will be what is actually written to the TPM NVRAM.","title":"Calculate the Certificate Hash Value"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#provision-the-certificate-hash-to-the-host-tpm","text":"Due to a new feature added in vSphere 6.5 Update 2, the process for provisioning Asset Tags on VMWare ESXi hosts has been significantly improved. Both the old and new process for provisioning Asset Tags is documented below. Intel recommends using vSphere 6.5 Update 2 or later due to the significant difference in the process.","title":"Provision the Certificate Hash to the Host TPM"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#vsphere-65-update-2-or-later","text":"Starting in ESXi 6.5u2, you can now use SSH to write Asset Tags directly with no need for TPM clears, reboots, PXE, or BIOS access. SSH to the ESXi host using root credentials. Then use the command: esxcli hardware tpm tag set -d <hash> You can use the following command to verify that the tag was written: esxcli hardware tpm tag get Reboot the host. After rebooting, the TPM PCR 22 will have the measured value of the hash.","title":"vSphere 6.5 Update 2 or Later"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#vsphere-65-update-1-or-older","text":"There is no direct interface from VMWare vCenter or ESXi previous to vSphere 6.5 Update 2 that will write the Tag information to the host TPM. Writing Asset Tag information to a TPM requires TPM ownership; VMWare ESXi takes TPM ownership with a secret password at boot time. This means that the process for writing Asset Tags to a VMWare host requires: Clear TPM ownership. This can be done via the system BIOS, or using One Touch Activation through the IPMI interface (if enabled by the server OEM). Reactivate TPM/TXT. This can be done via the system BIOS, or using One Touch Activation through the IPMI interface (if enabled by the server OEM). Booting to an OS that has the ability to issue TPM commands Typically the provisioning OS used is Ubuntu or RHEL, booted temporarily using PXE. Writing the Tag information The TPM index 0x40000010 must be defined, and the hash of the Asset Tag certificate must be written to that index. Clear TPM ownership. This can be done via the system BIOS, or using One Touch Activation through the IPMI interface (if enabled by the server OEM). Reactivate TPM/TXT This can be done via the system BIOS, or using One Touch Activation through the IPMI interface (if enabled by the server OEM). Boot back to VMWare ESXi. When the system is rebooted to ESXi, the Trusted Boot process will extend the value to PCR22, and this value can be used during attestation.","title":"vSphere 6.5 Update 1 or Older"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#creating-the-asset-tag-flavor-vmware-esxi-only","text":"While for RHEL and Windows hosts the Asset Tag Flavor is automatically created during the Tag Provisioning step, for VMWare ESXi hosts the Flavor must be created by importing it from the host after the Tag has been provisioned. POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"<VMWare vCenter connection string>\" , \"partial_flavor_types\" : [ \"ASSET_TAG\" ] } Once the Asset Tag Flavor is imported, the host can be attested including Asset Tags as normal.","title":"Creating the Asset Tag Flavor (VMWare ESXi Only)"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#retrieving-current-attestation-reports","text":"GET https://verification.service.com:8443/hvs/v2/reports?latestPerHost=true Authorization: Bearer <token>","title":"Retrieving Current Attestation Reports"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#retrieving-current-host-state-information","text":"GET https://verification.service.com:8443/hvs/v2/host-status?latestPerHost=true Authorization: Bearer <token>","title":"Retrieving Current Host State Information"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#upgrading-hosts-in-the-datacenter-to-a-new-bios-or-os-version","text":"Software and firmware updates are a common occurrence in the datacenter. Automatic Flavor matching makes this process relatively simple: Create a new Flavor for the new version. This may be manually created or imported directly from a sample host that has already received the upgrade. Be sure to create new Flavors for each TPM version represented in your datacenter. POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 / fla vors Au t horiza t io n : Bearer < t oke n > { \"connection_string\" : \"<connection string>\" , \"partial_flavor_types\" : [ \"PLATFORM\" , \"OS\" , \"HOST_UNIQUE\" ], fla vorgroup_ na mes\": [] } Update the hosts to the new software or firmware version as normal. On the next attestation attempt, the Verification Service will automatically match the updated hosts to the new Flavor. (Optional) If desired, delete the Flavor for the older version after the update is completed. This will cause any hosts that are still using the old version to attest as Untrusted. Which can easily flag hosts that missed the upgrade for remediation. DELETE https://verification.service.com:8443/hvs/v2/flavors/<flavorId> Authorization: Bearer <token>","title":"Upgrading Hosts in the Datacenter to a New BIOS or OS Version"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#removing-hosts-from-the-verification-service","text":"Hosts can be deleted at any time. Reports for that host will remain in the Verification Service database for audit purposes. DELETE https://verification.service.com:8443/hvs/v2/hosts/<hostId> Authorization: Bearer <token> The hostId can be retrieved either at the time the host is created, or by searching hosts using the host\u2019s hostname.","title":"Removing Hosts From the Verification Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#removing-flavors","text":"Flavors can be deleted; this will cause any hosts that match the deleted Flavor to evaluate as Untrusted. This can be done if, for example, an old BIOS version needs to be retired and should no longer exist in the datacenter. By deleting the PLATFORM Flavor, hosts with the old BIOS version will attest as Untrusted, flagging them for easy remediation. DELETE https://verification.service.com:8443/hvs/v2/flavors/<flavorId>","title":"Removing Flavors"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#invalidating-asset-tags","text":"Asset Tags can be deleted in two ways. Deleting the ASSET_TAG Flavor part will retain the Asset Tag certificate in the database, but will cause the host using this Tag to no longer use the Asset Tag for attestation (the Tag result will be disregarded and no tags will be exposed in the attestation Reports). DELETE https://verification.service.com:8443/hvs/v2/flavors/<assetTagflavorId> Authorization: Bearer <token> Deleting the actual Asset Tag certificate will remove the certificate from the database, but will not actually affect attestation results (the authority for attestation results is the Flavor). DELETE https://verification.service.com:8443/hvs/v2/tag-certificates/<assetTagCertificateId> Authorization: Bearer <token>","title":"Invalidating Asset Tags"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#remediating-an-untrusted-attestation","text":"Hosts can become Untrusted for a wide variety of causes. The first clue to finding the root cause for an Untrusted attestation is the attestation Report itself \u2013 the Report will show Trust results for the PLATFORM , OS , HOST_UNIQUE , and ASSET_TAG Flavor parts individually, along with the OVERALL trust. If the Report shows that the PLATFORM Flavor part trust is \u201cfalse\u201d for example, it means that the PLATFORM measurements did not match any Flavors in the host\u2019s Flavorgroup. Untrusted attestation Reports will contain faults that describe the specific attestation rules that were not satisfied. This often shows enough information to describe the cause of the Untrusted status. A fault like RequiredButNotDefined means that a Flavor part is required by the Flavorgroup policy, but no Flavors for that Flavor part exist in the Flavorgroup (for example, generally Flavorgroups should always require a PLATFORM Flavor part; if no PLATFORM Flavors are in the Flavorgroup, hosts in the Flavorgroup will attest with this fault). Other faults include: \"PcrMatchesConstant\" - describes a rule that evaluates whether a TPM PCR has a specific value \"PcrEventLogIntegrity\" - the module event log is replayed during attestation to verify that the resulting measurement matches the actual value in the module PCR. If the replay does not match, it indicates the event log cannot itself be trusted. \"AikCertificateTrusted\" \u2013 This rule evaluates whether the TPM quote was signed by the TPM associated with this host. As part of host registration, the public half of the Attestation Identity Keypair is captured, and this public key is used to verify the signature on TPM quotes from that host. See the Appendix for a full list of the rules evaluated during Attestation. The Flavor matching engine will use the most-similar Flavor for the attestation Report in the case of an Untrusted result. The fault will explain in a general sense what rule the host attestation violated. To remediate, the rule will need to be satisfied. This could mean creating a new Flavor to match the actual observed values, or it could mean that the host has been tampered with and should have its BIOS flashed or OS reloaded.","title":"Remediating an Untrusted attestation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#attestation-reporting","text":"Attestation results are delivered in the form of Host Reports. A Report can delivered in several different formats, which can change the type of data returned. The preferred format for Host Reports is a SAML attestation. A SAML-formatted report includes a chain or signatures that provides auditability for the Report. The SAML attestation will include the base trust status of the host, as well as the overall trust for each individual Flavor used in the attestation. The Report will also contain host information, such as TPM version, Operating System name and version, BIOS version, etc. The SAML Report will not, however, contain individual measurements and comparisons of values. This format of the Report is ideal for securely communicating the trust status of a host and for audit history. Attestation Reports can also be retrieved in json or xml format. These formats will not include the signature chain provided in the SAML format, but will contain the actual measurement values and expected Flavor values used for comparison. These reports are typically used for remediation, because they will show specifically why a given Host attested as Untrusted. The format for a Report is determined by the Accept header in the request. Attestations are automatically generated in the Verification Service by a repeating scheduled background process. This process looks for Attestation Reports that are close to expiration, and triggers a new Attestation Report. By default, Attestation Reports are valid for 90 minutes, and the background refresh process will trigger a new attestation when a Report is found to be within 3 minutes of expiration. A user can either retrieve the most recent currently valid Attestation Report for a given host, or may trigger a new Attestation Report to be generated. Typically, it is best to retrieve an existing Report for performance reasons. Generating a new Attestation Report requires the generation of a new TPM quote from the TPM of the host being attested; TPM performance differs greatly between vendors, and a quote can take anywhere between 2-7 seconds to generate.","title":"Attestation Reporting"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#sample-call-generating-a-new-attestation-report","text":"POST h tt ps : //veri f ica t io n .service.com : 8443 /hvs/v 2 /repor ts Au t horiza t io n : Bearer < t oke n > { \"host_name\" : \"host-1\" } Requires the permission reports:create","title":"Sample Call \u2013 Generating a New Attestation Report"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#sample-call-retrieving-an-existing-attestation-report","text":"GET https://verification.service.com:8443/hvs/v2/reports?hostName=HostName.server.com Authorization: Bearer <token> Below are the supported criteria options in order of precedence. If no host filter criteria is specified, then results are returned for all active hosts. id - unique UUID of the report entry in the database hostId - unique UUID of the host entry in the database hostName - name of the host hostHardwareId - hardware UUID of the host hostStatus - current state of the host, which supports the following options: CONNECTED - host is in connected state QUEUE - host is in queue to be processed CONNECTION_FAILURE - connection failure UNAUTHORIZED - unauthorized AIK_NOT_PROVISIONED - AIK certificate is not provisioned EC_NOT_PRESENT - endorsement certificate is not present MEASURED_LAUNCH_FAILURE - TXT measured launch failure TPM_OWNERSHIP_FAILURE - TPM ownership failureTPM_NOT_PRESENT - TPM is not present UNSUPPORTED_TPM - unsupported TPM version UNKNOWN - unknown host state Requires the permissions reports:search Other search criteria may also be used. By default, the most recent currently valid attestation is returned. However, different query parameters can be used to retrieve all attestations for a specific host over the last 30 days, for example.","title":"Sample Call \u2013 Retrieving an Existing Attestation Report"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#integration","text":"Intel\u00ae SecL can be integrated with scheduler services (or potentially other services) to provide additional security controls. For example, by integrating Intel\u00ae SecL with the OpenStack scheduler service, the OpenStack placement service can incorporate the Intel\u00ae SecL security attributes into VM scheduling.","title":"Integration"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#the-integration-hub","text":"The Integration Hub acts as the integration point between the Verification Service and a third party service. The primary purpose of the Hub is to collect and maintain up-to-date attestation information, and to \u201cpush\u201d that information to the external service. The secondary purpose is to allow for multitenancy, the Verification Service does not allow for permissions to be applied for specific hosts, so a user with the \u201cattestation\u201d role can access all attestations for all hosts. By using separate Integration Hub instances for each Cloud environment (or \"tenant\"), the Hub will push attestations only for the associated hosts to a given tenant\u2019s integration endpoints. For example, Tenant A is using hosts 1-10 for an OpenStack environment. Tenant B is using hosts 11-15 for a Kubernetes environment. Two Hub instances must be configured, one managing tenant A's OpenStack cluster and a second instance managing Tenant B's Kubernetes environment. Each integration Hub will automatically retrieve the list of hosts used by its configured orchestration endpoint, retrieve the attestation reports only for those hosts, and push the attestation attribute information to each configured endpoint. Neither tenant will have access to the Verification Service, and will not be able to see attestation or other host details regarding infrastructure used by other tenants. Different integration endpoints can be added to the Integration Hub through a plugin architecture. By default, the Integration Hub includes plugins for OpenStack and Kubernetes (Kubernetes deployments require the additional installation of two Intel\u00ae SecL-DC Custom Resource Definitions on the Kube Control Plane).","title":"The Integration Hub"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#integration-with-openstack","text":"Starting in the Rocky release, OpenStack can now use \u201cTraits\u201d to provide qualitative data about Nova Compute hosts, and to establish Trait requirements for VM instances. The updated scheduler will place VMs requiring a given Trait on Nova Compute nodes that meet the Trait requirements. Intel SecL-DC uses the Integration Hub to continually push platform integrity and Asset Tag information to the OpenStack Traits resources. This means the OpenStack scheduler natively supports workload scheduling incorporating Intel SecL-DC security attributes, including attestation report Trust status and Asset Tags. The OpenStack Placement Service will automatically attempt to place images with Trait requirements on compute nodes that have those Traits. Note This control only applies to instances launched using the OpenStack scheduler, and the Traits functions will not affect manually-launched instances where a specific Compute Node is defined (since this does not use the scheduler at all). Intel SecL-DC uses existing OpenStack interfaces and does not modify OpenStack code. The datacenter owner or OpenStack administrator is responsible for the security of the OpenStack workload scheduling process in general, and Intel recommends following published OpenStack security best practices.","title":"Integration with OpenStack"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#prerequisites","text":"Verification Service must be installed and running. OpenStack* Rocky (or later) Nova, Glance, Horizon, and Keystone services must be installed and running The Integration Hub must be installed and running.","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#setting-image-traits","text":"Image Traits define the policy for which Traits are required for that Image to be launched on a Nova Compute node. By setting these Traits to \u201crequired,\u201d the OpenStack scheduler will require these same Traits to be present on a Nova Compute node in order to launch instances of the image. To set the Image Traits for Intel SecL-DC, a specific naming convention is used. This naming convention will match the Traits that the Integration Hub will automatically push to OpenStack. Two types of Traits are currently supported \u2013 one Trait is used to require that the Compute Node be Trusted in the Attestation Report, and the other Trait is used to require specific Asset Tag key/value pairs. To require a Trusted Attestation Report: CUSTOM_ISECL_TRUSTED=required The naming convention for Asset Tags is more flexible, and any number of these Traits can be used simultaneously. Note All of the Traits must be present on the Compute Node for the scheduler to allow instances to land, so be sure not to set mutually exclusive Asset Tag values. CUSTOM_ISECL_AT_TAG_<key>_<value>=required` For example, to define a Trait that will require an Asset Tag where State = CA use the following: CUSTOM_ISECL_AT_TAG_STATE_CA= required These Traits can be set using CLI commands for OpenStack Glance: openstack image set --property trait:CUSTOM_ISECL_AT_STATE__CA = required <image_name> openstack image set --property trait:CUSTOM_ISECL_TRUSTED = required <image_name> To remove a Trait so that it is no longer required for an Image: openstack image unset --property trait:CUSTOM_ISECL_AT_STATE__CA <image_name> openstack image unset --property trait:CUSTOM_ISECL_TRUSTED <image_name>","title":"Setting Image Traits"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#configuring-the-integration-hub-for-use-with-openstack","text":"The Integration Hub must be configured with the API URLs and credentials for the OpenStack instance it will integrate with. This can be done during installation using the \"OPENSTACK_...\" variables shown in the ihub.env answer file sample (see the Installing the Integration Hub section). However, this configuration can also be performed after installation using CLI commands: ihub setup openstack --endpoint-url=\"http://openstack:5000/v3\" --endpoint-user=\"username\" --endpoint-pass=\"password\" Restart the Integration Hub after configuring the endpoint. Note that \"endpoint name\" should be replaced with any user-friendly name for the OpenStack instance you would prefer.","title":"Configuring the Integration Hub for Use with OpenStack"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#scheduling-instances","text":"Once Trait requirements are set for Images and the Integration Hub is configured to push attributes to OpenStack, instances can be launched in OpenStack as normal. As long as the OpenStack Nova scheduler is used to schedule the workloads, only compliant Compute Nodes will be scheduled to run instances of controlled Images. Note This control only applies to instances launched using the OpenStack scheduler, and the Traits functions will not affect manually-launched instances where a specific Compute Node is defined (since this does not use the scheduler at all). Intel SecL-DC uses existing OpenStack interfaces and does not modify OpenStack code. The datacenter owner or OpenStack administrator is responsible for the security of the OpenStack workload scheduling process in general, and Intel recommends following published OpenStack security best practices.","title":"Scheduling Instances"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#integration-with-kubernetes","text":"Through the use of Custom Resource Definitions for the Kubernetes Control Plane, Intel\u00ae Security Libraries can make Kubernetes aware of Intel\u00ae SecL security attributes and make them available for pod orchestration. In this way, a security-sensitive pod can be launched only on Trusted physical worker nodes, or on physical worker nodes that match specified Asset Tag values. Note This control only applies to pods launched using the Kubernetes scheduler, and these scheduling controls will not affect manually-launched instances where a specific worker node is defined (since this does not use the scheduler at all). Intel SecL-DC uses existing Kubernetes interfaces and does not modify Kubernetes code, using only the standard Custom Resource Definition mechanism to add this functionality to the Kubernetes Control Plane. The datacenter owner or Kubernetes administrator is responsible for the security of the Kubernetes workload scheduling process in general, and Intel recommends following published Kubernetes security best practices.","title":"Integration with Kubernetes"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#prerequisites_1","text":"Verification Service must be installed and running. Kubernetes Control Plane Node must be installed and running The supported Kubernetes versions are from 1.14.8 - 1.17.3 and the integration is validated with 1.14.8 and 1.17.3 Kubernetes Worker Nodes must be configured as physical hosts and attached to the Control Plane Node","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#installing-the-intel-secl-custom-resource-definitions","text":"Intel\u00ae SecL uses Custom Resource Definitions to add the ability to base orchestration decisions on Intel\u00ae SecL security attributes to Kubernetes. These CRDs allow Kubernetes administrators to configure pods to require specific security attributes so that the Kubernetes Control Plane Node will schedule those pods only on Worker Nodes that match the specified attributes. Two CRDs are required for integration with Intel\u00ae SecL \u2013 an extension for the Control Plane nodes, and a scheduler extension. A single installer will deploy both of these CRDs. The extensions are deployed as a Kubernetes deployment in the isecl namespace. Note Please refer detail steps given for 3.15 Installing the Intel\u00ae SecL Kubernetes Extensions and Integration Hub section.","title":"Installing the Intel\u00ae SecL Custom Resource Definitions"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#configuring-pods-to-require-intel-secl-attributes","text":"(Optional) Verify that the worker nodes have had their Intel\u00ae SecL security attributes populated: kubectl get nodes --show-labels The output should show the Trust staus and any Asset Tags applied to all of the registered Worker Nodes. Add the following to any Pod creation files: spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : isecl.trusted operator : In values : - \"true\" - key : isecl.TAG_Country operator : In values : - CA - US - key : isecl.TAG_Customer operator : In values : - Coke - Pepsi - key : isecl.TAG_State operator : In values : - CA The isecl.trusted key defines the requirement for a Trusted host. Only one of these keys should be used. The isecl.TAG_ keys indicate Asset Tags; if the workload should only launch on hosts with the COUNTRY=US Asset Tag, the pod should be launched with the matchExpression key isecl.TAG_COUNTRY with the value US . All of the matchExpression definitions must be true for a given worker node to launch the pod \u2013 in the example above, the host must be attested as Trusted with Asset Tags Country=US , Customer=Customer1 and State=CA . If the worker node has additional Asset Tags beyond the ones required, the pod will still be able to be launched on that node. However, if one of the specified Tags is missing or has a different value, that worker node will not be used for that pod.","title":"Configuring Pods to Require Intel\u00ae SecL Attributes"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/28Platform%20Integrity%20Attestation/#tainting-untrusted-worker-nodes","text":"Optionally, the Intel\u00ae SecL Kubernetes CRDs can be configured to flag worker nodes as tainted to prevent any pods from launching on them. This restriction is applied regardless of whether the pod has a specific trust policy \u2013 if a worker node is flagged as tainted no pods will be launched on that worker. This setting is disabled by default. To enable this setting: Edit the isecl-controller.yaml file under /opt/isecl-k8s-extensions/yamls/isecl-controller.yaml and set TAINT_UNTRUSTED_NODES=true Run kubectl apply -f /opt/isecl-k8s-extensions/yamls/isecl-controller.yaml Worker nodes that attest as untrusted will be tainted with the NoExecute flag and unable to launch pods. If a worker was previously considered tainted and the untrusted state is resolved, the Intel\u00ae SecL CRDs will remove the tainted flag and the worker will be able to launch pods again.","title":"Tainting Untrusted Worker Nodes"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/","text":"Workload Confidentiality Workload Confidentiality builds upon Platform Attestation to protect data in virtual machine and container images. At its core, this feature is about allowing an image owner to set policies that define the conditions under which their image will be allowed to run; if the policy conditions are met, the decryption key will be provided, and if the conditions are not met, the image will remain encrypted and inaccessible. This provides a level of enforcement beyond integration with orchestrators, and protects sensitive data when the image is at rest. Workload Encryption relies on Platform Attestation to define the security attributes of hosts. When a protected image is launched, the Workload Agent on the host launching the VM or container image will detect the attempt (using either Libvirt hooks for VMs, or as a function of CRI-O in the case of containers) and use the Image ID to find the Image Flavor on the Workload Service. The Workload Service will retrieve the current trust report for the host launching the image, and use that report to make a key retrieval request to the key transfer URL retrieved from the image flavor. The key transfer URL refers to the URL to the image owner\u2019s Key Broker Service, along with the ID of the key needed. In a typical production deployment, a Cloud Service Provider would enable Intel\u00ae SecL-DC security controls by installing the Intel\u00ae SecL-DC applications (with the exception of the Key Broker and Workload Policy Manager), and configuring each workload host to be Trusted (as per the Platform Integrity Attestation use case). The owner of the workload image(s) to be protected (for example, the end customer of the CSP) must install a Key Broker Service (which must be available for network communication from the Workload Service hosted on the CSP), the Workload Policy Manager, and their own Authentication and Authorization Service and Certificate Management Service (these will manage authentication and certificates for the KBS and WPM). Any number of image owner customers with their own unique KBS/WPM/AAS/CMS deployments may protect images that can be run by a single CSP deployment. The image owner will use the WPM to encrypt any image(s) to be protected; the WPM will automatically create a new image encryption key using the KBS, and will output the encrypted image and an Image Flavor. The image owner can then upload the encrypted image to the CSP\u2019s image storage service, and then upload the Image Flavor to the CSP-hosted WLS. When a compute host at the CSP attempts to launch a protected image, the WLA on the host will detect the launch request, and will issue a key transfer request to the WLS. The WLS will use the image ID to retrieve the Image Flavor, which contains the key retrieval URL for that image. This URL is hosted on the KBS of the image owner (which is why the KBS must be available to network requests from the WLS). The WLS will access the HVS to retrieve the current Platform Integrity Attestation report for the host, and will use this report to make a key transfer request to the KBS at the key transfer URL. The KBS will receive the request, verify that the Platform Integrity Attestation report is signed using a known SAML signing key (verifying that the report comes from a known and trusted HVS), and will then verify that the report shows that the host is trusted. If these requirements are met, the KBS will use the host\u2019s Binding Key (the public half of an asymmetric keypair generated by the host\u2019s TPM and included in the attestation report) as a Key Encryption Key to seal the Image Encryption Key to the TPM of the host that was attested. When the host receives the response to the key request, it will unseal the Image Encryption Key using its TPM. Because the Key Encryption Key is unique to this host\u2019s TPM, only the actual host that was attested will be able to gain access to the image. With the Image Encryption Key, the host\u2019s WLA will create the appropriate encrypted volume(s) for the image and begin the launch as normal. The WLA does not retain the key on disk; if/when the host is rebooted or the WLA is restarted, restarting the workloads based on protected images will trigger new key requests based on new Platform Integrity Attestation reports. In this way, if a host is compromised in a method detectable by the Platform Integrity feature, protected images will be unable to launch on this server. Virtual Machine Confidentiality Prerequisites To enable Virtual Machine Confidentiality, the following Intel\u00ae SecL-DC components must be installed and available: Authentication and Authorization Service Certificate Management Service Key Broker Service Host Verification Service Workload Service Trust Agent + Workload Agent (on each virtualization host) Workload Policy Manager See the Installation subsection on Recommended Service Layout for recommendations on how/where to install each service. It is strongly recommended to use a VM orchestration solution (for example, OpenStack) with the Intel\u00ae SecL-DC Integration Hub to schedule encrypted workloads on compute hosts that have already been pre-checked for their Platform Integrity status. See the Platform Integrity Attestation subsection on Integration with OpenStack for an example. You will need at least one QCOW2-format virtual machine image (for quick testing purposes, a very small minimal premade image like CirrOS is recommended; a good place to look for testing images is the OpenStack Image Guide found here: https://docs.openstack.org/image-guide/obtain-images.html ). One or more hypervisor compute nodes running QEMU/KVM is required. Each of these nodes must have the Intel\u00ae SecL-DC Trust Agent and Workload Agent installed, and they must be registered with the Verification Service. Each of these servers should show as trusted see the Platform Integrity Attestation section for details. You should have Flavors that match the system configuration for these hosts, and attestation reports should show all Flavor parts as trusted=true Hosts that are not trusted (including servers where there is no trust status, like hosts with no Trust Agent) will fail to launch any encrypted workloads. Workflow Encrypting Images wpm create-image-flavor -l <user-friendly unique label> -i <path to image file> -e <output path and filename for encrypted image> -o <output path for JSON image flavor> ` After generating the encrypted image with the WPM, the encrypted image can be uploaded to the Image Storage service of choice (for example, OpenStack Glance). Note that the ID of the image in this Image Storage service must be retained and used for the next steps. Uploading the Image Flavor POST h tt ps : //<Workload Service IP or Hos tna me> : 5000 /wls/ fla vors Au t horiza t io n : Bearer < t oke n > { <Image Flavor co ntent fr om WPM ou t pu t > } Use the above API request to upload the Image Flavor to the WLS. The Image Flavor will tell other Intel\u00ae SecL-DC components the Key Transfer URL for this image. Creating the Image Flavor to Image ID Association The WLS needs to know the ID of the image as it exists in the image storage service used by the CSP (for example, OpenStack Glance). Use the below API request to create an association between the Image Flavor created in the previous step and the image ID. POST h tt ps : //<Workload Service IP or Hos tna me> : 5000 /wls/images Au t horiza t io n : Bearer < t oke n > { \"id\" : \"<image ID on image storage>\" , \"flavor_ids\" : [ \"<Image Flavor ID>\" ] } Launching Encrypted VMs Instances of the protected images can now be launched as normal. Encrypted images will only be accessible on hosts with a Platform Integrity Attestation report showing the host is trusted. If the VM is launched on a host that is not trusted, the launch will fail, as the decryption key will not be provided. Container Confidentiality Container Confidentiality with Cri-o and Skopeo Prerequisites Container Confidentiality with Cri-o and Skopeo requires modified versions of both Cri-o and Skopeo. Both of these are automatically built with the Intel SecL build scripts, and can be found here after the script has executed: isecl/cc-crio/binaries/ Skopeo The patched version of Skopeo 0.1.41-dev must be installed on each Worker Node: https://github.com/lumjjb/skopeo/tree/sample_integration . The Skopeo wrapper that allows Skopeo to interface with the ISecL components must be installed on each Worker Node: https://github.com/lumjjb/skopeo/blob/sample_integration/vendor/github.com/lumjjb/seclkeywrap/keywrapper_secl.go . Copy the Skopeo wrapper into /usr/bin: cp isecl/cc-crio/binaries/skopeo /usr/bin/skopeo Add the following to the crio.service definition to always start Cri-o with the Intel SecL policy parameters enabled: vi /usr/local/lib/systemd/system/crio.service ExecStart=/usr/local/bin/crio \\ $CRIO_CONFIG_OPTIONS \\ $CRIO_RUNTIME_OPTIONS \\ $CRIO_STORAGE_OPTIONS \\ $CRIO_NETWORK_OPTIONS \\ $CRIO_METRICS_OPTIONS \\ --decryption-secl-parameters secl:enabled Cri-o 1.17 The patched version of Cri-o 1.17 must be installed on each Worker Node: https://github.com/lumjjb/cri-o/blob/1.16_encryption_sample_integration . Copy the CRI-O binary from IsecL build script to /usr/bin/: cp isecl/cc-crio/binaries/crio /usr/bin/crio The Cri-o wrapper that allows Cri-o to interface with ISecL components must be installed on each Worker Node: https://github.com/lumjjb/cri-o/blob/1.16_encryption_sample_integration/vendor/github.com/lumjjb/seclkeywrap/keywrapper_secl.go . GoLang 1.14.4 must be installed on each Kubernetes Worker Node Crictl must be installed on each Kubernetes Worker Node $ VERSION=\"v1.17.0\" $ wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz $ sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin $ rm -f crictl-$VERSION-linux-amd64.tar.gz Kubernetes must be configured to use Cri-o and Skopeo Platform Integrity Attestation must be configured for the physical Kubernetes Worker Nodes. This includes, at minimum, the CMS; AAS; HVS; KBS; WPM; and the Trust Agent must be installed on each Worker Node. See the Installation section for details installing these services. Each Kubernetes Worker Node should be Trusted in the attestation reports generated by the HVS. Only physical Worker Nodes are supported at this time. Workflow Skopeo Commands skopeo copy source-image destination-image Options: --encryption-key [secl:asset_tag|keyfile] Specifies the encryption protocol. When using secl protocol, provide either \"any\" or an asset tag in the form \"at_key:at_value\"; only one asset tag can be used at this time. Alternatively, a specific key can be provided to be used for encryption. --decryption-key [secl:enabled|keyfile] specifies the decryption Alternatively, a specific key can be provided to be used for decryption. This flag can be repeated if an image requires more than one key to be decrypted. See https://github.com/lumjjb/skopeo/blob/sample_integration/docs/skopeo-copy.1.md for more details. Examples Copy a container image from a registry to a local server: $ skopeo copy docker://docker.io/library/nginx:latest oci:nginx_local To encrypt an image (this will allow the image to run only on Trusted platforms): $ skopeo copy --encryption-key secl:any oci:nginx_local oci:nginx_secl_enc To encrypt an image with an Asset Tag (this will allow the image to run only on Trusted platforms with the specified Asset tag): $ skopeo copy --encryption-key secl:asset_tag_key:asset_tag_value oci:nginx_local oci:nginx_secl_enc_w_at To decrypt an image: $ skopeo copy --decryption-key secl:enabled oci:nginx_secl_enc oci:nginx_secl_dec To copy an encrypted image without decryption: $ skopeo copy oci:nginx_secl_enc oci:nginx_secl_enc_copy To copy a local image to a remote registry: $ skopeo copy oci:nginx_secl_enc docker://10.80.245.116/nginx_secl_enc:latest Prepare an Image Convert the image to an OCI image using Skopeo: $ skopeo copy docker-daemon:custom-image:latest oci:custom-image:latest Encrypt the image using Skopeo copy command $ skopeo copy --encryption-key secl:any oci:custom-image:latest oci:custom-image:enc Push the image to a registry: $ skopeo copy oci:custom-image:enc docker://Registry.server.com:5000/custom-image:enc Alternatively, encrypt the image and push it to a registry in a single step: $ skopeo copy --encryption-key secl:any oci:custom-image:latest docker://registry.server.com:5000/custom-image:enc Pulling and Encrypting a Container Image Skopeo can be used to pull a container image from an external registry (a private Docker registry is used in the examples below). This image may be encrypted already, but if you wish to pull an image for encryption, it must be in plaintext format. Skopeo has a wrapper that can interact with the Workload Policy Manager. When trying to encrypt an image, Skopeo calls the WPM CLI fetch-key command. In the command, the KBS is called in order to create a new key. The return from the KBS includes the key retrieval URL, which is used when trying to decrypt. After the key is returned to the WPM, the WPM passes the key back to Skopeo. Skopeo uses the key to encrypt the image layer by layer as well as associate the encrypted image with the key's URL. Skopeo then uploads the encrypted image to a remote container registry. The modified Cri-o and wrapper will modify the Cri-o commands to allow Intel SecL policies to be utilized. Launching an Encrypted Container Image Cri-o allows for pulling and decryption of an encrypted container image from a container registry. When trying to pull and decrypt a container image, Cri-o has a hook that calls into the Workload Agent (WLA). The WLA will call into the Workload Service (WLS) and pass it the key URL associated with the encrypted image as well as the host's hardware UUID. These two serve as input to /keys endpoint of the WLS. The WLS initializes a HVS client in order to retrieve the host SAML report and then validates the report. If the host is trusted, the WLS will attempt to get the key. First, it will check if it's been cached alredy. If not, it will initialize a KBS client. The WLS uses this client to retrieve the key from the KBS. If the key is retrieved, it will be cached in the WLS temporarily so that the WLS will not need to requery the KBS if attempting to decrypt with the same key. The key is then passed back to the WLA as the return of the WLS's keys API. Finally, the key is returned to Cri-o, which uses the key to decrypt the container image layer by layer. Containers of the protected images can now be launched as normal using Kubernetes pods and deployments. Encrypted images will only be accessible on hosts with a Platform Integrity Attestation report showing the host is trusted. If the Crio Container is launched on a host that is not trusted, the launch will fail, as the decryption key will not be provided.","title":"Workload Confidentiality"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#workload-confidentiality","text":"Workload Confidentiality builds upon Platform Attestation to protect data in virtual machine and container images. At its core, this feature is about allowing an image owner to set policies that define the conditions under which their image will be allowed to run; if the policy conditions are met, the decryption key will be provided, and if the conditions are not met, the image will remain encrypted and inaccessible. This provides a level of enforcement beyond integration with orchestrators, and protects sensitive data when the image is at rest. Workload Encryption relies on Platform Attestation to define the security attributes of hosts. When a protected image is launched, the Workload Agent on the host launching the VM or container image will detect the attempt (using either Libvirt hooks for VMs, or as a function of CRI-O in the case of containers) and use the Image ID to find the Image Flavor on the Workload Service. The Workload Service will retrieve the current trust report for the host launching the image, and use that report to make a key retrieval request to the key transfer URL retrieved from the image flavor. The key transfer URL refers to the URL to the image owner\u2019s Key Broker Service, along with the ID of the key needed. In a typical production deployment, a Cloud Service Provider would enable Intel\u00ae SecL-DC security controls by installing the Intel\u00ae SecL-DC applications (with the exception of the Key Broker and Workload Policy Manager), and configuring each workload host to be Trusted (as per the Platform Integrity Attestation use case). The owner of the workload image(s) to be protected (for example, the end customer of the CSP) must install a Key Broker Service (which must be available for network communication from the Workload Service hosted on the CSP), the Workload Policy Manager, and their own Authentication and Authorization Service and Certificate Management Service (these will manage authentication and certificates for the KBS and WPM). Any number of image owner customers with their own unique KBS/WPM/AAS/CMS deployments may protect images that can be run by a single CSP deployment. The image owner will use the WPM to encrypt any image(s) to be protected; the WPM will automatically create a new image encryption key using the KBS, and will output the encrypted image and an Image Flavor. The image owner can then upload the encrypted image to the CSP\u2019s image storage service, and then upload the Image Flavor to the CSP-hosted WLS. When a compute host at the CSP attempts to launch a protected image, the WLA on the host will detect the launch request, and will issue a key transfer request to the WLS. The WLS will use the image ID to retrieve the Image Flavor, which contains the key retrieval URL for that image. This URL is hosted on the KBS of the image owner (which is why the KBS must be available to network requests from the WLS). The WLS will access the HVS to retrieve the current Platform Integrity Attestation report for the host, and will use this report to make a key transfer request to the KBS at the key transfer URL. The KBS will receive the request, verify that the Platform Integrity Attestation report is signed using a known SAML signing key (verifying that the report comes from a known and trusted HVS), and will then verify that the report shows that the host is trusted. If these requirements are met, the KBS will use the host\u2019s Binding Key (the public half of an asymmetric keypair generated by the host\u2019s TPM and included in the attestation report) as a Key Encryption Key to seal the Image Encryption Key to the TPM of the host that was attested. When the host receives the response to the key request, it will unseal the Image Encryption Key using its TPM. Because the Key Encryption Key is unique to this host\u2019s TPM, only the actual host that was attested will be able to gain access to the image. With the Image Encryption Key, the host\u2019s WLA will create the appropriate encrypted volume(s) for the image and begin the launch as normal. The WLA does not retain the key on disk; if/when the host is rebooted or the WLA is restarted, restarting the workloads based on protected images will trigger new key requests based on new Platform Integrity Attestation reports. In this way, if a host is compromised in a method detectable by the Platform Integrity feature, protected images will be unable to launch on this server.","title":"Workload Confidentiality"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#virtual-machine-confidentiality","text":"","title":"Virtual Machine Confidentiality"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#prerequisites","text":"To enable Virtual Machine Confidentiality, the following Intel\u00ae SecL-DC components must be installed and available: Authentication and Authorization Service Certificate Management Service Key Broker Service Host Verification Service Workload Service Trust Agent + Workload Agent (on each virtualization host) Workload Policy Manager See the Installation subsection on Recommended Service Layout for recommendations on how/where to install each service. It is strongly recommended to use a VM orchestration solution (for example, OpenStack) with the Intel\u00ae SecL-DC Integration Hub to schedule encrypted workloads on compute hosts that have already been pre-checked for their Platform Integrity status. See the Platform Integrity Attestation subsection on Integration with OpenStack for an example. You will need at least one QCOW2-format virtual machine image (for quick testing purposes, a very small minimal premade image like CirrOS is recommended; a good place to look for testing images is the OpenStack Image Guide found here: https://docs.openstack.org/image-guide/obtain-images.html ). One or more hypervisor compute nodes running QEMU/KVM is required. Each of these nodes must have the Intel\u00ae SecL-DC Trust Agent and Workload Agent installed, and they must be registered with the Verification Service. Each of these servers should show as trusted see the Platform Integrity Attestation section for details. You should have Flavors that match the system configuration for these hosts, and attestation reports should show all Flavor parts as trusted=true Hosts that are not trusted (including servers where there is no trust status, like hosts with no Trust Agent) will fail to launch any encrypted workloads.","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#workflow","text":"","title":"Workflow"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#encrypting-images","text":"wpm create-image-flavor -l <user-friendly unique label> -i <path to image file> -e <output path and filename for encrypted image> -o <output path for JSON image flavor> ` After generating the encrypted image with the WPM, the encrypted image can be uploaded to the Image Storage service of choice (for example, OpenStack Glance). Note that the ID of the image in this Image Storage service must be retained and used for the next steps.","title":"Encrypting Images"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#uploading-the-image-flavor","text":"POST h tt ps : //<Workload Service IP or Hos tna me> : 5000 /wls/ fla vors Au t horiza t io n : Bearer < t oke n > { <Image Flavor co ntent fr om WPM ou t pu t > } Use the above API request to upload the Image Flavor to the WLS. The Image Flavor will tell other Intel\u00ae SecL-DC components the Key Transfer URL for this image.","title":"Uploading the Image Flavor"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#creating-the-image-flavor-to-image-id-association","text":"The WLS needs to know the ID of the image as it exists in the image storage service used by the CSP (for example, OpenStack Glance). Use the below API request to create an association between the Image Flavor created in the previous step and the image ID. POST h tt ps : //<Workload Service IP or Hos tna me> : 5000 /wls/images Au t horiza t io n : Bearer < t oke n > { \"id\" : \"<image ID on image storage>\" , \"flavor_ids\" : [ \"<Image Flavor ID>\" ] }","title":"Creating the Image Flavor to Image ID Association"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#launching-encrypted-vms","text":"Instances of the protected images can now be launched as normal. Encrypted images will only be accessible on hosts with a Platform Integrity Attestation report showing the host is trusted. If the VM is launched on a host that is not trusted, the launch will fail, as the decryption key will not be provided.","title":"Launching Encrypted VMs"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#container-confidentiality","text":"","title":"Container Confidentiality"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#container-confidentiality-with-cri-o-and-skopeo","text":"","title":"Container Confidentiality with Cri-o and Skopeo"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#prerequisites_1","text":"Container Confidentiality with Cri-o and Skopeo requires modified versions of both Cri-o and Skopeo. Both of these are automatically built with the Intel SecL build scripts, and can be found here after the script has executed: isecl/cc-crio/binaries/ Skopeo The patched version of Skopeo 0.1.41-dev must be installed on each Worker Node: https://github.com/lumjjb/skopeo/tree/sample_integration . The Skopeo wrapper that allows Skopeo to interface with the ISecL components must be installed on each Worker Node: https://github.com/lumjjb/skopeo/blob/sample_integration/vendor/github.com/lumjjb/seclkeywrap/keywrapper_secl.go . Copy the Skopeo wrapper into /usr/bin: cp isecl/cc-crio/binaries/skopeo /usr/bin/skopeo Add the following to the crio.service definition to always start Cri-o with the Intel SecL policy parameters enabled: vi /usr/local/lib/systemd/system/crio.service ExecStart=/usr/local/bin/crio \\ $CRIO_CONFIG_OPTIONS \\ $CRIO_RUNTIME_OPTIONS \\ $CRIO_STORAGE_OPTIONS \\ $CRIO_NETWORK_OPTIONS \\ $CRIO_METRICS_OPTIONS \\ --decryption-secl-parameters secl:enabled Cri-o 1.17 The patched version of Cri-o 1.17 must be installed on each Worker Node: https://github.com/lumjjb/cri-o/blob/1.16_encryption_sample_integration . Copy the CRI-O binary from IsecL build script to /usr/bin/: cp isecl/cc-crio/binaries/crio /usr/bin/crio The Cri-o wrapper that allows Cri-o to interface with ISecL components must be installed on each Worker Node: https://github.com/lumjjb/cri-o/blob/1.16_encryption_sample_integration/vendor/github.com/lumjjb/seclkeywrap/keywrapper_secl.go . GoLang 1.14.4 must be installed on each Kubernetes Worker Node Crictl must be installed on each Kubernetes Worker Node $ VERSION=\"v1.17.0\" $ wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz $ sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin $ rm -f crictl-$VERSION-linux-amd64.tar.gz Kubernetes must be configured to use Cri-o and Skopeo Platform Integrity Attestation must be configured for the physical Kubernetes Worker Nodes. This includes, at minimum, the CMS; AAS; HVS; KBS; WPM; and the Trust Agent must be installed on each Worker Node. See the Installation section for details installing these services. Each Kubernetes Worker Node should be Trusted in the attestation reports generated by the HVS. Only physical Worker Nodes are supported at this time.","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#workflow_1","text":"","title":"Workflow"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#skopeo-commands","text":"skopeo copy source-image destination-image","title":"Skopeo Commands"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#options","text":"--encryption-key [secl:asset_tag|keyfile] Specifies the encryption protocol. When using secl protocol, provide either \"any\" or an asset tag in the form \"at_key:at_value\"; only one asset tag can be used at this time. Alternatively, a specific key can be provided to be used for encryption. --decryption-key [secl:enabled|keyfile] specifies the decryption Alternatively, a specific key can be provided to be used for decryption. This flag can be repeated if an image requires more than one key to be decrypted. See https://github.com/lumjjb/skopeo/blob/sample_integration/docs/skopeo-copy.1.md for more details.","title":"Options:"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#examples","text":"Copy a container image from a registry to a local server: $ skopeo copy docker://docker.io/library/nginx:latest oci:nginx_local To encrypt an image (this will allow the image to run only on Trusted platforms): $ skopeo copy --encryption-key secl:any oci:nginx_local oci:nginx_secl_enc To encrypt an image with an Asset Tag (this will allow the image to run only on Trusted platforms with the specified Asset tag): $ skopeo copy --encryption-key secl:asset_tag_key:asset_tag_value oci:nginx_local oci:nginx_secl_enc_w_at To decrypt an image: $ skopeo copy --decryption-key secl:enabled oci:nginx_secl_enc oci:nginx_secl_dec To copy an encrypted image without decryption: $ skopeo copy oci:nginx_secl_enc oci:nginx_secl_enc_copy To copy a local image to a remote registry: $ skopeo copy oci:nginx_secl_enc docker://10.80.245.116/nginx_secl_enc:latest","title":"Examples"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#prepare-an-image","text":"Convert the image to an OCI image using Skopeo: $ skopeo copy docker-daemon:custom-image:latest oci:custom-image:latest Encrypt the image using Skopeo copy command $ skopeo copy --encryption-key secl:any oci:custom-image:latest oci:custom-image:enc Push the image to a registry: $ skopeo copy oci:custom-image:enc docker://Registry.server.com:5000/custom-image:enc Alternatively, encrypt the image and push it to a registry in a single step: $ skopeo copy --encryption-key secl:any oci:custom-image:latest docker://registry.server.com:5000/custom-image:enc","title":"Prepare an Image"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#pulling-and-encrypting-a-container-image","text":"Skopeo can be used to pull a container image from an external registry (a private Docker registry is used in the examples below). This image may be encrypted already, but if you wish to pull an image for encryption, it must be in plaintext format. Skopeo has a wrapper that can interact with the Workload Policy Manager. When trying to encrypt an image, Skopeo calls the WPM CLI fetch-key command. In the command, the KBS is called in order to create a new key. The return from the KBS includes the key retrieval URL, which is used when trying to decrypt. After the key is returned to the WPM, the WPM passes the key back to Skopeo. Skopeo uses the key to encrypt the image layer by layer as well as associate the encrypted image with the key's URL. Skopeo then uploads the encrypted image to a remote container registry. The modified Cri-o and wrapper will modify the Cri-o commands to allow Intel SecL policies to be utilized.","title":"Pulling and Encrypting a Container Image"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/29Workload%20Confidentiality/#launching-an-encrypted-container-image","text":"Cri-o allows for pulling and decryption of an encrypted container image from a container registry. When trying to pull and decrypt a container image, Cri-o has a hook that calls into the Workload Agent (WLA). The WLA will call into the Workload Service (WLS) and pass it the key URL associated with the encrypted image as well as the host's hardware UUID. These two serve as input to /keys endpoint of the WLS. The WLS initializes a HVS client in order to retrieve the host SAML report and then validates the report. If the host is trusted, the WLS will attempt to get the key. First, it will check if it's been cached alredy. If not, it will initialize a KBS client. The WLS uses this client to retrieve the key from the KBS. If the key is retrieved, it will be cached in the WLS temporarily so that the WLS will not need to requery the KBS if attempting to decrypt with the same key. The key is then passed back to the WLA as the return of the WLS's keys API. Finally, the key is returned to Cri-o, which uses the key to decrypt the container image layer by layer. Containers of the protected images can now be launched as normal using Kubernetes pods and deployments. Encrypted images will only be accessible on hosts with a Platform Integrity Attestation report showing the host is trusted. If the Crio Container is launched on a host that is not trusted, the launch will fail, as the decryption key will not be provided.","title":"Launching an Encrypted Container Image"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/","text":"Intel\u00ae Security Libraries Components Intel\u00ae Security Libraries Components Certificate Management Service Starting with Intel\u00ae SecL-DC 1.6, most non-TPM-related certificates used by Intel\u00ae SecL-DC applications will be issued by the new Certificate Management Service. This includes acting as a root CA and issuing TLS certificates for all of the various web services. Authentication and Authorization Service Starting with Intel\u00ae SecL-DC 1.6, authentication and authorization for all Intel\u00ae SecL applications will be centrally managed by the new Authentication and Authorization Service (AAS). Previously, each application would manage its own users and permissions independently; this change allows authentication and authorization management to be centralized. Verification Service The Verification Service component of Intel\u00ae Security Libraries performs the core Platform Integrity and Data Sovereignty functionality by acting as a remote attestation authority. Platform security technologies like Intel\u00ae TXT, Intel\u00ae BootGuard, and UEFI SecureBoot extend measurements of platform components (such as the system BIOS/UEFI, OS kernel, etc) to a Trusted Platform module as the server boots. Known-good measurements for each of these components can be directly imported from a sample server. These expected measurements can then be compared against actual measurements from registered servers, allowing the Verification Service to attest to the \"trustiness\" of the platform, meaning whether the platform booted into a \"known-good\" state. Workload Service The Workload Service acts as a management service for handling Workload Flavors (Flavors used for Virtual Machines and Containers). In the Intel\u00ae SecL-DC 1.6 release, the Workload Service uses Flavors to map decryption key IDs to image IDs. When a launch request for an encrypted workload image is intercepted by the Workload Agent, the Workload Service will handle mapping the image ID to the appropriate key ID and key request URL, and will initiate the key transfer request to the Key Broker. Trust Agent The Trust Agent resides on physical servers and enables both remote attestation and the extended chain of trust capabilities. The Agent maintains ownership of the server's Trusted Platform Module, allowing secure attestation quotes to be sent to the Verification Service. Incorporating the Intel\u00ae SecL HostInfo and TpmProvider libraries, the Trust Agent serves to report on platform security capabilities and platform integrity measurements. The Trust Agent is supported for Windows Server 2016 Datacenter and Red Hat Enterprise Linux (RHEL) 8.1 and later. Workload Agent The Workload Agent is the component responsible for handling all of the functions needed for Workload Confidentiality for virtual machines and containers on a physical server. The Workload Agent uses libvirt hooks to identify VM lifecycle events (VM start, stop, hibernate, etc), and intercepts those events to perform needed functions like requesting decryption keys, creation and deletion of encrypted LUKS volumes, using the TPM to unseal decryption keys, etc. The WLA also performs analogous functionality for containers. Integration Hub The Integration Hub acts as a middle-man between the Verification Service and one or more scheduler services (such as OpenStack* Nova), and \"pushes\" attestation information retrieved from the Verification Service to one or more scheduler services according to an assignment of hosts to specific tenants. In this way, Tenant A can receive attestation information for hosts that belong to Tenant A, but receive no information about hosts belonging to Tenant B. The Integration Hub serves to disassociate the process of retrieving attestations from actual scheduler queries, so that scheduler services can adhere to best practices and retain better performance at scale. The Integration Hub will regularly query the Intel\u00ae SecL Verification Service for SAML attestations for each host. The Integration Hub maintains only the most recent currently valid attestation for each host, and will refresh attestations when they would expire. The Integration Hub will verify the signature of the SAML attestation for each host assigned to a tenant, then parse the attestation status and asset tag information, and then will securely push the parsed key/value pairs to the plugin endpoints enabled. The Integration Hub features a plugin design for adding new scheduler endpoint types. Currently the Integration Hub supports OpenStack Nova and Kubernetes endpoint plugins. Other integration plugins may be added. Workload Policy Manager The Workload Policy Manager is a Linux command line utility used by an image owner to encrypt VM (qcow2) or container images, and to create an Image Flavor used to provide the encryption key transfer URL during launch requests. The WPM utility will use an existing or request a new key from the Key Broker Service, use that key to encrypt the image, and output the Image Flavor in JSON format. The encrypted image can then be uploaded to the image store of choice (like OpenStack Glance), and the Image Flavor can be uploaded to the Workload Service. The ID of the image on the image storage system is then mapped to the Image Flavor in the WLS; when the image is used to launch a new instance, the WLS will find the Image Flavor associated with that image ID, and use the Image Flavor to determine the key transfer URL. Key Broker Service The Key Broker Service is effectively a policy compliance engine. Its job is to manage key transfer requests, releasing keys only to servers that meet policy requirements. The Key Broker registers one or more SAML signing certificates from any Verification Services that it will trust. When a key transfer request is received, the request includes a trust attestation report signed by the Verification Service. If the signature matches a registered SAML key, the Broker will then look at the actual report to ensure the server requesting the key matches the image policy (currently only overall system trust is supported as a policy requirement). If the report indicates the policy requirements are met, the image decryption key is wrapped using a public key unique to the TPM of the host that was attested in the report, such that only the host that was attested can unseal the decryption key and gain access to the image. Intel\u00ae Security Libraries Binary Installation Intel\u00ae SecL services can be deployed as direct binary installations (on bare metal or in VMs), or can be deployed as containers. This section details the binary-based installation of Intel SecL services; the next major section details container-based deployments. It is recommended to deploy all control-plane services (CMS, AAS, HVS, WLS) as either containers or binaries, and not a mix of the two. The Trust Agent/Workload Agent, KBS, and WPM can be installed as binaries or deployed as containers regardless of the installation method used for the control plane. Building Binary Installers Intel\u00ae Security Libraries is distributed as open source code, and must be compiled into installation binaries before installation. Instructions and sample scripts for building the Intel\u00ae SecL-DC components can be found here. After the components have been built, the installation binaries can be found in the directories created by the build scripts. <servicename>/out/<servicename>.bin In addition, the build script will produce some sample database creation scripts that can be used during installation to configure database requirements (instructions are given in the installation sections): create_db: authservice/out/create_db.sh install_pgdb: authservice/out/install_pgdb.sh In addition, sample Ansible roles to automatically build and deploy a testbed environment are provided here. Also provided are sample API calls organized by workflows for Postman here. Hardware Considerations Intel\u00ae SecL-DC supports and uses a variety of Intel security features, but there are some key requirements to consider before beginning an installation. Most important among these is the Root of Trust configuration. This involves deciding what combination of TXT, Boot Guard, tboot, and UEFI Secure Boot to enable on platforms that will be attested using Intel\u00ae SecL. Key points: - At least one \"Static Root of Trust\" mechanism must be used (TXT and/or BtG) - For Legacy BIOS systems, tboot must be used (which requires TXT) - For UEFI mode systems, UEFI SecureBoot must be used* Use the chart below for a guide to acceptable configuration options. . Recommended Service Layout The Intel\u00ae SecL-DC services can be installed in a variety of layouts, partially depending on the use cases desired and the OS of the server(s) to be protected. In general, the Intel\u00ae SecL-DC applications can be divided into management services that are deployed on the network on the management plane, and host or node components that must be installed on each protected server. Management services can typically be deployed anywhere with network access to all of the protected servers. This could be a set of individual VMs per service; containers; or all installed on a single physical or virtual machine. Node components must be installed on each protected physical server. Typically this is needed for Windows and Linux deployments. Platform Integrity The most basic use case enabled by Intel\u00ae SecL-DC, Platform Integrity requires only the Verification Service and, to protect Windows or Linux hosts, the Trust Agent. This also enables the Application Integrity use case by default for Linux systems. The Integration Hub may be added to provide integration support for OpenStack or Kubernetes. The Hub is often installed on the same machine as the Verification Service, but optionally can be installed separately. Workload Confidentiality Workload Confidentiality introduces a number of additional services and agents. For a POC environment, all of the management services can be installed on a single machine or VM. This includes: - Certificate Management Service (CMS) - Authorization and Authentication Service (AAS) - Host Verification Service (HVS) - Workload Service (WLS) - Integration Hub (HUB) - Key Broker Service (KBS) with backend key management - Workload Policy Manager (WPM) In a production environment, it is strongly suggested that the WPM and KBS be deployed (with their own CMS and AAS) separately for each image owner. For a Cloud Service Provider, this would mean that each customer/tenant who will use the Workload Confidentiality feature would have their own dedicated AAS/CMS/KBS/WPM operated on their own networks, not controlled by the CSP. This is because the Key Broker and WPM are the tools used to define the policies that will allow images to launch, and these policies and their enforcement should remain entirely under the control of the image owner. The node components must be installed on each protected physical server: - Trust Agent (TA) - Workload Agent (WLA) Recommended Service Layout & Architecture - Containerized Deployment with K8s The containerized deployment makes use of Kubernetes orchestrator for single node and multi node deployments. A single-node deployment uses Microk8s to deploy the entire control plane in a pod on a single device. This is best for POC or demo environments, but can also be used when integrating Intel SecL with another application that runs on a virtual machine - the single node deployment can run in the same VM as the integrated application to keep all functions local. Single Node: A multi-node deployment is a more typical Kubernetes architecture, where the Intel SecL management plane is simply deployed as a Pod, with the Intel SecL agents (the WLA and the TA, depending on use case) deployed as a DaemonSet. Multi Node: Services Deployments & Agent DaemonSets: Every service including databases will be deployed as separate K8s deployment with 1 replica, i.e(1 pod per deployment). Each deployment will be further exposed through k8s service and also will be having corresponding Persistent Volume Claims(PV) for configuration and log directories and mounted on persistent storage. In case of daemonsets/agents, the configuration and log directories will be mounted on respective Baremetal worker nodes. For stateful services which requires database like shvs, aas, scs, A separate database deployment will be created for each of such services. The data present on the database deployment will also made to persist on a NFS, through K8s persistent storage mechanism Networking within the Cluster: Networking Outside the Cluster: Installing/Configuring the Database The Intel\u00ae SecL-DC Authentication and Authorization Service (AAS) requires a Postgresql 11 database. Scripts (install_pgdb.sh, create_db.sh) are provided with the AAS that will automatically add the Postgresql repositories and install/configure a sample database. If this script will not be used, a Postgresql 11 database must be installed by the user before executing the AAS installation. Using the Provided Database Installation Script Install a sample Postgresql 11 database using the install_pgdb.sh script. This script will automatically install the Postgresql database and client packages required. Add the Postgresql repository: https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm Create the iseclpgdb.env answer file: ISECL_PGDB_IP_INTERFACES = localhost ISECL_PGDB_PORT = 5432 ISECL_PGDB_SAVE_DB_INSTALL_LOG = true ISECL_PGDB_CERT_DNS = localhost ISECL_PGDB_CERT_IP = 127 .0.0.1 Note that the values above assume that the database will be accessed locally. If the database server will be external to the Intel\u00ae SecL services, change these values to the hostname or FQDN and IP address where the client will access the database server. Run the following command: dnf module disable postgresql -y Execute the installation script: ./install_pgdb.sh Note The database installation only needs to be performed once if the same database server will be used for all services that require a database. Only the \"create_db\" step needs to be repeated if the database server will be shared. Provisioning the Database Each Intel\u00ae SecL service that uses a database (the Authentication and Authorization Service, the Verification Service, the Integration Hub, the Workload Service) requires its own schema and access. After installation, the database must be created initialized and tables created. Execute the create_db.sh script to configure the database. If a single shared database server will be used for each Intel\u00ae SecL service (for example, if all management plane services will be installed on a single VM), run the script multiple times, once for each service that requires a database. If separate database servers will be used (for example, if the management plane services will reside on separate systems and will use their own local database servers), execute the script on each server hosting a database. ./create_db.sh <database name> <database_username> <database_password> For example: ./create_db.sh isecl_hvs_db hvs_db_username hvs_db_password ./create_db.sh isecl_aas_db aas_db_username aas_db_password ./create_db.sh isecl_wls_db wls_db_username wls_db_password Note that the database name, username, and password details for each service must be used in the corresponding installation answer file for that service. Database Server TLS Certificate The database client for Intel\u00ae SecL services requires the database TLS certificate to authenticate communication with the database server. If the database server for a service is located on the same server that the service will run on, only the path to this certificate is needed. If the provided Postgres scripts are used, the certificate will be located in /usr/local/pgsql/data/server.crt If the database server will be run separately from the Intel\u00ae SecL service(s), the certificate will need to be copied from the database server to the service machine before installing the Intel\u00ae SecL services. The database client for Intel\u00ae SecL services will validate that the Subject Alternative Names in the database server\u2019s TLS certificate contain the hostname(s)/IP address(es) that the clients will use to access the database server. If configuring a database without using the provided scripts, ensure that these attributes are present in the database TLS certificate. Using NATS with Intel SecL Intel SecL-DC can utilize a NATS server to manage connectivity between the Host Verification Service and any number of deployed Trust Agent hosts. This acts as an alternative to communication via REST APIs - in NATS mode, a connection is established with the NATS server, and messages are sent and received over that connection. The NATS server should be deployed on the control plane and will need network connectivity to other control plane services as well as any Trust Agent hosts. While NATS is not installed by Intel SecL directly, sample instructions for deploying a NATS server for use with Intel SecL can be found below (this is intended to be a sample only; please consult https://nats.io for official NATS documentation) : ###Download and install the NATS-server binary (see https://github.com/nats-io/nats-server/releases/latest) rpm -i https://github.com/nats-io/nats-server/releases/download/v2.3.0/nats-server-v2.3.0-amd64.rpm ###Install tar and unzip yum install -y tar unzip ###Install cfssl and cfssljson (for \u201cControl-Plane Deployment\u201d step #6). wget https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssljson_1.6.0_linux_amd64 -o /usr/bin/cfssljson && chmod +x /usr/bin/cfssljson wget https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssl_1.6.0_linux_amd64 -o /usr/bin/cfssl && chmod +x /usr/bin/cfssl Use the following additional options in populate-users.env and run populate-users: ISECL_INSTALL_COMPONENTS = TA,HVS,AAS,NATS NATS_CERT_SAN_LIST = <NATS server ip>,<NATS server FQDN>,localhost NATS_CERT_COMMON_NAME = NATS TLS Certificate Note that the ISECL_INSTALL_COMPONENTS list should reflect the actual components used in your deployment of Intel SecL, as dictated by the use cases to be enabled. The important part here is to add the \"NATS\" element to the list. To generate a long-lived token for use in environments where the Trust Agent may not be provisioned for an extended period of time beyond the usual lifetime of an authentication token, add the following to populate-users.env: CUSTOM_CLAIMS_COMPONENTS = <List of services for which the long-lived installation token will be valid. Typically this is only the Trust Agent, and so the only component in the list is typically \"TA\"> CUSTOM_CLAIMS_TOKEN_VALIDITY_SECS = <duration in seconds for the token to last> CCC_ADMIN_USERNAME = <username for generating long-lived tokens> CCC_ADMIN_PASSWORD = <password for generating long-lived tokens> When populate-users is run, there will now be an additional \"Custom Claims Token For TA:\" bearer token. When installing the HVS, add the following to hvs.env: NATS_SERVERS = nats://<server ip>:4222 Alternatively, if the HVS is already installed, add the following in /etc/hvs/config.yml : nats : servers : - nats://<NATS server IP>:4222 Be sure to restart the HVS after changing the configuration. Download TLS certificates for the NATS server from the CMS: export NATS_CERT_COMMON_NAME = \"NATS TLS Certificate\" export CMS_ENDPOINT_URL = https://<CMS server ip or hostname>:8445/cms/v1 export NATS_CERT_SAN_LIST = \" <NATS server ip>,<NATS server FQDN>,localhost\" ./download-tls-certs.sh -d ./ -n \" $NATS_CERT_COMMON_NAME \" -u \" $CMS_ENDPOINT_URL \" -s \" $NATS_CERT_SAN_LIST \" -t $BEARER_TOKEN The download-tls-certs.sh script will conenct to the CMS and will out put two files: server.pem sslcert-key.pem Create a \u201cserver.conf\u201d configuration file for nats-server (be sure the paths to the .pem files are correct): lis ten : 0.0.0.0 : 4222 tls : { cer t _ f ile : \"./server.pem\" key_ f ile : \"./sslcert-key.pem\" } Append the operator/account credentials from the AAS installation to the server.conf (the following can be run if NATS will run on the same machine as the AAS): cat /etc/authservice/nats/server.conf >> server.conf The final server.conf should look like the following: lis ten : 0.0.0.0 : 4222 tls : { cer t _ f ile : \"/<path>/server.pem\" key_ f ile : \"/<path>/sslcert-key.pem\" } // Opera t or ISecL - opera t or opera t or : eyJ 0e XAiOiJKV 1 QiLCJhbGciOiJlZDI 1 NTE 5 LW 5 rZXki f Q.eyJleHAiOjE 3 ODE 2 MzI 0 NjUsImp 0 aSI 6 IkJWQVZUQkc 1 M 01 aMkFaSTVYUjRFNVlPS 0 xHTk 5 ZTE 40 SllYV 0 U 3 T 1 I 1 M 0 VQSDJOU 0 pFSEEiLCJpYXQiOjE 2 MjM 5 NTI 0 NjUsImlzcyI 6 Ik 9 DTENLS 1 UzS 0 lMWjZaRDRESDNWNTdUSkJESUdKSllMWk 1 RNEhKUU 9 DNFJFUVEyVkFUVU 01 SlA 1 IiwibmF t ZSI 6 IklTZWNMLW 9 wZXJhdG 9 yIiwic 3 ViIjoiT 0 NMQ 0 t LVTNLSUxaNlpENERIM 1 Y 1 N 1 RKQkRJR 0 pKWUxaTVE 0 SEpRT 0 M 0 UkVRUTJWQVRVTTVKUDUiLCJuYXRzIjp 7 I n R 5 cGUiOiJvcGVyYXRvciIsI n Zlc n Npb 24 iOjJ 9 f Q.PDlhAwk 1 cLHpbCCAJhKGKvv 36 J_NXc 2 PSs n 6 i 3 z n mjDYHXG 3 C_HhO 9 zxsl n 9 Bd 9 ViolRw_L 10 N 1 QwoMjzCB t BQ resolver : MEMORY resolver_preload : { // Accou nt ISecL - accou nt ADR 7 WNJ 2EEE IYASP 5 YHDFDW 6 P 3 ICBFPXRRJVWU 6 CLGXOWVDIM 7 VIOXCM : eyJ 0e XAiOiJKV 1 QiLCJhbGciOiJlZDI 1 NTE 5 LW 5 rZXki f Q.eyJleHAiOjE 3 ODE 2 MzI 0 NjUsImp 0 aSI 6 IjdVUlg 0 M 0 RSTUxEV 0 JEVTdMU 0 dDNTQ 0 UzZCRFFGVDc 1 TzZVWUE 1 QUdYNkxGV 0 FNWUNOTkEiLCJpYXQiOjE 2 MjM 5 NTI 0 NjUsImlzcyI 6 Ik 9 DTENLS 1 UzS 0 lMWjZaRDRESDNWNTdUSkJESUdKSllMWk 1 RNEhKUU 9 DNFJFUVEyVkFUVU 01 SlA 1 IiwibmF t ZSI 6 IklTZWNMLWFjY 291 b n QiLCJzdWIiOiJBRFI 3 V 05 KMkVFRUlZQVNQNVlIREZEVzZQM 0 lDQkZQWFJSSlZXVTZDTEdYT 1 dWRElNN 1 ZJT 1 hDTSIsIm 5 hdHMiO ns ibGl ta XRzIjp 7 I n N 1 Y n MiOi 0 xLCJkYXRhIjo t MSwicGF 5 bG 9 hZCI 6 LTEsIml t cG 9 ydHMiOi 0 xLCJleHBvc n RzIjo t MSwid 2 lsZGNhcmRzIjp 0 c n VlLCJjb 25 uIjo t MSwibGVhZiI 6 LTF 9 LCJkZWZhdWx 0 X 3 Blcm 1 pc 3 Npb 25 zIjp 7 I n B 1 YiI 6e30 sI n N 1 YiI 6e319 LCJ 0e XBlIjoiYWNjb 3 VudCIsI n Zlc n Npb 24 iOjJ 9 f Q.AB 6e NFVE 7 KJspvX 7 DN - x_ - L 4 mMNhPc - sDk 01 iOL - hEwYK fe oL 9 RAcdrOTwQX 3 CuJHMu - a 3 m 5 TpW fl g 1 D 4 S 1 MCQ } Start NATS server: ./nats-server -c server.conf Installing the Certificate Management Service Required For The CMS is REQUIRED for all use cases. Platform Integrity with Data Sovereignty and Signed Flavors Application Integrity Workload Confidentiality (both VMs and Containers) Supported Operating Systems The Intel\u00ae Security Libraries Certificate Management Service supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04 Recommended Hardware 1 vCPUs RAM: 2 GB 10 GB One network interface with network access to all Intel\u00ae SecL-DC services Installation To install the Intel\u00ae SecL-DC Certificate Management Service: Copy the Certificate Management Service installation binary to the /root/ directory. Create the cms.env installation answer file for an unattended installation: AAS_TLS_SAN = <comma-separated list of IPs and hostnames for the AAS> AAS_API_URL = https://<Authentication and Authorization Service IP or Hostname>:8444/aas/v1 SAN_LIST = <Comma-separated list of IP addresses and hostnames for the CMS>,127.0.0.1,localhost The SAN list will be used to authenticate the Certificate Signing Request from the AAS to the CMS. Only a CSR originating from a host matching the SAN list will be honored. Later, in the AAS authservice.env installation answer file, this same SAN list will be provided for the AAS installation. These lists must match, and must be valid for IPs and/or hostnames used by the AAS system. If both the AAS and CMS will be installed on the same system, \"127.0.0.1,localhost\" may be used. The SAN list variables also accept the wildcards \u201c?\u201d (for single-character wildcards) and \"*\" (for multiple-character wildcards) to allow address ranges or multiple FQDNs. The AAS_API_URL represents the URL for the AAS that will exist after the AAS is installed. For all configuration options and their descriptions, refer to the Intel\u00ae SecL Configuration section on the Certificate Management Service. Execute the installer binary. ./cms-v4.0.0.bin When the installation completes, the Certificate Management Service is available. The services can be verified by running cms status from the command line. cms status After installation is complete, the CMS will output a bearer token to the console. This token will be used with the AAS during installation to authenticate certificate requests to the CMS. If this token expires or otherwise needs to be recreated, use the following command: cms setup cms_auth_token --force In addition, the SHA384 digest of the CMS TLS certificate will be needed for installation of the remaining Intel\u00ae SecL services. The digest can be obtained using the following command: cms tlscertsha384 Installing the Authentication and Authorization Service Required For The AAS is REQUIRED for all use cases. Platform Integrity with Data Sovereignty and Signed Flavors Application Integrity Workload Confidentiality (both VMs and Containers) Prerequisites The following must be completed before installing the Authentication and Authorization Service: The Certificate Management Service must be installed and available The Authentication and Authorization Service database must be available Package Dependencies The Intel\u00ae SecL-DC Authentication and Authorization Service (AAS) requires a Postgresql 11 database. A script (install_pgdb.sh) is provided with the AAS that will automatically add the Postgresql repositories and install/configure a sample database. If this script will not be used, a Postgresql 11 database must be installed by the user before executing the AAS installation. Supported Operating Systems The Intel\u00ae Security Libraries Authentication and Authorization Service supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04 Recommended Hardware 1 vCPUs RAM: 2 GB 10 GB One network interface with network access to all Intel\u00ae SecL-DC services Installation To install the AAS, a bearer token from the CMS is required. This bearer token is output at the end of the CMS installation. However, if a new token is needed, simply use the following command from the CMS command line: cms setup cms_auth_token --force Create the authservice.env installation answer file: CMS_BASE_URL = https://<CMS IP or hostname>:8445/cms/v1/ CMS_TLS_CERT_SHA384 = <CMS TLS certificate sha384> AAS_DB_HOSTNAME = <IP or hostname of database server> AAS_DB_PORT = <database port number ; default is 5432 > AAS_DB_NAME = <database name> AAS_DB_USERNAME = <database username> AAS_DB_PASSWORD = <database password> AAS_DB_SSLCERTSRC = <path to database TLS certificate ; the default location is typically /usr/local/pgsql/data/server.crt> AAS_ADMIN_USERNAME = <username for AAS administrative user> AAS_ADMIN_PASSWORD = <password for AAS administrative user> SAN_LIST = <comma-separated list of IPs and hostnames for the AAS ; this should match the value for the AAS_TLS_SAN in the cms.env file from the CMS installation> BEARER_TOKEN = <bearer token from CMS installation> Execute the AAS installer: ./authservice-v4.0.0.bin Note The AAS_ADMIN credentials specified in this answer file will have administrator rights for the AAS and can be used to create other users, create new roles, and assign roles to users. Creating Users After installation is complete, a number of roles and user accounts must be generated. Most of these accounts will be service users, used by the various Intel\u00ae SecL services to work together. Another set of users will be used for installation permissions, and a final administrative user will be created to provide the initial authentication interface for the actual human user. The administrative user can be used to create additional users with appropriately restricted roles based on organizational needs. Creating these required users and roles is facilitated by a script that will accept credentials and some configuration settings from an answer file and automate the process. Create the populate-users.env file: ISECL_INSTALL_COMPONENTS = KBS,TA,WLS,WPM,IHUB,HVS,WLA,AAS AAS_API_URL = https://<AAS IP address or hostname>:8444/aas/v1 AAS_ADMIN_USERNAME = <AAS username> AAS_ADMIN_PASSWORD = <AAS password> HVS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Host Verification Service> IH_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Integration Hub> WLS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Workload Service> KBS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Key Broker Service> TA_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Trust Agent> HVS_SERVICE_USERNAME = <Username for the HVS service user> HVS_SERVICE_PASSWORD = <Password for the HVS service user> IHUB_SERVICE_USERNAME = <Username for the Hub service user> IHUB_SERVICE_PASSWORD = <Password for the Hub service user> WPM_SERVICE_USERNAME = <Username for the WPM service user> WPM_SERVICE_PASSWORD = <Password for the WPM service user> WLS_SERVICE_USERNAME = <Username for the WLS service user> WLS_SERVICE_PASSWORD = <Password for the WLS service user> WLA_SERVICE_USERNAME = <Username for the WLA service user> WLA_SERVICE_PASSWORD = <Password for the WLA service user> GLOBAL_ADMIN_USERNAME = <Username for the global Administrator user GLOBAL_ADMIN_PASSWORD = <Password for the global Administrator user INSTALL_ADMIN_USERNAME = <Username for the installation user INSTALL_ADMIN_PASSWORD = <Password for the global installation user Note The ISECL_INSTALL_COMPONENTS variable is a comma-separated list of the components that will be used in your environment. Not all services are required for every use case. If a given service will not be used in your deployment, simply delete the unnecessary service abbreviation from the ISECL_INSTALL_COMPONENTS list, and leave the SAN and credential variables for that service blank. Note The SAN list variables each support wildcards( \"*\" and \"?\"). In particular, without wildcards the Trust Agent SAN list would need to explicitly list each hostname or IP address for all Trust Agents that will be installed, which is not generally feasible. Using wildcards, domain names and entire IP ranges can be included in the SAN list, which will allow any host matching those ranges to install the relevant service. The SAN list specified here must exactly match the SAN list for the applicable service in that service\u2019s env installation file. Execute the populate-users script: ./populate-users Note The script can be executed with the \u2013output_json argument to create the populate-user.json .This json output file will contain all of the users created by the script, along with usernames, passwords, and role assignments. This file can be used both as a record of the service and administrator accounts, and can be used as alternative inputs to recreate the same users with the same credentials in the future if needed. Be sure to protect this file if the \u2013output_json argument is used. The script will automatically generate the following users: Verification Service User Integration Hub Service User Workload Policy Manager Service User Workload Service User Name Workload Service User Global Admin User Installation User These user accounts will be used during installation of several of the Intel\u00ae SecL-DC applications. In general, whenever credentials are required by an installation answer file, the variable name should match the name of the corresponding variable used in the populate-users.env file. The Global Admin user account has all roles for all services. This is a default administrator account that can be used to perform any task, including creating any other users. In general this account is useful for POC installations, but in production it should be used only to create user accounts with more restrictive roles. The administrator credentials should be protected and not shared. The populate-users script will also output an installation token. This token has all privileges needed for installation of the Intel\u00ae SecL services, and uses the credentials provided with the INSTALLATION_ADMIN_USERNAME and password. The remaining Intel \u00ae SecL-DC services require this token (set as the BEARER_TOKEN variable in the installation env files) to grant the appropriate privileges for installation. By default this token will be valid for two hours; the populate-users script can be rerun with the same populate-users.env file to regenerate the token if more time is required, or the INSTALLATION_ADMIN_USERNAME and password can be used to generate an authentication token. Installing the Host Verification Service This section details how to install the Intel\u00ae SecL-DC services. For instructions on running these services as containers, see the following section. Required For The Host Verification Service is REQUIRED for all use cases. Platform Integrity with Data Sovereignty and Signed Flavors Application Integrity Workload Confidentiality (both VMs and Containers) Prerequisites The following must be completed before installing the Verification Service: The Certificate Management Service must be installed and available The Authentication and Authorization Service must be installed and available The Verification Service database must be available Package Dependencies The Intel\u00ae Security Libraries Verification Service requires the following packages and their dependencies: logback Postgres* client and server 11.6 (server component optional if an external Postgres database is used) unzip zip openssl wget net-tools python3-policycoreutils If they are not already installed, the Verification Service installer attempts to install these automatically using the package manager. Automatic installation requires access to package repositories (the RHEL subscription repositories, the EPEL repository, or a suitable mirror), which may require an Internet connection. If the packages are to be installed from the package repository, be sure to update the repository package lists before installation. Supported Operating Systems The Intel\u00ae Security Libraries Verification Service supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04 Recommended Hardware 4 vCPUs RAM: 8 GB 100 GB One network interface with network access to all managed servers (Optional) One network interface for Asset Tag provisioning (only required for \u201cpull\u201d tag provisioning; required to provision Asset Tags to VMware ESXi servers). Installation To install the Verification Service, follow these steps: Copy the Verification Service installation binary to the /root directory. Create the hvs.env installation answer file. A sample minimal hvs.env file is provided below. For all configuration options and their descriptions, refer to the Intel\u00ae SecL Configuration section on the Verification Service. # Authentication URL and service account credentials AAS_API_URL = https://isecl-aas:8444/aas/v1 HVS_SERVICE_USERNAME = <username> HVS_SERVICE_PASSWORD = <password> # CMS URL and CMS webserivce TLS hash for server verification CMS_BASE_URL = https://isecl-cms:8445/cms/v1 CMS_TLS_CERT_SHA384 = <digest> # TLS Configuration SAN_LIST = 127 .0.0.1,192.168.1.1,hvs.server.com #comma-separated list of IP addresses and hostnames for the HVS to be used in the Subject Alternative Names list in the TLS Certificate # Installation admin bearer token for CSR approval request to CMS BEARER_TOKEN = eyJhbGciOiJSUzM4NCIsImtpZCI6ImE\u2026 # Database HVS_DB_NAME = <database name> HVS_DB_USERNAME = <database username> HVS_DB_PASSWORD = <database password> HVS_DB_SSLCERTSRC = /tmp/dbcert.pem # Not required if VS_DB_SSLCERT is given Execute the installer binary. ./hvs-v4.0.0.bin When the installation completes, the Verification Service is available. The services can be verified by running hvs status from the Verification Service command line. hvs status Installing the Workload Service Required For The WLS is REQUIRED for the following use cases. Workload Confidentiality (both VMs and Containers) Prerequisites The following must be completed before installing the Workload Service: The Certificate Management Service must be installed and available The Authentication and Authorization Service must be installed and available The Verification Service must be installed and available The Workload Service database must be available Supported Operating Systems The Intel\u00ae Security Libraries Workload Service supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04 Recommended Hardware Installation Copy the Workload Service installation binary to the /root directory. Create the workload-service.env installation answer file WLS_DB_USERNAME = <database username> WLS_DB_PASSWORD = <database password> WLS_DB_HOSTNAME = <IP or hostname of database server> WLS_DB_PORT = <Database port ; 5432 by default> WLS_DB = <name of the WLS database> WLS_DB_SSLCERTSRC = <path to database TLS certificate ; the default location is typically /usr/local/pgsql/data/server.crt > HVS_URL = https://<Ip address or hostname of the Host verification Service>:8443/hvs/v2/ WLS_SERVICE_USERNAME = <username for WLS service account> WLS_SERVICE_PASSWORD = <password for WLS service account> CMS_BASE_URL = https://<IP or hostname to CMS>:8445/cms/v1/ CMS_TLS_CERT_SHA384 = <sha384 of CMS TLS certificate> AAS_API_URL = https://<IP or hostname to AAS>:8444/aas/v1/ SAN_LIST = <comma-separated list of IPs and hostnames for the WLS> BEARER_TOKEN = <Installation token from populate-users script> Execute the WLS installer binary: ./wls-v4.0.0.bin Installing the Trust Agent for Linux Required For The Trust Agent for Linux is REQUIRED for all use cases. Platform Integrity with Data Sovereignty and Signed Flavors Application Integrity Workload Confidentiality (both VMs and Containers) Package Dependencies The Trust Agent requires the following packages and their dependencies: Tboot (Optional, for TXT-based deployments without UEFI SecureBoot only) openssl tar redhat-lsb If they are not already installed, the Trust Agent installer attempts to install these automatically using the package manager. Automatic installation requires access to package repositories (the RHEL subscription repositories, the EPEL repository, or a suitable mirror), which may require an Internet connection. If the packages are to be installed from the package repository, be sure to update the repository package lists before installation. Tboot will not be installed automatically. Instructions for installing and configuring tboot are documented later in this section. Supported Operating Systems The Intel\u00ae Security Libraries Trust Agent for Linux supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04 Prerequisites The following must be completed before installing the Trust Agent: Supported server hardware including an Intel\u00ae Xeon\u00ae processor with Intel Trusted Execution Technology activated in the system BIOS. Trusted Platform Module (version 2.0) installed and activated in the system BIOS, with cleared ownership status OR a known TPM ownership secret. Note Starting in Intel SecL-DV 4.0, the Trust Agent will now default to using a null TPM owner secret, and does not require ownership permissions except during the provisioning step. If ownership has already been taken when the Trust Agent will be provisioned, it will be necessary to either provide the ownership secret or clear the TPM ownership before provisioning. System must be booted to a tboot boot option OR use UEFI SecureBoot. (Provisioning step only) Intel\u00ae SecL Verification Service server installed and active. (Required for NATS mode only) A NATS server must be configured and available (REQUIRED for servers configured with TXT and tboot only) If the server is installed using an LVM, the LVM name must be identical for all Trust Agent systems. The Grub bootloader line that calls the Linux kernel will contain the LVM name of the root volume, and this line with all arguments is part of what is measured in the TXT/Tboot boot process. This will cause the OS Flavor measurements to differ between two otherwise identical hosts if their LVM names are different. Simply using a uniform name for the LVM during OS installation will resolve this possible discrepancy. (Optional, REQUIRED for Virtual Machine Confidentiality only): QEMU/KVM must be installed Libvirt must be installed Tboot Installation Tboot is required to build a complete Chain of Trust for Intel\u00ae TXT systems that are not using UEFI Secure Boot. Tboot acts to initiate the Intel\u00ae TXT SINIT ACM (Authenticated Code Module), which populates several TPM measurements including measurement of the kernel, grub command line, and initrd. Without either tboot or UEFI Secure Boot, the Chain of Trust will be broken because the OS-related components will be neither measured nor signature-verified prior to execution. Because tboot acts to initiate the Intel\u00ae TXT SINIT ACM, tboot is only required for platforms using Intel\u00ae TXT, and is not required for platforms using another hardware Root of Trust technology like Intel\u00ae Boot Guard. Intel\u00ae SecL-DC requires tboot 1.10.1 or greater. This may be a later version of tboot than is available on public software repositories. The most current version of tboot can be found here: https://sourceforge.net/projects/tboot/files/tboot/ Tboot requires configuration of the grub boot loader after installation. To install and configure tboot: Install tboot yum install tboot If the package manager does not support a late enough version of tboot, it will need to be compiled from source and installed manually. Instructions can be found here: https://sourceforge.net/p/tboot/wiki/Home/ Note that the step \"copy platform SINIT to /boot\" should not be required, as datacenter platforms include the SINIT in the system BIOS package. Ensure that multiboot2.mod and relocator.mod are available for grub2 This step may not be necessary for all OS versions, for instance, this step is NA in case of Tboot installation on Ubuntu 18.04. In order to utilize tboot, grub2 requires these two modules from the grub2-efi-x64-modules package to be located in the correct directory (if they're absent, the host will throw a grub error when it tries to boot using tboot). These files must be present in this directory: /boot/efi/EFI/redhat/x86_64-efi/multiboot2.mod /boot/efi/EFI/redhat/x86_64-efi/relocator.mod If the files are not present in this directory, they can be moved from their installation location: cp /usr/lib/grub/x86_64-efi/multiboot2.mod /boot/efi/EFI/redhat/x86_64-efi/ cp /usr/lib/grub/x86_64-efi/relocator.mod /boot/efi/EFI/redhat/x86_64-efi/ Make a backup of your current grub.cfg file The below examples assume a RedHat OS that has been installed on a platform using UEFI boot mode. The grub path will be slightly different for platforms using a non-RedHat OS. cp /boot/efi/EFI/redhat/grub.cfg /boot/efi/EFI/redhat/grub.cfg.bak Generate a new grub.cfg with the tboot boot option # For RHEL grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg # For Ubuntu grub-mkconfig -o /boot/efi/EFI/redhat/grub.cfg Update the default boot option Ensure that the GRUB_DEFAULT value is set to the tboot option. a. Update /etc/default/grub and set the GRUB_DEFAULT value to 'tboot-1.10.1' GRUB_DEFAULT='tboot-1.10.1' b. Regenerate grub.cfg : # For RHEL grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg # For Ubuntu grub-mkconfig -o /boot/efi/EFI/redhat/grub.cfg Reboot the system Because measurement happens at system boot, a reboot is needed to boot to the tboot boot option and populate measurements in the TPM. Verify a successful trusted boot with tboot Tboot provides the txt-stat command to show the tboot log. The first part of the output of this command can be used to verify a successful trusted launch. In the output below, note the \u201cTXT measured launch\u201d and \u201csecrets flag set\u201d at the bottom. Both of these should show \" TRUE \" if the tboot measured launch was successful. If either of these show \" FALSE \" the measured launch has failed. This usually simply indicates that the tboot boot option was not selected during boot. If the measured launch was successful, proceed to install the Trust Agent. Intel(r) TXT Configuration Registers: STS: 0x0001c091 senter_done: TRUE sexit_done: FALSE mem_config_lock: FALSE private_open: TRUE locality_1_open: TRUE locality_2_open: TRUE ESTS: 0x00 txt_reset: FALSE E2STS: 0x0000000000000006 secrets: TRUE ERRORCODE: 0x00000000 DIDVID: 0x00000001b0078086 vendor_id: 0x8086 device_id: 0xb007 revision_id: 0x1 FSBIF: 0xffffffffffffffff QPIIF: 0x000000009d003000 SINIT.BASE: 0x6fec0000 SINIT.SIZE: 262144B (0x40000) HEAP.BASE: 0x6ff00000 HEAP.SIZE: 1048576B (0x100000) DPR: 0x0000000070000051 lock: TRUE top: 0x70000000 size: 5MB (5242880B) PUBLIC.KEY: 9c 78 f0 d8 53 de 85 4a 2f 47 76 1c 72 b8 6a 11 16 4a 66 a9 84 c1 aa d7 92 e3 14 4f b7 1c 2d 11 *********************************************************** TXT measured launch: TRUE secrets flag set: TRUE *********************************************************** NATS Mode vs HTTP Mode The Trust Agent can operate in either HTTP mode (default) or NATS mode. This distinction controls how the Agent communicates with the HVS. In HTTP mode, the TAgent presents a set of API endpoints for the HVS to access via individual TLS requests. In NATS mode, the Agent and HVS are connected via a persistent session maintained via a NATS server; in this mode, the Trust Agent will not listen on any HTTP ports. Installation Installation of the Trust Agent is split into two major steps: Installation, which covers the creation of system files and folders, and Provisioning, which involves the creation of keys and secrets and links the Trust Agent to a specific Verification Service. Both operations can be performed at the same time using an installation answer file. Without the answer file, the Trust Agent can be installed and left in an un-provisioned state regardless of whether a Verification Service is up and running, until such time as the datacenter administrator is ready to run the provisioning step and link the Trust Agent to a Verification Service. To install the Trust Agent for Linux: Copy the Trust Agent installation binary to the /root/ directory. (Optional; required to perform Provisioning and Installation at the same time.) Create the trustagent.env answer file in the /root directory (for full configuration options, see section 9.2). The minimum configuration options for installation are provided below. For Platform Attestation only, provide the following in trustagent.env HVS_URL = https://<Verification Service IP or Hostname>:8443/hvs/v2 PROVISION_ATTESTATION = y GRUB_FILE = <path to grub.cfg> CURRENT_IP = <Trust Agent IP address> CMS_TLS_CERT_SHA384 = <CMS TLS digest> BEARER_TOKEN = <Installation token from populate-users script> AAS_API_URL = https://<AAS IP or Hostname>:8444/aas/v1 CMS_BASE_URL = https://<CMS IP or Hostname>:8445/cms/v1 SAN_LIST = <Comma-separated list of IP addresses and hostnames for the TAgent matching the SAN list specified in the populate-users script ; may include wildcards> For Workload Confidentiality with VM Encryption, add the following ( in addition to the basic Platform Attestation sample): WLA_SERVICE_USERNAME = <Username for the WLA service user> WLA_SERVICE_PASSWORD = <Username for the WLA service user> WLS_API_URL = https://<WLS IP address or hostname>:5000/wls/ For Workload Confidentiality with Container Encryption, add the following ( in addition to the basic Platform Attestation sample): WLA_SERVICE_USERNAME = <Username for the WLA service user> WLA_SERVICE_PASSWORD = <Username for the WLA service user> WLS_API_URL = https://<WLS IP address or hostname>:5000/wls/ REGISTRY_SCHEME_TYPE = https ##For the CRI-O container runtime: WA_WITH_CONTAINER_SECURITY_CRIO = yes For NATS mode, add the following (in addition to the basic Platform Attestation sample and any other optional features): TA_SERVICE_MODE=outbound NATS_SERVERS=<nats-server-ip>:4222 TA_HOST_ID=<Any unique identifier for the host; this could be the server FQDN, a UUID, or any other unique identifier> Note that the TA_HOST_ID unique identifier will also be the ID used as part of the connection string to reach this Trust Agent host in NATS mode. Execute the Trust Agent installer and wait for the installation to complete. ./trustagent-v4.0.0.bin If the trustagent.env answer file was provided with the minimum required options, the Trust Agent will be installed and also Provisioned to the Verification Service specified in the answer file. If no answer file was provided, the Trust Agent will be installed, but will not be Provisioned. TPM-related functionality will not be available from the Trust Agent until the Provisioning step is completed. The Trust Agent will add a new grub menu entry for application measurement. This new entry will include tboot if the existing grub contains tboot as the default boot option. Note If the Linux Trust Agent is installed without being Provisioned, the Trust Agent process will not actually run until the Provisioning step has been completed. Legacy BIOS systems using tboot ONLY) Update the grub boot loader: grub2-mkconfig -o /boot/grub2/grub.cfg After Provisioning is completed, the Linux Trust Agent must be rebooted so that the default SOFTWARE Flavor manifest can be measured and extended to the TPM. If the Workload Agent will also be installed on the system (see the next section), wait to reboot the server until after the Workload Agent has been installed, as this modifies the default SOFTWARE Flavor manifest. Installing the Workload Agent Required For Workload Confidentiality (both VMs and Containers) Supported Operating Systems The Intel\u00ae Security Libraries Workload Agent supports Red Hat Enterprise Linux 8.2 Prerequisites The following must be completed before installing the Workload Agent: Intel\u00ae SecL Trust Agent installed and active. cryptsetup (REQUIRED for Virtual Machine Confidentiality only): QEMU/KVM must be installed libvirt must be installed Installation Copy the Workload Agent installation binary to the /root/ directory Verify that the trustagent.env answer file is present. This file was necessary for installing/provisioning the Trust Agent. Note that the additional content required for Workload Confidentiality with either VM Encryption or Container Encryption must be included in the trustagent.env file (samples provided in the previous section) for use by the Workload Agent. Execute the Workload Agent installer binary. ./workload-agent-v4.0.0.bin Reboot the server. The Workload Agent populates files that are needed for the default SOFTWARE Flavor, and a reboot is required for those measurements to happen. Trust Agent Provisioning \"Provisioning\" the Trust Agent involves connecting to a Verification Service to download the Verification Service PrivacyCA certificate, create a new Attestation Identity Keypair in the TPM, and verify or create the TPM Endorsement Certificate and Endorsement Key. The Verification Service PrivacyCA root certificate is used to sign the EC, and the EC is used to generate the Attestation Identity Keypair. The AIK is used by the Verification Service to verify the integrity of quotes from the host\u2019s TPM. Provisioning is the only time that the Trust Agent requires TPM ownership permissions. If no TPM ownership secret is provided in the trustagent.env file, the Agent will use a null ownership secret to perform the provisioning steps. If a TPM ownership secret is provided in the trustagent.env answer file, the Agent will attempt to use the specified secret. If TPM ownership is in a clear state, the Agent will take ownership if a secret is specified. If the TPM is already \"owned,\" the Agent will try to use the specified secret; if the specified secret does not match the actual ownership password, the provisioning will fail. Intel recommends using the default \"null\" ownership secret, as this makes it easy for other applications to also use the Trust Agent, and can prevent the need to clear ownership in the case of a need to re-provision. Provisioning can be performed separately from installation (meaning you can install the Trust Agent without Provisioning, and then Provision later). If the trustagent.env answer file is present and has the required Verification Service information during installation, the Agent will automatically run the Provisioning steps. Note The trustagent.env answer file must contain user credentials for a user with sufficient privileges. The minimum role required for performing provisioning is the \"AttestationRegister\" role. Note If the Linux Trust Agent is installed without being Provisioned, the Trust Agent process will not actually run until the Provisioning step has been completed. If the answer file is not present during installation, the Agent can be provisioned later by adding the trustagent.env file and running the following command: tagent setup -f <trustagent.env file path> Trust Agent Registration Registration creates a host record with connectivity details and other host information in the Verification Service database. This host record will be used by the Verification Service to retrieve TPM attestation quotes from the Trust Agent to generate an attestation report. The Trust Agent can register the host with a Verification Service by running the following command (the trustagent.env answer file must be present in the current working directory): tagent setup create-host Hosts can also be registered using a REST API request to the Verification Service: POST <https://verification.service.com:8443/hvs/v2/hosts> { \"host_name\": \"<hostname of host to be registered>\" \"connection_string\": \"intel:https://<hostname or IP address>:1443\", \"flavorgroup_names\": [], \"description\": \"<description>\" } Note When a new host is registered, the Verification Service will automatically attempt to match the host to appropriate Flavors. If appropriate Flavors are not found, the host will still be registered, but will be in an Untrusted state until/unless appropriate Flavors are added to the Verification Service. Importing the HOST_UNIQUE Flavor RHEL and VMWare ESXi hosts have measured components that are unique to each host. This means that a special HOST_UNIQUE flavor part needs to be imported for each RHEL and ESXi host, in addition to any other OS or Platform Flavors. Note Importing a Flavor requires user credentials for a user with sufficient privileges. The minimum role required for creating the HOST_UNIQUE Flavor part is the \u201chost_unique_flavor_creator\u201d role. This role can only create HOST_UNIQUE Flavor parts, and cannot create any other Flavors. On Red Hat Enterprise Linux hosts with the Trust Agent, this can be performed from the Trust Agent command line (this requires the trustagent.env answer file to be present in the current working directory): tagent setup create-host-unique-flavor This can also be performed using a REST API (required for VMWare ESXi hosts): POST https://verification.service.com:8443/hvs/v2/flavors { \"connection_string\": \"<Connection string>\", \"partial_flavor_types\": [\"HOST_UNIQUE\"] } Installing the Intel\u00ae SecL Kubernetes Extensions and Integration Hub Intel\u00ae SecL uses Custom Resource Definitions to add the ability to base orchestration decisions on Intel\u00ae SecL security attributes to Kubernetes. These CRDs allow Kubernetes administrators to configure pods to require specific security attributes so that the Kubernetes Control Plane Node will schedule those pods only on Worker Nodes that match the specified attributes. Two CRDs are required for integration with Intel\u00ae SecL \u2013 an extension for the Control Plane nodes, and a scheduler extension. The extensions are deployed as a Kubernetes deployment in the isecl namespace. Deploy Intel\u00ae SecL Custom Controller #Install skopeo to load container image for controller and scheduler from archive dnf install -y skopeo Copy isecl-k8s-extensions-*.tar.gz to Kubernetes Control plane machine and extract the contents #Copy scp /<build_path>/binaries/isecl-k8s-extensions-*.tar.gz <user>@<k8s_controller_machine>:/<path>/ #Extract tar -xvzf /<path>/isecl-k8s-extensions-*.tar.gz cd /<path>/isecl-k8s-extensions/ Create hostattributes.crd.isecl.intel.com CRD #1.14<=k8s_version<=1.16 kubectl apply -f yamls/crd-1.14.yaml #1.16<=k8s_version<=1.18 kubectl apply -f yamls/crd-1.17.yaml Check whether the CRD is created kubectl get crds Load the isecl-controller container image cd /<path>/isecl-k8s-extensions/ skopeo copy oci-archive:<isecl-k8s-controller-*.tar> docker://<docker_private_registry_server>:5000/<imageName>:<tagName> Udate image name as above in controller yaml \"/opt/isecl-k8s-extensions/yamls/isecl-controller.yaml\" containers: - name: isecl-controller image: <docker_private_registry_server>:5000/<imageName>:<tagName> Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterRoleBinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole = system:node --user = system:serviceaccount:isecl:isecl Fetch token required for ihub installation kubectl get secrets -n isecl #The below token will be used for ihub installation to update 'KUBERNETES_TOKEN' in ihub.env when configured with Kubernetes Tenant kubectl describe secret default-token-<name> -n isecl Additional Optional Configurable fields for isecl-controller configuration in isecl-controller.yaml Field Required Type Default Description LOG_LEVEL Optional string INFO Determines the log level LOG_MAX_LENGTH Optional int 1500 Determines the maximum length of characters in a line in log file TAG_PREFIX Optional string isecl A custom prefix which can be applied to isecl attributes that are pushed from IH. For example, if the tag-prefix is isecl. and trusted attribute in CRD becomes isecl.trusted . TAINT_UNTRUSTED_NODES Optional string false If set to true. NoExec taint applied to the nodes for which trust status is set to false, Applicable only for HVS based attestation Installing the Intel\u00ae SecL Integration Hub Note The Integration Hub is only required to integrate Intel\u00ae SecL with third-party scheduler services, such as OpenStack Nova or Kubernetes. The Hub is not required for usage models that do not require Intel\u00ae SecL security attributes to be pushed to an integration endpoint. Required For The Hub is REQUIRED for the following use cases. Workload Confidentiality (both VMs and Containers) The Hub is OPTIONAL for the following use cases (used only if orchestration or other integration support is needed): Platform Integrity with Data Sovereignty and Signed Flavors Application Integrity Deployment Architecture Considerations for the Hub A separate Hub instance is REQUIRED for each Cloud environment (also referred to as a Hub \"tenant\"). For example, if a single datacenter will have an OpenStack cluster and also two separate Kubernetes clusters, a total of three Hub instances must be installed, though additional instances of other Intel SecL services are not required (in the same example, only a single Verification Service is required). Each Hub will manage a single orchestrator environment. Each Hub instance should be installed on a separate VM or physical server Prerequisites The Intel\u00ae Security Libraries Integration Hub can be run as a VM or as a bare-metal server. The Hub may be installed on the same server (physical or VM) as the Verification Service. The Verification Service must be installed and available The Authentication and Authorization Service must be installed and available The Certificate Management Service must be installed and available (REQUIRED for Kubernetes integration only) The Intel SecL Custom Resource Definitions must be installed and available (see the Integration section for details) Package Dependencies The Intel\u00ae SecL Integration Hub requires a number of packages and their dependencies: If these are not already installed, the Integration Hub installer attempts to install these packages automatically using the package manager. Automatic installation requires access to package repositories (the RHEL subscription repositories, the EPEL repository, or a suitable mirror), which may require an Internet connection. If the packages are to be installed from the package repository, be sure to update your repository package lists before installation. Supported Operating Systems The Intel Security Libraries Integration Hub supports Red Hat Enterprise Linux 8.2 Recommended Hardware 1 vCPUs RAM: 2 GB 1 GB free space to install the Verification Service services. Additional free space is needed for the Integration Hub database and logs (database and log space requirements are dependent on the number of managed servers). One network interface with network access to the Verification Service. One network interface with network access to any integration endpoints (for example, OpenStack Nova). Installing the Integration Hub To install the Integration Hub, follow these steps: Copy the API Server certificate of the Kubernetes Controller to machine where Integration Hub will be installed to /root/ directory Note In most Kubernetes distributions the Kubernetes certificate and key is normally present under /etc/kubernetes/pki . However this might differ in case of some specific Kubernetes distributions. In ihub.env KUBERNETES_TOKEN token can be retrieved from Kubernetes using the following command: kubectl get secrets -n isecl -o jsonpath=\"{.items[?(@.metadata.annotations['kubernetes\\.io/service-account\\.name']=='default')].data.token}\"|base64 --decode KUBERNETES_CERT_FILE=/<any_path>/apiserver.crt in this path can be specified any ex: /root which can taken care by IHUB during installation and copied to '/etc/ihub' directory. Create the ihub.env installation answer file. See the sample file below. # Authentication URL and service account credentials AAS_API_URL = https://isecl-aas:8444/aas/v1 IHUB_SERVICE_USERNAME = <Username for the Hub service user> IHUB_SERVICE_PASSWORD = <Password for the Hub service user> # CMS URL and CMS webserivce TLS hash for server verification CMS_BASE_URL = https://isecl-cms:8445/cms/v1 CMS_TLS_CERT_SHA384 = <TLS hash> # TLS Configuration TLS_SAN_LIST = 127 .0.0.1,192.168.1.1,hub.server.com #comma-separated list of IP addresses and hostnames for the Hub to be used in the Subject Alternative Names list in the TLS Certificate # Verification Service URL HVS_BASE_URL = https://isecl-hvs:8443/hvs/v2 ATTESTATION_TYPE = HVS #Integration tenant type. Currently supported values are \"KUBENETES\" or \"OPENSTACK\" TENANT = <KUBERNETES or OPENSTACK> # OpenStack Integration Credentials - required for OpenStack integration only OPENSTACK_AUTH_URL = <OpenStack Keystone URL ; typically http://openstack-ip:5000/> OPENSTACK_PLACEMENT_URL = <OpenStack Nova API URL ; typically http://openstack-ip:8778/> OPENSTACK_USERNAME = <OpenStack username> OPENSTACK_PASSWORD = <OpenStack password> # Kubernetes Integration Credentials - required for Kubernetes integration only KUBERNETES_URL = https://kubernetes:6443/ KUBERNETES_CRD = custom-isecl KUBERNETES_CERT_FILE = /root/apiserver.crt KUBERNETES_TOKEN = eyJhbGciOiJSUzI1NiIsImtpZCI6Ik...... # Installation admin bearer token for CSR approval request to CMS - mandatory BEARER_TOKEN = eyJhbGciOiJSUzM4NCIsImtpZCI6ImE\u2026 * Update the token obtained in Step 8 of Deploy Intel\u00ae SecL Custom Controller along with other relevant tenant configuration options in ihub.env Copy the Integration Hub installation binary to the /root directory & execute the installer binary. ./ihub-v4.0.0.bin Copy the /etc/ihub/ihub_public_key.pem to Kubernetes Controller machine to /<path>/secrets/ directory #On K8s-Controller machine mkdir -p /<path>/secrets #On IHUB machine, copy scp /etc/ihub/ihub_public_key.pem <user>@<k8s_controller_machine>:/<path>/secrets/hvs_ihub_public_key.pem After installation, the Hub must be configured to integrate with a Cloud orchestration platform (for example, OpenStack or Kubernetes). See the Integration section for details. Deploy Intel\u00ae SecL Extended Scheduler Install cfssl and cfssljson on Kubernetes Control Plane #Install wget dnf install wget -y #Download cfssl to /usr/local/bin/ wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl #Download cfssljson to /usr/local/bin wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create TLS key-pair for isecl-scheduler service which is signed by Kubernetes apiserver.crt cd /<path>/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh #Set K8s_CONTROLLER_IP,HOSTNAME export CONTROLLER_IP = <k8s_machine_ip> export HOSTNAME = <k8s_machine_hostname> #Create TLS key-pair ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \" $CONTROLLER_IP \" , \" $HOSTNAME \" -c <k8s_ca_authority_cert> -k <k8s_ca_authority_key> Note In most Kubernetes distributions the Kubernetes certificate and key is normally present under /etc/kubernetes/pki . However this might differ in case of some specific Kubernetes distributions. Copy the TLS key-pair generated to /<path>/secrets/ directory cp /<path>/isecl-k8s-extensions/server.key /<path>/secrets/ cp /<path>/isecl-k8s-extensions/server.crt /<path>/secrets/ Load the isecl-scheduler container image cd /<path>/isecl-k8s-extensions/ skopeo copy oci-archive:<isecl-k8s-scheduler-*.tar> docker://<docker_private_registry_server>:5000/<imageName>:<tagName> Update image name as above in scheduler yaml \"/opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml\" containers: - name: isecl-scheduler image: <docker_private_registry_server>:5000/<imageName>:<tagName> Create scheduler-secret for isecl-scheduler cd /<path>/ kubectl create secret generic scheduler-certs --namespace isecl --from-file = secrets The isecl-scheduler.yaml file includes support for both SGX and Workload Security put together. For only working with Workload Security scenarios , the following line needs to be made empty in the yaml file. The scheduler and controller yaml files are located under /<path>/isecl-k8s-extensions/yamls - name : SGX_IHUB_PUBLIC_KEY_PATH value : \"\" Deploy isecl-scheduler cd /<path>/isecl-k8s-extensions/ kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl Additional optional fields for isecl-scheduler configuration in isecl-scheduler.yaml Field Required Type Default Description LOG_LEVEL Optional string INFO Determines the log level LOG_MAX_LENGTH Optional int 1500 Determines the maximum length of characters in a line in log file TAG_PREFIX Optional string isecl. A custom prefix which can be applied to isecl attributes that are pushed from IH. For example, if the tag-prefix is *isecl.* and *trusted* attribute in CRD becomes *isecl.trusted* . PORT Optional int 8888 ISecl scheduler service port HVS_IHUB_PUBLIC_KEY_PATH Required string Required for IHub with HVS Attestation SGX_IHUB_PUBLIC_KEY_PATH Required string Required for IHub with SGX Attestation TLS_CERT_PATH Required string Path of tls certificate signed by kubernetes CA TLS_KEY_PATH Required string Path of tls key Configuring kube-scheduler to establish communication with isecl-scheduler Note The below is a sample when using kubeadm as the Kubernetes distribution, the scheduler configuration files would be different for any other Kubernetes distributions being used. Add a mount path to the /etc/kubernetes/manifests/kube-scheduler.yaml file for the Intel SecL scheduler extension: - mountPath : /<path>/isecl-k8s-extensions/ name : extendedsched readOnly : true Add a volume path to the /etc/kubernetes/manifests/kube-scheduler.yaml file for the Intel SecL scheduler extension: - hostPath : path : /<path>/isecl-k8s-extensions/ type : \"\" name : extendedsched Add policy-config-file path in the /etc/kubernetes/manifests/kube-scheduler.yaml file under command section: - command : - kube-scheduler - --policy-config-file=/<path>/isecl-k8s-extensions/scheduler-policy.json - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=true Restart kubelet systemctl restart kubelet Logs will be appended to older logs in /var/log/isecl-k8s-extensions Whenever the CRD's are deleted and restarted for updates, the CRD's using the yaml files present under /opt/isecl-k8s-extensions/yamls/ . Kubernetes Version 1.14-1.15 uses crd-1.14.yaml and 1.16-1.17 uses crd-1.17.yaml kubectl delete crd hostattributes.crd.isecl.intel.com kubectl apply -f /opt/isecl-k8s-extensions/yamls/crd-<version>.yaml (Optional) Verify that the Intel \u00ae SecL K8s extensions have been started: To verify the Intel SecL CRDs have been deployed: kubectl get -o json hostattributes.crd.isecl.intel.com Installing the Key Broker Service Required For The KBS is REQUIRED for the following use cases: Workload Confidentiality (both VMs and Containers) Prerequisites The following must be completed before installing the Key Broker: The Verification Service must be installed and available The Authentication and Authorization Service must be installed and available The Certificate Management Service must be installed and available (Recommended; Required if a 3 rd -party Key Management Server will be used) A KMIP 2.0-compliant 3 rd -party Key management Server must be available. The Key Broker will require the KMIP server\u2019s client certificate, client key and root ca certificate. The KMIP server's client certificate must contain a Subject Alternative Name that includes the KMIP server's hostname. The Key Broker uses the gemalto kmip-go client to connect to a KMIP server The Key Broker has been validated using the pykmip 0.9.1 KMIP server as a 3 rd -party Key Management Server. While any general KMIP 2.0-compliant Key Management Server should work, implementation differences among KMIP providers may prevent functionality with specific providers. Package Dependencies Supported Operating Systems The Intel\u00ae Security Libraries Key Broker Service supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04 Recommended Hardware Installation Copy the Key Broker installation binary to the /root/ directory. Create the installation answer file kbs.env: AAS_API_URL = https://<AAS IP or hostname>:8444/aas/v1 CMS_BASE_URL = https://<CMS IP or hostname>:8445/cms/v1/ ENDPOINT_URL = https://<KBS IP or hostname>:9443/kbs/v1/ SAN_LIST = <comma-separated list of hostnames and IP addresses for the Key Broker> CMS_TLS_CERT_SHA384 = <SHA384 hash of CMS TLS certificate> BEARER_TOKEN = <Installation token from populate-users script> ### OPTIONAL - KMIP configuration only KEY_MANAGER = KMIP KMIP_SERVER_IP = <IP address of KMIP server> KMIP_SERVER_PORT = <Port number of KMIP server> KMIP_VERSION = <KMIP protocol version> KMIP_USERNAME = <Username of KMIP server> KMIP_PASSWORD = <Password of KMIP server> ### KMIP_HOSTNAME must be used to provide, KMIP server certificate's SAN(IP/DNS) or valid COMMON NAME. Only FQDN names are allowed. KMIP_HOSTNAME = <Hostname of KMIP server> ### Retrieve the following certificates and keys from the KMIP server KMIP_CLIENT_KEY_PATH = <path>/client_key.pem KMIP_ROOT_CERT_PATH = <path>/root_certificate.pem KMIP_CLIENT_CERT_PATH = <path>/client_certificate.pem Execute the KBS installer. ./kbs-4.0.0.bin Configure the Key Broker to use a KMIP-compliant Key Management Server The Key Broker must be configured to use a 3 rd -party KMIP key manager as part of installation using kbs.env installation variables. To configure the Key Broker to point to a 3 rd -party KMIP-compliant Key Management Server: Copy the KMIP server\u2019s client certificate, client key and root ca certificate to the Key Broker system Configure the variables in kbs.env for kmip support as below during installation KEY_MANAGER = KMIP KMIP_SERVER_IP = <IP address of KMIP server> KMIP_SERVER_PORT = <Port number of KMIP server> KMIP_HOSTNAME = <hostname of the KMIP server. Must match the hostname used in the Subject Alternative Name fort eh KMIP server client certificate.> ## KMIP_VERSION variable can be used to mention KMIP protocol version. ## This is an OPTIONAL field, default value is set to '2.0'. KBS supports KMIP version '1.4' and '2.0'. KMIP_VERSION = <KMIP protocol version> ## KMIP_HOSTNAME can be used to configure TLS config with ServerName. ## KMIP server certificate should contain SAN(IP/DNS) or valid COMMON NAME and this value can be provided in KMIP_HOSTNAME. Only FQDN names are allowed. ## This is an OPTIONAL field, if KMIP_HOSTNAME is not provided then KMIP_SERVER_IP will be considered as ServerName in TLS configuration. KMIP_HOSTNAME = <Hostname of KMIP server> ## KMIP supports authentication mechanism to authenticate requestor. This is an OPTIONAL field. ## This feature can be added to KBS by updating kbs.env with KMIP_USERNAME and KMIP_PASSWORD. ## These are OPTIONAL variables. PyKMIP doesn't supports this feature. This feature is validated in Thales cipher trust manager. KMIP_USERNAME = <Username of KMIP server> KMIP_PASSWORD = <Password of KMIP server> ### Retrieve the following certificates and keys from the KMIP server KMIP_CLIENT_KEY_PATH = <path>/client_key.pem KMIP_ROOT_CERT_PATH = <path>/root_certificate.pem KMIP_CLIENT_CERT_PATH = <path>/client_certificate.pem The KBS configuration can be found in /etc/kbs/config.yml , KMIP configuration can be updated in this configuration shell kmip: version: \"2.0\" server-ip: \"127.0.0.1\" server-port: \"5696\" hostname: \"localhost\" kmip-username: \"<kmip-username>\" kmip-password: \"<kmip-password>\" client-key-path: \"<path>/client-key.pem\" client-cert-path: \"<path>/client-certificate.pem\" root-cert-path: \"<path>/root-certificate.pem\" Restart the Key Broker for the settings to take effect kbs stop kbs start Importing Verification Service Certificates After installation, the Key Broker must import the SAML and PrivacyCA certificates from any Verification Services it will trust. This provides the Key Broker a way to ensure that only attestations that come from a \u201cknown\u201d Verification Service. The SAML and PrivacyCA certificates needed can be found on the Verification Service. Importing a SAML certificate Display the SAML certificate: cat /etc/hvs/certs/trustedca/saml-crt.pem Use the SAML certificate output in the following POST call to the Key Broker: POST https://<Key Broker IP address or hostname>:9443/kbs/v1/saml-certificates Content-Type: application/x-pem-file -----BEGIN CERTIFICATE----- MIID9TCCAl2gAwIBAgIBCTANBgkqhkiG9w0BAQwFADBQMQswCQYDVQQGEwJVUzEL MAkGA1UECBMCU0YxCzAJBgNVBAcTAlNDMQ4wDAYDVQQKEwVJTlRFTDEXMBUGA1UE AxMOQ01TIFNpZ25pbmcgQ0EwHhcNMTkxMjExMTkzOTU1WhcNMjAxMjExMTkzOTU1 WjAYMRYwFAYDVQQDEw1tdHdpbHNvbi1zYW1sMIIBojANBgkqhkiG9w0BAQEFAAOC AY8AMIIBigKCAYEArbrDpzR4Ry0MVhSJULHZoiVL020YqtyRH+R2NlVXTpJzqmEA Ep2utfcP8+mSCT7DLpGBO6KACPCz3pmqj3wZyqZNTrG7IF2Z4Fuf641fPcxA3WVH 3lXz0L5Ep4jOUdfT8kj4hHxHJVJhDsW4J2fds2RGnn8bZG/QbmmGNRfqdxht0zMh 63ik8jBWNWHxYSRbck27FyTj9hDU+z+rFfIdNv1SiQ9FyndgOytK/m7ijoAetkSF bCsauzUL7DFdRzTmB2GCF/Zd957V51GNpvan6uwqDTL6T4NFX2sqoVduu/WIyTpO /6D2aA741CR3Bmk9945TSeDKZNz2HkihuE+d8ES68W1t4rvox/Noi74e0k35AqcQ Q3P0DZpD+XaRapz5CHcOPwOpZ3A/8wN2f+CS2HqDx8FwABkh7l8OdiIWs8+TDQZe 1x4e/50jE/8zMR/tsAy1EXkm3OTOVxih0u18J84x4OT+rHAIcoQ+TOJ40aHrWGHg kVCfiCUzYYT/W/RBAgMBAAGjEjAQMA4GA1UdDwEB/wQEAwIGwDANBgkqhkiG9w0B AQwFAAOCAYEAP/ABHdPquBNrMOCU+v7SfMLmIfJymA15mCorMEiKZ1d7oNnoPP0G pfyRA4TUiyFLCOLi4jIXWuu4Lt6RUz6bnzn8JRWD5ocIJGGxWjOA66xyS3o9iG7G otOh1pzp5wlwPG7r8ZJ7Q26J+NuHpN1GW5U5Vjww1J9rEKnsKp45QHkG2nXEujdx YXmKXtEG2gOMVjaLiqromf6VxbdNoKHZGEfqU3H5ymMgqIrnXl3MivA30CymCDLl rJGRQSwOfzywPCnUOAVptBwLs2kwOtdvnq+BTK3q/dKKoNiFURj/mQ70egquW9ly TOkYivmKqMZxZlq0//cre4K35aCW3ZArzGgNM8Pk0V/hZp8ZHrWLNAdo4w/Pj1oC Yq7R0RQ8jQerkewYBfrv3O3e9c22h48fsHnun6F3sbcDjws/sWJIOcrPyqJE26HY DmIKpvjqc0jI31ndBBwkb+RIBFkz1Ycob9rsW16uVqbjBFDjJ5QKOdXxhqulyboa JAF53vmU+1jE -----END CERTIFICATE----- Importing a PrivacyCA Certificate Use OpenSSL to display the PrivacyCA certificate content: openssl x509 -in /etc/hvs/certs/trustedca/privacy-ca/privacy-ca-cert.pem Use the PrivacyCA certificate output in the following POST call to the Key Broker: POST https://<Key Broker IP address or hostname>:9443/kbs/v1/tpm-identity-certificates Content-Type: application/x-pem-file -----BEGIN CERTIFICATE----- MIIHaDCCBdCgAwIBAgIGAW72eWZ9MA0GCSqGSIb3DQEBCwUAMBsxGTAXBgNVBAMT EG10d2lsc29uLXBjYS1haWswHhcNMTkxMjExMTkzOTQxWhcNMjkxMjEwMTkzOTQx WjAbMRkwFwYDVQQDExBtdHdpbHNvbi1wY2EtYWlrMIIBojANBgkqhkiG9w0BAQEF AAOCAY8AMIIBigKCAYEAmWqBr2YiycZbF/QgFbxTr4YiHtueWBdW0sibtH1QRSbI KtkbFsmr6J6QiLBaXcF7KVN6DaD0j5sU4cZSttqKwlSUnn07xjWJRP1EcvSaufO1 MarewgBpFQcI2T6aTs1ziV77BoKz0kWteURz1jT1KSwuattxTelpmgucDp98MqW/ uWsliHUVxh51JTE1yn7Vf1QCWz3a+NDH98Lgr5ks337yx3VBK59Dwtsmfsrd5tMn IuV9Jw0Y2UEdDi004FXI4q64MsMpWA7t5ONRAU+VNU0Y3saXeNBDg9J363imOHIH haP8ixDhqZ+Xb/TGafgFeEHBkJTv6bWpDqodbWVDbgZloxJzcNgtimQw3RbyrB3C KijlEo5BQY6bOcdMG7gCq77u/fbOvLb5IXzS8ZDpwuwCQNnBP4UJXwAflO7COG7P mpj9bTV1OtFiPtYFc4JdGdaf1Pl2zWGeR0c3PIzYQxqvtTVtFX+oRWRsgaEdxKf7 LJx4aIjXwP2s6PIiOSalAgMBAAGjggOwMIIDrDCCAbMGA1UdDgSCAaoEggGmMIIB ojANBgkqhkiG9w0BAQEFAAOCAY8AMIIBigKCAYEAmWqBr2YiycZbF/QgFbxTr4Yi HtueWBdW0sibtH1QRSbIKtkbFsmr6J6QiLBaXcF7KVN6DaD0j5sU4cZSttqKwlSU nn07xjWJRP1EcvSaufO1MarewgBpFQcI2T6aTs1ziV77BoKz0kWteURz1jT1KSwu attxTelpmgucDp98MqW/uWsliHUVxh51JTE1yn7Vf1QCWz3a+NDH98Lgr5ks337y x3VBK59Dwtsmfsrd5tMnIuV9Jw0Y2UEdDi004FXI4q64MsMpWA7t5ONRAU+VNU0Y 3saXeNBDg9J363imOHIHhaP8ixDhqZ+Xb/TGafgFeEHBkJTv6bWpDqodbWVDbgZl oxJzcNgtimQw3RbyrB3CKijlEo5BQY6bOcdMG7gCq77u/fbOvLb5IXzS8ZDpwuwC QNnBP4UJXwAflO7COG7Pmpj9bTV1OtFiPtYFc4JdGdaf1Pl2zWGeR0c3PIzYQxqv tTVtFX+oRWRsgaEdxKf7LJx4aIjXwP2s6PIiOSalAgMBAAEwDwYDVR0TAQH/BAUw AwEB/zCCAeAGA1UdIwSCAdcwggHTgIIBpjCCAaIwDQYJKoZIhvcNAQEBBQADggGP ADCCAYoCggGBAJlqga9mIsnGWxf0IBW8U6+GIh7bnlgXVtLIm7R9UEUmyCrZGxbJ q+iekIiwWl3BeylTeg2g9I+bFOHGUrbaisJUlJ59O8Y1iUT9RHL0mrnztTGq3sIA aRUHCNk+mk7Nc4le+waCs9JFrXlEc9Y09SksLmrbcU3paZoLnA6ffDKlv7lrJYh1 FcYedSUxNcp+1X9UAls92vjQx/fC4K+ZLN9+8sd1QSufQ8LbJn7K3ebTJyLlfScN GNlBHQ4tNOBVyOKuuDLDKVgO7eTjUQFPlTVNGN7Gl3jQQ4PSd+t4pjhyB4Wj/IsQ 4amfl2/0xmn4BXhBwZCU7+m1qQ6qHW1lQ24GZaMSc3DYLYpkMN0W8qwdwioo5RKO QUGOmznHTBu4Aqu+7v32zry2+SF80vGQ6cLsAkDZwT+FCV8AH5Tuwjhuz5qY/W01 dTrRYj7WBXOCXRnWn9T5ds1hnkdHNzyM2EMar7U1bRV/qEVkbIGhHcSn+yyceGiI 18D9rOjyIjkmpQIDAQABoR+kHTAbMRkwFwYDVQQDExBtdHdpbHNvbi1wY2EtYWlr ggYBbvZ5Zn0wDQYJKoZIhvcNAQELBQADggGBAC3PEB8Av0PBJgrJMxzMbuf1FCdD AUrfYmP81Hs0/v70efviMEF2s3GAyLHD9v+1nNFCQrjcNCar18k45BlcodBEmxKA DZoioFykRtlha6ByVvuN6wD93KQbKsXPKhUp8X67fLuOcQgfc3BoDRlw/Ha1Ib6X fliE+rQzLCOgClK7ZdTwl9Ok0VbR7Mbal/xShIqr2WopjBtal9p4RsnIxilTHI+m qzbV8zvZXYfYtEb3MMMT5EnjIV8O498KKOjxohD2vqaxqItd58pOi6z/q5f4pLHc DvdsJecJEoWb2bxWQdBgthMjX6AUV/B5G/LTfaPwVbTLdEc+S6Nrobf/TFYV0pvG OzF3ltYag0fupuYJ991s/JhVwgJhCGq7YourDGkNIWAjt0Z2FWuQKnxWvmResgkS WTeXt+1HCFSo5WcAZWV8R9FYv7tzFxPY8aoLj82sgrOE4IwRqaA8KMbq3anF4RCk +D8k6etqMcNHFS8Fj6GlCd80mb4Q3sxuCiBvZw== -----END CERTIFICATE----- Installing the Workload Policy Manager Required For The WPM is REQUIRED for the following use cases. Workload Confidentiality (both VMs and Containers) Package Dependencies Supported Operating Systems The Intel\u00ae Security Libraries Workload Policy Manager supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04 Recommended Hardware 2 vCPUs RAM: 8 GB 100 GB One network interface with network access to the Key Broker and Workload Service Additional memory and disk space may be required depending on the size of images to be encrypted Installation Copy the WPM installer to the /root directory Create the wpm.env answer file: KBS_BASE_URL = https://<IP address or hostname of the KBS>:9443/v1/ WPM_SERVICE_USERNAME = <WPM_Service username from populate-users script> WPM_SERVICE_PASSWORD = <WPM Service password from populate-users script> CMS_TLS_CERT_SHA384 = <Sha384 hash of the CMS TLS certificate> CMS_BASE_URL = https://<IP address or hostname for CMS>:8445/cms/v1/ AAS_API_URL = https://<Hostname or IP address of the AAS>:8444/aas/v1 BEARER_TOKEN = <Installation token from populate-users script> For Container Encryption only, add the following line to the wpm.env installation answer file: ##For the CRI-O container runtime: WPM_WITH_CONTAINER_SECURITY_CRIO = yes Execute the WPM installer: ./wpm-v4.0.0.bin","title":"Intel\u00ae Security Libraries Components"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#intel-security-libraries-components","text":"","title":"Intel\u00ae Security Libraries Components"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#intel-security-libraries-components_1","text":"Certificate Management Service Starting with Intel\u00ae SecL-DC 1.6, most non-TPM-related certificates used by Intel\u00ae SecL-DC applications will be issued by the new Certificate Management Service. This includes acting as a root CA and issuing TLS certificates for all of the various web services. Authentication and Authorization Service Starting with Intel\u00ae SecL-DC 1.6, authentication and authorization for all Intel\u00ae SecL applications will be centrally managed by the new Authentication and Authorization Service (AAS). Previously, each application would manage its own users and permissions independently; this change allows authentication and authorization management to be centralized. Verification Service The Verification Service component of Intel\u00ae Security Libraries performs the core Platform Integrity and Data Sovereignty functionality by acting as a remote attestation authority. Platform security technologies like Intel\u00ae TXT, Intel\u00ae BootGuard, and UEFI SecureBoot extend measurements of platform components (such as the system BIOS/UEFI, OS kernel, etc) to a Trusted Platform module as the server boots. Known-good measurements for each of these components can be directly imported from a sample server. These expected measurements can then be compared against actual measurements from registered servers, allowing the Verification Service to attest to the \"trustiness\" of the platform, meaning whether the platform booted into a \"known-good\" state. Workload Service The Workload Service acts as a management service for handling Workload Flavors (Flavors used for Virtual Machines and Containers). In the Intel\u00ae SecL-DC 1.6 release, the Workload Service uses Flavors to map decryption key IDs to image IDs. When a launch request for an encrypted workload image is intercepted by the Workload Agent, the Workload Service will handle mapping the image ID to the appropriate key ID and key request URL, and will initiate the key transfer request to the Key Broker. Trust Agent The Trust Agent resides on physical servers and enables both remote attestation and the extended chain of trust capabilities. The Agent maintains ownership of the server's Trusted Platform Module, allowing secure attestation quotes to be sent to the Verification Service. Incorporating the Intel\u00ae SecL HostInfo and TpmProvider libraries, the Trust Agent serves to report on platform security capabilities and platform integrity measurements. The Trust Agent is supported for Windows Server 2016 Datacenter and Red Hat Enterprise Linux (RHEL) 8.1 and later. Workload Agent The Workload Agent is the component responsible for handling all of the functions needed for Workload Confidentiality for virtual machines and containers on a physical server. The Workload Agent uses libvirt hooks to identify VM lifecycle events (VM start, stop, hibernate, etc), and intercepts those events to perform needed functions like requesting decryption keys, creation and deletion of encrypted LUKS volumes, using the TPM to unseal decryption keys, etc. The WLA also performs analogous functionality for containers. Integration Hub The Integration Hub acts as a middle-man between the Verification Service and one or more scheduler services (such as OpenStack* Nova), and \"pushes\" attestation information retrieved from the Verification Service to one or more scheduler services according to an assignment of hosts to specific tenants. In this way, Tenant A can receive attestation information for hosts that belong to Tenant A, but receive no information about hosts belonging to Tenant B. The Integration Hub serves to disassociate the process of retrieving attestations from actual scheduler queries, so that scheduler services can adhere to best practices and retain better performance at scale. The Integration Hub will regularly query the Intel\u00ae SecL Verification Service for SAML attestations for each host. The Integration Hub maintains only the most recent currently valid attestation for each host, and will refresh attestations when they would expire. The Integration Hub will verify the signature of the SAML attestation for each host assigned to a tenant, then parse the attestation status and asset tag information, and then will securely push the parsed key/value pairs to the plugin endpoints enabled. The Integration Hub features a plugin design for adding new scheduler endpoint types. Currently the Integration Hub supports OpenStack Nova and Kubernetes endpoint plugins. Other integration plugins may be added. Workload Policy Manager The Workload Policy Manager is a Linux command line utility used by an image owner to encrypt VM (qcow2) or container images, and to create an Image Flavor used to provide the encryption key transfer URL during launch requests. The WPM utility will use an existing or request a new key from the Key Broker Service, use that key to encrypt the image, and output the Image Flavor in JSON format. The encrypted image can then be uploaded to the image store of choice (like OpenStack Glance), and the Image Flavor can be uploaded to the Workload Service. The ID of the image on the image storage system is then mapped to the Image Flavor in the WLS; when the image is used to launch a new instance, the WLS will find the Image Flavor associated with that image ID, and use the Image Flavor to determine the key transfer URL. Key Broker Service The Key Broker Service is effectively a policy compliance engine. Its job is to manage key transfer requests, releasing keys only to servers that meet policy requirements. The Key Broker registers one or more SAML signing certificates from any Verification Services that it will trust. When a key transfer request is received, the request includes a trust attestation report signed by the Verification Service. If the signature matches a registered SAML key, the Broker will then look at the actual report to ensure the server requesting the key matches the image policy (currently only overall system trust is supported as a policy requirement). If the report indicates the policy requirements are met, the image decryption key is wrapped using a public key unique to the TPM of the host that was attested in the report, such that only the host that was attested can unseal the decryption key and gain access to the image.","title":"Intel\u00ae Security Libraries Components"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#intel-security-libraries-binary-installation","text":"Intel\u00ae SecL services can be deployed as direct binary installations (on bare metal or in VMs), or can be deployed as containers. This section details the binary-based installation of Intel SecL services; the next major section details container-based deployments. It is recommended to deploy all control-plane services (CMS, AAS, HVS, WLS) as either containers or binaries, and not a mix of the two. The Trust Agent/Workload Agent, KBS, and WPM can be installed as binaries or deployed as containers regardless of the installation method used for the control plane.","title":"Intel\u00ae Security Libraries Binary Installation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#building-binary-installers","text":"Intel\u00ae Security Libraries is distributed as open source code, and must be compiled into installation binaries before installation. Instructions and sample scripts for building the Intel\u00ae SecL-DC components can be found here. After the components have been built, the installation binaries can be found in the directories created by the build scripts. <servicename>/out/<servicename>.bin In addition, the build script will produce some sample database creation scripts that can be used during installation to configure database requirements (instructions are given in the installation sections): create_db: authservice/out/create_db.sh install_pgdb: authservice/out/install_pgdb.sh In addition, sample Ansible roles to automatically build and deploy a testbed environment are provided here. Also provided are sample API calls organized by workflows for Postman here.","title":"Building Binary Installers"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#hardware-considerations","text":"Intel\u00ae SecL-DC supports and uses a variety of Intel security features, but there are some key requirements to consider before beginning an installation. Most important among these is the Root of Trust configuration. This involves deciding what combination of TXT, Boot Guard, tboot, and UEFI Secure Boot to enable on platforms that will be attested using Intel\u00ae SecL. Key points: - At least one \"Static Root of Trust\" mechanism must be used (TXT and/or BtG) - For Legacy BIOS systems, tboot must be used (which requires TXT) - For UEFI mode systems, UEFI SecureBoot must be used* Use the chart below for a guide to acceptable configuration options. .","title":"Hardware Considerations"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#recommended-service-layout","text":"The Intel\u00ae SecL-DC services can be installed in a variety of layouts, partially depending on the use cases desired and the OS of the server(s) to be protected. In general, the Intel\u00ae SecL-DC applications can be divided into management services that are deployed on the network on the management plane, and host or node components that must be installed on each protected server. Management services can typically be deployed anywhere with network access to all of the protected servers. This could be a set of individual VMs per service; containers; or all installed on a single physical or virtual machine. Node components must be installed on each protected physical server. Typically this is needed for Windows and Linux deployments.","title":"Recommended Service Layout"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#platform-integrity","text":"The most basic use case enabled by Intel\u00ae SecL-DC, Platform Integrity requires only the Verification Service and, to protect Windows or Linux hosts, the Trust Agent. This also enables the Application Integrity use case by default for Linux systems. The Integration Hub may be added to provide integration support for OpenStack or Kubernetes. The Hub is often installed on the same machine as the Verification Service, but optionally can be installed separately.","title":"Platform Integrity"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#workload-confidentiality","text":"Workload Confidentiality introduces a number of additional services and agents. For a POC environment, all of the management services can be installed on a single machine or VM. This includes: - Certificate Management Service (CMS) - Authorization and Authentication Service (AAS) - Host Verification Service (HVS) - Workload Service (WLS) - Integration Hub (HUB) - Key Broker Service (KBS) with backend key management - Workload Policy Manager (WPM) In a production environment, it is strongly suggested that the WPM and KBS be deployed (with their own CMS and AAS) separately for each image owner. For a Cloud Service Provider, this would mean that each customer/tenant who will use the Workload Confidentiality feature would have their own dedicated AAS/CMS/KBS/WPM operated on their own networks, not controlled by the CSP. This is because the Key Broker and WPM are the tools used to define the policies that will allow images to launch, and these policies and their enforcement should remain entirely under the control of the image owner. The node components must be installed on each protected physical server: - Trust Agent (TA) - Workload Agent (WLA)","title":"Workload Confidentiality"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#recommended-service-layout-architecture-containerized-deployment-with-k8s","text":"The containerized deployment makes use of Kubernetes orchestrator for single node and multi node deployments. A single-node deployment uses Microk8s to deploy the entire control plane in a pod on a single device. This is best for POC or demo environments, but can also be used when integrating Intel SecL with another application that runs on a virtual machine - the single node deployment can run in the same VM as the integrated application to keep all functions local. Single Node: A multi-node deployment is a more typical Kubernetes architecture, where the Intel SecL management plane is simply deployed as a Pod, with the Intel SecL agents (the WLA and the TA, depending on use case) deployed as a DaemonSet. Multi Node: Services Deployments & Agent DaemonSets: Every service including databases will be deployed as separate K8s deployment with 1 replica, i.e(1 pod per deployment). Each deployment will be further exposed through k8s service and also will be having corresponding Persistent Volume Claims(PV) for configuration and log directories and mounted on persistent storage. In case of daemonsets/agents, the configuration and log directories will be mounted on respective Baremetal worker nodes. For stateful services which requires database like shvs, aas, scs, A separate database deployment will be created for each of such services. The data present on the database deployment will also made to persist on a NFS, through K8s persistent storage mechanism Networking within the Cluster: Networking Outside the Cluster:","title":"Recommended Service Layout &amp; Architecture - Containerized Deployment with K8s"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installingconfiguring-the-database","text":"The Intel\u00ae SecL-DC Authentication and Authorization Service (AAS) requires a Postgresql 11 database. Scripts (install_pgdb.sh, create_db.sh) are provided with the AAS that will automatically add the Postgresql repositories and install/configure a sample database. If this script will not be used, a Postgresql 11 database must be installed by the user before executing the AAS installation.","title":"Installing/Configuring the Database"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#using-the-provided-database-installation-script","text":"Install a sample Postgresql 11 database using the install_pgdb.sh script. This script will automatically install the Postgresql database and client packages required. Add the Postgresql repository: https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm Create the iseclpgdb.env answer file: ISECL_PGDB_IP_INTERFACES = localhost ISECL_PGDB_PORT = 5432 ISECL_PGDB_SAVE_DB_INSTALL_LOG = true ISECL_PGDB_CERT_DNS = localhost ISECL_PGDB_CERT_IP = 127 .0.0.1 Note that the values above assume that the database will be accessed locally. If the database server will be external to the Intel\u00ae SecL services, change these values to the hostname or FQDN and IP address where the client will access the database server. Run the following command: dnf module disable postgresql -y Execute the installation script: ./install_pgdb.sh Note The database installation only needs to be performed once if the same database server will be used for all services that require a database. Only the \"create_db\" step needs to be repeated if the database server will be shared.","title":"Using the Provided Database Installation Script"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#provisioning-the-database","text":"Each Intel\u00ae SecL service that uses a database (the Authentication and Authorization Service, the Verification Service, the Integration Hub, the Workload Service) requires its own schema and access. After installation, the database must be created initialized and tables created. Execute the create_db.sh script to configure the database. If a single shared database server will be used for each Intel\u00ae SecL service (for example, if all management plane services will be installed on a single VM), run the script multiple times, once for each service that requires a database. If separate database servers will be used (for example, if the management plane services will reside on separate systems and will use their own local database servers), execute the script on each server hosting a database. ./create_db.sh <database name> <database_username> <database_password> For example: ./create_db.sh isecl_hvs_db hvs_db_username hvs_db_password ./create_db.sh isecl_aas_db aas_db_username aas_db_password ./create_db.sh isecl_wls_db wls_db_username wls_db_password Note that the database name, username, and password details for each service must be used in the corresponding installation answer file for that service.","title":"Provisioning the Database"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#database-server-tls-certificate","text":"The database client for Intel\u00ae SecL services requires the database TLS certificate to authenticate communication with the database server. If the database server for a service is located on the same server that the service will run on, only the path to this certificate is needed. If the provided Postgres scripts are used, the certificate will be located in /usr/local/pgsql/data/server.crt If the database server will be run separately from the Intel\u00ae SecL service(s), the certificate will need to be copied from the database server to the service machine before installing the Intel\u00ae SecL services. The database client for Intel\u00ae SecL services will validate that the Subject Alternative Names in the database server\u2019s TLS certificate contain the hostname(s)/IP address(es) that the clients will use to access the database server. If configuring a database without using the provided scripts, ensure that these attributes are present in the database TLS certificate.","title":"Database Server TLS Certificate"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#using-nats-with-intel-secl","text":"Intel SecL-DC can utilize a NATS server to manage connectivity between the Host Verification Service and any number of deployed Trust Agent hosts. This acts as an alternative to communication via REST APIs - in NATS mode, a connection is established with the NATS server, and messages are sent and received over that connection. The NATS server should be deployed on the control plane and will need network connectivity to other control plane services as well as any Trust Agent hosts. While NATS is not installed by Intel SecL directly, sample instructions for deploying a NATS server for use with Intel SecL can be found below (this is intended to be a sample only; please consult https://nats.io for official NATS documentation) : ###Download and install the NATS-server binary (see https://github.com/nats-io/nats-server/releases/latest) rpm -i https://github.com/nats-io/nats-server/releases/download/v2.3.0/nats-server-v2.3.0-amd64.rpm ###Install tar and unzip yum install -y tar unzip ###Install cfssl and cfssljson (for \u201cControl-Plane Deployment\u201d step #6). wget https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssljson_1.6.0_linux_amd64 -o /usr/bin/cfssljson && chmod +x /usr/bin/cfssljson wget https://github.com/cloudflare/cfssl/releases/download/v1.6.0/cfssl_1.6.0_linux_amd64 -o /usr/bin/cfssl && chmod +x /usr/bin/cfssl Use the following additional options in populate-users.env and run populate-users: ISECL_INSTALL_COMPONENTS = TA,HVS,AAS,NATS NATS_CERT_SAN_LIST = <NATS server ip>,<NATS server FQDN>,localhost NATS_CERT_COMMON_NAME = NATS TLS Certificate Note that the ISECL_INSTALL_COMPONENTS list should reflect the actual components used in your deployment of Intel SecL, as dictated by the use cases to be enabled. The important part here is to add the \"NATS\" element to the list. To generate a long-lived token for use in environments where the Trust Agent may not be provisioned for an extended period of time beyond the usual lifetime of an authentication token, add the following to populate-users.env: CUSTOM_CLAIMS_COMPONENTS = <List of services for which the long-lived installation token will be valid. Typically this is only the Trust Agent, and so the only component in the list is typically \"TA\"> CUSTOM_CLAIMS_TOKEN_VALIDITY_SECS = <duration in seconds for the token to last> CCC_ADMIN_USERNAME = <username for generating long-lived tokens> CCC_ADMIN_PASSWORD = <password for generating long-lived tokens> When populate-users is run, there will now be an additional \"Custom Claims Token For TA:\" bearer token. When installing the HVS, add the following to hvs.env: NATS_SERVERS = nats://<server ip>:4222 Alternatively, if the HVS is already installed, add the following in /etc/hvs/config.yml : nats : servers : - nats://<NATS server IP>:4222 Be sure to restart the HVS after changing the configuration. Download TLS certificates for the NATS server from the CMS: export NATS_CERT_COMMON_NAME = \"NATS TLS Certificate\" export CMS_ENDPOINT_URL = https://<CMS server ip or hostname>:8445/cms/v1 export NATS_CERT_SAN_LIST = \" <NATS server ip>,<NATS server FQDN>,localhost\" ./download-tls-certs.sh -d ./ -n \" $NATS_CERT_COMMON_NAME \" -u \" $CMS_ENDPOINT_URL \" -s \" $NATS_CERT_SAN_LIST \" -t $BEARER_TOKEN The download-tls-certs.sh script will conenct to the CMS and will out put two files: server.pem sslcert-key.pem Create a \u201cserver.conf\u201d configuration file for nats-server (be sure the paths to the .pem files are correct): lis ten : 0.0.0.0 : 4222 tls : { cer t _ f ile : \"./server.pem\" key_ f ile : \"./sslcert-key.pem\" } Append the operator/account credentials from the AAS installation to the server.conf (the following can be run if NATS will run on the same machine as the AAS): cat /etc/authservice/nats/server.conf >> server.conf The final server.conf should look like the following: lis ten : 0.0.0.0 : 4222 tls : { cer t _ f ile : \"/<path>/server.pem\" key_ f ile : \"/<path>/sslcert-key.pem\" } // Opera t or ISecL - opera t or opera t or : eyJ 0e XAiOiJKV 1 QiLCJhbGciOiJlZDI 1 NTE 5 LW 5 rZXki f Q.eyJleHAiOjE 3 ODE 2 MzI 0 NjUsImp 0 aSI 6 IkJWQVZUQkc 1 M 01 aMkFaSTVYUjRFNVlPS 0 xHTk 5 ZTE 40 SllYV 0 U 3 T 1 I 1 M 0 VQSDJOU 0 pFSEEiLCJpYXQiOjE 2 MjM 5 NTI 0 NjUsImlzcyI 6 Ik 9 DTENLS 1 UzS 0 lMWjZaRDRESDNWNTdUSkJESUdKSllMWk 1 RNEhKUU 9 DNFJFUVEyVkFUVU 01 SlA 1 IiwibmF t ZSI 6 IklTZWNMLW 9 wZXJhdG 9 yIiwic 3 ViIjoiT 0 NMQ 0 t LVTNLSUxaNlpENERIM 1 Y 1 N 1 RKQkRJR 0 pKWUxaTVE 0 SEpRT 0 M 0 UkVRUTJWQVRVTTVKUDUiLCJuYXRzIjp 7 I n R 5 cGUiOiJvcGVyYXRvciIsI n Zlc n Npb 24 iOjJ 9 f Q.PDlhAwk 1 cLHpbCCAJhKGKvv 36 J_NXc 2 PSs n 6 i 3 z n mjDYHXG 3 C_HhO 9 zxsl n 9 Bd 9 ViolRw_L 10 N 1 QwoMjzCB t BQ resolver : MEMORY resolver_preload : { // Accou nt ISecL - accou nt ADR 7 WNJ 2EEE IYASP 5 YHDFDW 6 P 3 ICBFPXRRJVWU 6 CLGXOWVDIM 7 VIOXCM : eyJ 0e XAiOiJKV 1 QiLCJhbGciOiJlZDI 1 NTE 5 LW 5 rZXki f Q.eyJleHAiOjE 3 ODE 2 MzI 0 NjUsImp 0 aSI 6 IjdVUlg 0 M 0 RSTUxEV 0 JEVTdMU 0 dDNTQ 0 UzZCRFFGVDc 1 TzZVWUE 1 QUdYNkxGV 0 FNWUNOTkEiLCJpYXQiOjE 2 MjM 5 NTI 0 NjUsImlzcyI 6 Ik 9 DTENLS 1 UzS 0 lMWjZaRDRESDNWNTdUSkJESUdKSllMWk 1 RNEhKUU 9 DNFJFUVEyVkFUVU 01 SlA 1 IiwibmF t ZSI 6 IklTZWNMLWFjY 291 b n QiLCJzdWIiOiJBRFI 3 V 05 KMkVFRUlZQVNQNVlIREZEVzZQM 0 lDQkZQWFJSSlZXVTZDTEdYT 1 dWRElNN 1 ZJT 1 hDTSIsIm 5 hdHMiO ns ibGl ta XRzIjp 7 I n N 1 Y n MiOi 0 xLCJkYXRhIjo t MSwicGF 5 bG 9 hZCI 6 LTEsIml t cG 9 ydHMiOi 0 xLCJleHBvc n RzIjo t MSwid 2 lsZGNhcmRzIjp 0 c n VlLCJjb 25 uIjo t MSwibGVhZiI 6 LTF 9 LCJkZWZhdWx 0 X 3 Blcm 1 pc 3 Npb 25 zIjp 7 I n B 1 YiI 6e30 sI n N 1 YiI 6e319 LCJ 0e XBlIjoiYWNjb 3 VudCIsI n Zlc n Npb 24 iOjJ 9 f Q.AB 6e NFVE 7 KJspvX 7 DN - x_ - L 4 mMNhPc - sDk 01 iOL - hEwYK fe oL 9 RAcdrOTwQX 3 CuJHMu - a 3 m 5 TpW fl g 1 D 4 S 1 MCQ } Start NATS server: ./nats-server -c server.conf","title":"Using NATS with Intel SecL"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installing-the-certificate-management-service","text":"","title":"Installing the Certificate Management Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#required-for","text":"The CMS is REQUIRED for all use cases. Platform Integrity with Data Sovereignty and Signed Flavors Application Integrity Workload Confidentiality (both VMs and Containers)","title":"Required For"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#supported-operating-systems","text":"The Intel\u00ae Security Libraries Certificate Management Service supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04","title":"Supported Operating Systems"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#recommended-hardware","text":"1 vCPUs RAM: 2 GB 10 GB One network interface with network access to all Intel\u00ae SecL-DC services","title":"Recommended Hardware"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installation","text":"To install the Intel\u00ae SecL-DC Certificate Management Service: Copy the Certificate Management Service installation binary to the /root/ directory. Create the cms.env installation answer file for an unattended installation: AAS_TLS_SAN = <comma-separated list of IPs and hostnames for the AAS> AAS_API_URL = https://<Authentication and Authorization Service IP or Hostname>:8444/aas/v1 SAN_LIST = <Comma-separated list of IP addresses and hostnames for the CMS>,127.0.0.1,localhost The SAN list will be used to authenticate the Certificate Signing Request from the AAS to the CMS. Only a CSR originating from a host matching the SAN list will be honored. Later, in the AAS authservice.env installation answer file, this same SAN list will be provided for the AAS installation. These lists must match, and must be valid for IPs and/or hostnames used by the AAS system. If both the AAS and CMS will be installed on the same system, \"127.0.0.1,localhost\" may be used. The SAN list variables also accept the wildcards \u201c?\u201d (for single-character wildcards) and \"*\" (for multiple-character wildcards) to allow address ranges or multiple FQDNs. The AAS_API_URL represents the URL for the AAS that will exist after the AAS is installed. For all configuration options and their descriptions, refer to the Intel\u00ae SecL Configuration section on the Certificate Management Service. Execute the installer binary. ./cms-v4.0.0.bin When the installation completes, the Certificate Management Service is available. The services can be verified by running cms status from the command line. cms status After installation is complete, the CMS will output a bearer token to the console. This token will be used with the AAS during installation to authenticate certificate requests to the CMS. If this token expires or otherwise needs to be recreated, use the following command: cms setup cms_auth_token --force In addition, the SHA384 digest of the CMS TLS certificate will be needed for installation of the remaining Intel\u00ae SecL services. The digest can be obtained using the following command: cms tlscertsha384","title":"Installation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installing-the-authentication-and-authorization-service","text":"","title":"Installing the Authentication and Authorization Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#required-for_1","text":"The AAS is REQUIRED for all use cases. Platform Integrity with Data Sovereignty and Signed Flavors Application Integrity Workload Confidentiality (both VMs and Containers)","title":"Required For"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#prerequisites","text":"The following must be completed before installing the Authentication and Authorization Service: The Certificate Management Service must be installed and available The Authentication and Authorization Service database must be available","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#package-dependencies","text":"The Intel\u00ae SecL-DC Authentication and Authorization Service (AAS) requires a Postgresql 11 database. A script (install_pgdb.sh) is provided with the AAS that will automatically add the Postgresql repositories and install/configure a sample database. If this script will not be used, a Postgresql 11 database must be installed by the user before executing the AAS installation.","title":"Package Dependencies"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#supported-operating-systems_1","text":"The Intel\u00ae Security Libraries Authentication and Authorization Service supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04","title":"Supported Operating Systems"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#recommended-hardware_1","text":"1 vCPUs RAM: 2 GB 10 GB One network interface with network access to all Intel\u00ae SecL-DC services","title":"Recommended Hardware"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installation_1","text":"To install the AAS, a bearer token from the CMS is required. This bearer token is output at the end of the CMS installation. However, if a new token is needed, simply use the following command from the CMS command line: cms setup cms_auth_token --force Create the authservice.env installation answer file: CMS_BASE_URL = https://<CMS IP or hostname>:8445/cms/v1/ CMS_TLS_CERT_SHA384 = <CMS TLS certificate sha384> AAS_DB_HOSTNAME = <IP or hostname of database server> AAS_DB_PORT = <database port number ; default is 5432 > AAS_DB_NAME = <database name> AAS_DB_USERNAME = <database username> AAS_DB_PASSWORD = <database password> AAS_DB_SSLCERTSRC = <path to database TLS certificate ; the default location is typically /usr/local/pgsql/data/server.crt> AAS_ADMIN_USERNAME = <username for AAS administrative user> AAS_ADMIN_PASSWORD = <password for AAS administrative user> SAN_LIST = <comma-separated list of IPs and hostnames for the AAS ; this should match the value for the AAS_TLS_SAN in the cms.env file from the CMS installation> BEARER_TOKEN = <bearer token from CMS installation> Execute the AAS installer: ./authservice-v4.0.0.bin Note The AAS_ADMIN credentials specified in this answer file will have administrator rights for the AAS and can be used to create other users, create new roles, and assign roles to users.","title":"Installation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#creating-users","text":"After installation is complete, a number of roles and user accounts must be generated. Most of these accounts will be service users, used by the various Intel\u00ae SecL services to work together. Another set of users will be used for installation permissions, and a final administrative user will be created to provide the initial authentication interface for the actual human user. The administrative user can be used to create additional users with appropriately restricted roles based on organizational needs. Creating these required users and roles is facilitated by a script that will accept credentials and some configuration settings from an answer file and automate the process. Create the populate-users.env file: ISECL_INSTALL_COMPONENTS = KBS,TA,WLS,WPM,IHUB,HVS,WLA,AAS AAS_API_URL = https://<AAS IP address or hostname>:8444/aas/v1 AAS_ADMIN_USERNAME = <AAS username> AAS_ADMIN_PASSWORD = <AAS password> HVS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Host Verification Service> IH_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Integration Hub> WLS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Workload Service> KBS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Key Broker Service> TA_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Trust Agent> HVS_SERVICE_USERNAME = <Username for the HVS service user> HVS_SERVICE_PASSWORD = <Password for the HVS service user> IHUB_SERVICE_USERNAME = <Username for the Hub service user> IHUB_SERVICE_PASSWORD = <Password for the Hub service user> WPM_SERVICE_USERNAME = <Username for the WPM service user> WPM_SERVICE_PASSWORD = <Password for the WPM service user> WLS_SERVICE_USERNAME = <Username for the WLS service user> WLS_SERVICE_PASSWORD = <Password for the WLS service user> WLA_SERVICE_USERNAME = <Username for the WLA service user> WLA_SERVICE_PASSWORD = <Password for the WLA service user> GLOBAL_ADMIN_USERNAME = <Username for the global Administrator user GLOBAL_ADMIN_PASSWORD = <Password for the global Administrator user INSTALL_ADMIN_USERNAME = <Username for the installation user INSTALL_ADMIN_PASSWORD = <Password for the global installation user Note The ISECL_INSTALL_COMPONENTS variable is a comma-separated list of the components that will be used in your environment. Not all services are required for every use case. If a given service will not be used in your deployment, simply delete the unnecessary service abbreviation from the ISECL_INSTALL_COMPONENTS list, and leave the SAN and credential variables for that service blank. Note The SAN list variables each support wildcards( \"*\" and \"?\"). In particular, without wildcards the Trust Agent SAN list would need to explicitly list each hostname or IP address for all Trust Agents that will be installed, which is not generally feasible. Using wildcards, domain names and entire IP ranges can be included in the SAN list, which will allow any host matching those ranges to install the relevant service. The SAN list specified here must exactly match the SAN list for the applicable service in that service\u2019s env installation file. Execute the populate-users script: ./populate-users Note The script can be executed with the \u2013output_json argument to create the populate-user.json .This json output file will contain all of the users created by the script, along with usernames, passwords, and role assignments. This file can be used both as a record of the service and administrator accounts, and can be used as alternative inputs to recreate the same users with the same credentials in the future if needed. Be sure to protect this file if the \u2013output_json argument is used. The script will automatically generate the following users: Verification Service User Integration Hub Service User Workload Policy Manager Service User Workload Service User Name Workload Service User Global Admin User Installation User These user accounts will be used during installation of several of the Intel\u00ae SecL-DC applications. In general, whenever credentials are required by an installation answer file, the variable name should match the name of the corresponding variable used in the populate-users.env file. The Global Admin user account has all roles for all services. This is a default administrator account that can be used to perform any task, including creating any other users. In general this account is useful for POC installations, but in production it should be used only to create user accounts with more restrictive roles. The administrator credentials should be protected and not shared. The populate-users script will also output an installation token. This token has all privileges needed for installation of the Intel\u00ae SecL services, and uses the credentials provided with the INSTALLATION_ADMIN_USERNAME and password. The remaining Intel \u00ae SecL-DC services require this token (set as the BEARER_TOKEN variable in the installation env files) to grant the appropriate privileges for installation. By default this token will be valid for two hours; the populate-users script can be rerun with the same populate-users.env file to regenerate the token if more time is required, or the INSTALLATION_ADMIN_USERNAME and password can be used to generate an authentication token.","title":"Creating Users"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installing-the-host-verification-service","text":"This section details how to install the Intel\u00ae SecL-DC services. For instructions on running these services as containers, see the following section.","title":"Installing the Host Verification Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#required-for_2","text":"The Host Verification Service is REQUIRED for all use cases. Platform Integrity with Data Sovereignty and Signed Flavors Application Integrity Workload Confidentiality (both VMs and Containers)","title":"Required For"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#prerequisites_1","text":"The following must be completed before installing the Verification Service: The Certificate Management Service must be installed and available The Authentication and Authorization Service must be installed and available The Verification Service database must be available","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#package-dependencies_1","text":"The Intel\u00ae Security Libraries Verification Service requires the following packages and their dependencies: logback Postgres* client and server 11.6 (server component optional if an external Postgres database is used) unzip zip openssl wget net-tools python3-policycoreutils If they are not already installed, the Verification Service installer attempts to install these automatically using the package manager. Automatic installation requires access to package repositories (the RHEL subscription repositories, the EPEL repository, or a suitable mirror), which may require an Internet connection. If the packages are to be installed from the package repository, be sure to update the repository package lists before installation.","title":"Package Dependencies"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#supported-operating-systems_2","text":"The Intel\u00ae Security Libraries Verification Service supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04","title":"Supported Operating Systems"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#recommended-hardware_2","text":"4 vCPUs RAM: 8 GB 100 GB One network interface with network access to all managed servers (Optional) One network interface for Asset Tag provisioning (only required for \u201cpull\u201d tag provisioning; required to provision Asset Tags to VMware ESXi servers).","title":"Recommended Hardware"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installation_2","text":"To install the Verification Service, follow these steps: Copy the Verification Service installation binary to the /root directory. Create the hvs.env installation answer file. A sample minimal hvs.env file is provided below. For all configuration options and their descriptions, refer to the Intel\u00ae SecL Configuration section on the Verification Service. # Authentication URL and service account credentials AAS_API_URL = https://isecl-aas:8444/aas/v1 HVS_SERVICE_USERNAME = <username> HVS_SERVICE_PASSWORD = <password> # CMS URL and CMS webserivce TLS hash for server verification CMS_BASE_URL = https://isecl-cms:8445/cms/v1 CMS_TLS_CERT_SHA384 = <digest> # TLS Configuration SAN_LIST = 127 .0.0.1,192.168.1.1,hvs.server.com #comma-separated list of IP addresses and hostnames for the HVS to be used in the Subject Alternative Names list in the TLS Certificate # Installation admin bearer token for CSR approval request to CMS BEARER_TOKEN = eyJhbGciOiJSUzM4NCIsImtpZCI6ImE\u2026 # Database HVS_DB_NAME = <database name> HVS_DB_USERNAME = <database username> HVS_DB_PASSWORD = <database password> HVS_DB_SSLCERTSRC = /tmp/dbcert.pem # Not required if VS_DB_SSLCERT is given Execute the installer binary. ./hvs-v4.0.0.bin When the installation completes, the Verification Service is available. The services can be verified by running hvs status from the Verification Service command line. hvs status","title":"Installation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installing-the-workload-service","text":"","title":"Installing the Workload Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#required-for_3","text":"The WLS is REQUIRED for the following use cases. Workload Confidentiality (both VMs and Containers)","title":"Required For"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#prerequisites_2","text":"The following must be completed before installing the Workload Service: The Certificate Management Service must be installed and available The Authentication and Authorization Service must be installed and available The Verification Service must be installed and available The Workload Service database must be available","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#supported-operating-systems_3","text":"The Intel\u00ae Security Libraries Workload Service supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04","title":"Supported Operating Systems"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#recommended-hardware_3","text":"","title":"Recommended Hardware"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installation_3","text":"Copy the Workload Service installation binary to the /root directory. Create the workload-service.env installation answer file WLS_DB_USERNAME = <database username> WLS_DB_PASSWORD = <database password> WLS_DB_HOSTNAME = <IP or hostname of database server> WLS_DB_PORT = <Database port ; 5432 by default> WLS_DB = <name of the WLS database> WLS_DB_SSLCERTSRC = <path to database TLS certificate ; the default location is typically /usr/local/pgsql/data/server.crt > HVS_URL = https://<Ip address or hostname of the Host verification Service>:8443/hvs/v2/ WLS_SERVICE_USERNAME = <username for WLS service account> WLS_SERVICE_PASSWORD = <password for WLS service account> CMS_BASE_URL = https://<IP or hostname to CMS>:8445/cms/v1/ CMS_TLS_CERT_SHA384 = <sha384 of CMS TLS certificate> AAS_API_URL = https://<IP or hostname to AAS>:8444/aas/v1/ SAN_LIST = <comma-separated list of IPs and hostnames for the WLS> BEARER_TOKEN = <Installation token from populate-users script> Execute the WLS installer binary: ./wls-v4.0.0.bin","title":"Installation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installing-the-trust-agent-for-linux","text":"","title":"Installing the Trust Agent for Linux"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#required-for_4","text":"The Trust Agent for Linux is REQUIRED for all use cases. Platform Integrity with Data Sovereignty and Signed Flavors Application Integrity Workload Confidentiality (both VMs and Containers)","title":"Required For"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#package-dependencies_2","text":"The Trust Agent requires the following packages and their dependencies: Tboot (Optional, for TXT-based deployments without UEFI SecureBoot only) openssl tar redhat-lsb If they are not already installed, the Trust Agent installer attempts to install these automatically using the package manager. Automatic installation requires access to package repositories (the RHEL subscription repositories, the EPEL repository, or a suitable mirror), which may require an Internet connection. If the packages are to be installed from the package repository, be sure to update the repository package lists before installation. Tboot will not be installed automatically. Instructions for installing and configuring tboot are documented later in this section.","title":"Package Dependencies"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#supported-operating-systems_4","text":"The Intel\u00ae Security Libraries Trust Agent for Linux supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04","title":"Supported Operating Systems"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#prerequisites_3","text":"The following must be completed before installing the Trust Agent: Supported server hardware including an Intel\u00ae Xeon\u00ae processor with Intel Trusted Execution Technology activated in the system BIOS. Trusted Platform Module (version 2.0) installed and activated in the system BIOS, with cleared ownership status OR a known TPM ownership secret. Note Starting in Intel SecL-DV 4.0, the Trust Agent will now default to using a null TPM owner secret, and does not require ownership permissions except during the provisioning step. If ownership has already been taken when the Trust Agent will be provisioned, it will be necessary to either provide the ownership secret or clear the TPM ownership before provisioning. System must be booted to a tboot boot option OR use UEFI SecureBoot. (Provisioning step only) Intel\u00ae SecL Verification Service server installed and active. (Required for NATS mode only) A NATS server must be configured and available (REQUIRED for servers configured with TXT and tboot only) If the server is installed using an LVM, the LVM name must be identical for all Trust Agent systems. The Grub bootloader line that calls the Linux kernel will contain the LVM name of the root volume, and this line with all arguments is part of what is measured in the TXT/Tboot boot process. This will cause the OS Flavor measurements to differ between two otherwise identical hosts if their LVM names are different. Simply using a uniform name for the LVM during OS installation will resolve this possible discrepancy. (Optional, REQUIRED for Virtual Machine Confidentiality only): QEMU/KVM must be installed Libvirt must be installed","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#tboot-installation","text":"Tboot is required to build a complete Chain of Trust for Intel\u00ae TXT systems that are not using UEFI Secure Boot. Tboot acts to initiate the Intel\u00ae TXT SINIT ACM (Authenticated Code Module), which populates several TPM measurements including measurement of the kernel, grub command line, and initrd. Without either tboot or UEFI Secure Boot, the Chain of Trust will be broken because the OS-related components will be neither measured nor signature-verified prior to execution. Because tboot acts to initiate the Intel\u00ae TXT SINIT ACM, tboot is only required for platforms using Intel\u00ae TXT, and is not required for platforms using another hardware Root of Trust technology like Intel\u00ae Boot Guard. Intel\u00ae SecL-DC requires tboot 1.10.1 or greater. This may be a later version of tboot than is available on public software repositories. The most current version of tboot can be found here: https://sourceforge.net/projects/tboot/files/tboot/ Tboot requires configuration of the grub boot loader after installation. To install and configure tboot: Install tboot yum install tboot If the package manager does not support a late enough version of tboot, it will need to be compiled from source and installed manually. Instructions can be found here: https://sourceforge.net/p/tboot/wiki/Home/ Note that the step \"copy platform SINIT to /boot\" should not be required, as datacenter platforms include the SINIT in the system BIOS package. Ensure that multiboot2.mod and relocator.mod are available for grub2 This step may not be necessary for all OS versions, for instance, this step is NA in case of Tboot installation on Ubuntu 18.04. In order to utilize tboot, grub2 requires these two modules from the grub2-efi-x64-modules package to be located in the correct directory (if they're absent, the host will throw a grub error when it tries to boot using tboot). These files must be present in this directory: /boot/efi/EFI/redhat/x86_64-efi/multiboot2.mod /boot/efi/EFI/redhat/x86_64-efi/relocator.mod If the files are not present in this directory, they can be moved from their installation location: cp /usr/lib/grub/x86_64-efi/multiboot2.mod /boot/efi/EFI/redhat/x86_64-efi/ cp /usr/lib/grub/x86_64-efi/relocator.mod /boot/efi/EFI/redhat/x86_64-efi/ Make a backup of your current grub.cfg file The below examples assume a RedHat OS that has been installed on a platform using UEFI boot mode. The grub path will be slightly different for platforms using a non-RedHat OS. cp /boot/efi/EFI/redhat/grub.cfg /boot/efi/EFI/redhat/grub.cfg.bak Generate a new grub.cfg with the tboot boot option # For RHEL grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg # For Ubuntu grub-mkconfig -o /boot/efi/EFI/redhat/grub.cfg Update the default boot option Ensure that the GRUB_DEFAULT value is set to the tboot option. a. Update /etc/default/grub and set the GRUB_DEFAULT value to 'tboot-1.10.1' GRUB_DEFAULT='tboot-1.10.1' b. Regenerate grub.cfg : # For RHEL grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg # For Ubuntu grub-mkconfig -o /boot/efi/EFI/redhat/grub.cfg Reboot the system Because measurement happens at system boot, a reboot is needed to boot to the tboot boot option and populate measurements in the TPM. Verify a successful trusted boot with tboot Tboot provides the txt-stat command to show the tboot log. The first part of the output of this command can be used to verify a successful trusted launch. In the output below, note the \u201cTXT measured launch\u201d and \u201csecrets flag set\u201d at the bottom. Both of these should show \" TRUE \" if the tboot measured launch was successful. If either of these show \" FALSE \" the measured launch has failed. This usually simply indicates that the tboot boot option was not selected during boot. If the measured launch was successful, proceed to install the Trust Agent. Intel(r) TXT Configuration Registers: STS: 0x0001c091 senter_done: TRUE sexit_done: FALSE mem_config_lock: FALSE private_open: TRUE locality_1_open: TRUE locality_2_open: TRUE ESTS: 0x00 txt_reset: FALSE E2STS: 0x0000000000000006 secrets: TRUE ERRORCODE: 0x00000000 DIDVID: 0x00000001b0078086 vendor_id: 0x8086 device_id: 0xb007 revision_id: 0x1 FSBIF: 0xffffffffffffffff QPIIF: 0x000000009d003000 SINIT.BASE: 0x6fec0000 SINIT.SIZE: 262144B (0x40000) HEAP.BASE: 0x6ff00000 HEAP.SIZE: 1048576B (0x100000) DPR: 0x0000000070000051 lock: TRUE top: 0x70000000 size: 5MB (5242880B) PUBLIC.KEY: 9c 78 f0 d8 53 de 85 4a 2f 47 76 1c 72 b8 6a 11 16 4a 66 a9 84 c1 aa d7 92 e3 14 4f b7 1c 2d 11 *********************************************************** TXT measured launch: TRUE secrets flag set: TRUE ***********************************************************","title":"Tboot Installation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#nats-mode-vs-http-mode","text":"The Trust Agent can operate in either HTTP mode (default) or NATS mode. This distinction controls how the Agent communicates with the HVS. In HTTP mode, the TAgent presents a set of API endpoints for the HVS to access via individual TLS requests. In NATS mode, the Agent and HVS are connected via a persistent session maintained via a NATS server; in this mode, the Trust Agent will not listen on any HTTP ports.","title":"NATS Mode vs HTTP Mode"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installation_4","text":"Installation of the Trust Agent is split into two major steps: Installation, which covers the creation of system files and folders, and Provisioning, which involves the creation of keys and secrets and links the Trust Agent to a specific Verification Service. Both operations can be performed at the same time using an installation answer file. Without the answer file, the Trust Agent can be installed and left in an un-provisioned state regardless of whether a Verification Service is up and running, until such time as the datacenter administrator is ready to run the provisioning step and link the Trust Agent to a Verification Service. To install the Trust Agent for Linux: Copy the Trust Agent installation binary to the /root/ directory. (Optional; required to perform Provisioning and Installation at the same time.) Create the trustagent.env answer file in the /root directory (for full configuration options, see section 9.2). The minimum configuration options for installation are provided below. For Platform Attestation only, provide the following in trustagent.env HVS_URL = https://<Verification Service IP or Hostname>:8443/hvs/v2 PROVISION_ATTESTATION = y GRUB_FILE = <path to grub.cfg> CURRENT_IP = <Trust Agent IP address> CMS_TLS_CERT_SHA384 = <CMS TLS digest> BEARER_TOKEN = <Installation token from populate-users script> AAS_API_URL = https://<AAS IP or Hostname>:8444/aas/v1 CMS_BASE_URL = https://<CMS IP or Hostname>:8445/cms/v1 SAN_LIST = <Comma-separated list of IP addresses and hostnames for the TAgent matching the SAN list specified in the populate-users script ; may include wildcards> For Workload Confidentiality with VM Encryption, add the following ( in addition to the basic Platform Attestation sample): WLA_SERVICE_USERNAME = <Username for the WLA service user> WLA_SERVICE_PASSWORD = <Username for the WLA service user> WLS_API_URL = https://<WLS IP address or hostname>:5000/wls/ For Workload Confidentiality with Container Encryption, add the following ( in addition to the basic Platform Attestation sample): WLA_SERVICE_USERNAME = <Username for the WLA service user> WLA_SERVICE_PASSWORD = <Username for the WLA service user> WLS_API_URL = https://<WLS IP address or hostname>:5000/wls/ REGISTRY_SCHEME_TYPE = https ##For the CRI-O container runtime: WA_WITH_CONTAINER_SECURITY_CRIO = yes For NATS mode, add the following (in addition to the basic Platform Attestation sample and any other optional features): TA_SERVICE_MODE=outbound NATS_SERVERS=<nats-server-ip>:4222 TA_HOST_ID=<Any unique identifier for the host; this could be the server FQDN, a UUID, or any other unique identifier> Note that the TA_HOST_ID unique identifier will also be the ID used as part of the connection string to reach this Trust Agent host in NATS mode. Execute the Trust Agent installer and wait for the installation to complete. ./trustagent-v4.0.0.bin If the trustagent.env answer file was provided with the minimum required options, the Trust Agent will be installed and also Provisioned to the Verification Service specified in the answer file. If no answer file was provided, the Trust Agent will be installed, but will not be Provisioned. TPM-related functionality will not be available from the Trust Agent until the Provisioning step is completed. The Trust Agent will add a new grub menu entry for application measurement. This new entry will include tboot if the existing grub contains tboot as the default boot option. Note If the Linux Trust Agent is installed without being Provisioned, the Trust Agent process will not actually run until the Provisioning step has been completed. Legacy BIOS systems using tboot ONLY) Update the grub boot loader: grub2-mkconfig -o /boot/grub2/grub.cfg After Provisioning is completed, the Linux Trust Agent must be rebooted so that the default SOFTWARE Flavor manifest can be measured and extended to the TPM. If the Workload Agent will also be installed on the system (see the next section), wait to reboot the server until after the Workload Agent has been installed, as this modifies the default SOFTWARE Flavor manifest.","title":"Installation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installing-the-workload-agent","text":"","title":"Installing the Workload Agent"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#required-for_5","text":"Workload Confidentiality (both VMs and Containers)","title":"Required For"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#supported-operating-systems_5","text":"The Intel\u00ae Security Libraries Workload Agent supports Red Hat Enterprise Linux 8.2","title":"Supported Operating Systems"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#prerequisites_4","text":"The following must be completed before installing the Workload Agent: Intel\u00ae SecL Trust Agent installed and active. cryptsetup (REQUIRED for Virtual Machine Confidentiality only): QEMU/KVM must be installed libvirt must be installed","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installation_5","text":"Copy the Workload Agent installation binary to the /root/ directory Verify that the trustagent.env answer file is present. This file was necessary for installing/provisioning the Trust Agent. Note that the additional content required for Workload Confidentiality with either VM Encryption or Container Encryption must be included in the trustagent.env file (samples provided in the previous section) for use by the Workload Agent. Execute the Workload Agent installer binary. ./workload-agent-v4.0.0.bin Reboot the server. The Workload Agent populates files that are needed for the default SOFTWARE Flavor, and a reboot is required for those measurements to happen.","title":"Installation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#trust-agent-provisioning","text":"\"Provisioning\" the Trust Agent involves connecting to a Verification Service to download the Verification Service PrivacyCA certificate, create a new Attestation Identity Keypair in the TPM, and verify or create the TPM Endorsement Certificate and Endorsement Key. The Verification Service PrivacyCA root certificate is used to sign the EC, and the EC is used to generate the Attestation Identity Keypair. The AIK is used by the Verification Service to verify the integrity of quotes from the host\u2019s TPM. Provisioning is the only time that the Trust Agent requires TPM ownership permissions. If no TPM ownership secret is provided in the trustagent.env file, the Agent will use a null ownership secret to perform the provisioning steps. If a TPM ownership secret is provided in the trustagent.env answer file, the Agent will attempt to use the specified secret. If TPM ownership is in a clear state, the Agent will take ownership if a secret is specified. If the TPM is already \"owned,\" the Agent will try to use the specified secret; if the specified secret does not match the actual ownership password, the provisioning will fail. Intel recommends using the default \"null\" ownership secret, as this makes it easy for other applications to also use the Trust Agent, and can prevent the need to clear ownership in the case of a need to re-provision. Provisioning can be performed separately from installation (meaning you can install the Trust Agent without Provisioning, and then Provision later). If the trustagent.env answer file is present and has the required Verification Service information during installation, the Agent will automatically run the Provisioning steps. Note The trustagent.env answer file must contain user credentials for a user with sufficient privileges. The minimum role required for performing provisioning is the \"AttestationRegister\" role. Note If the Linux Trust Agent is installed without being Provisioned, the Trust Agent process will not actually run until the Provisioning step has been completed. If the answer file is not present during installation, the Agent can be provisioned later by adding the trustagent.env file and running the following command: tagent setup -f <trustagent.env file path>","title":"Trust Agent Provisioning"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#trust-agent-registration","text":"Registration creates a host record with connectivity details and other host information in the Verification Service database. This host record will be used by the Verification Service to retrieve TPM attestation quotes from the Trust Agent to generate an attestation report. The Trust Agent can register the host with a Verification Service by running the following command (the trustagent.env answer file must be present in the current working directory): tagent setup create-host Hosts can also be registered using a REST API request to the Verification Service: POST <https://verification.service.com:8443/hvs/v2/hosts> { \"host_name\": \"<hostname of host to be registered>\" \"connection_string\": \"intel:https://<hostname or IP address>:1443\", \"flavorgroup_names\": [], \"description\": \"<description>\" } Note When a new host is registered, the Verification Service will automatically attempt to match the host to appropriate Flavors. If appropriate Flavors are not found, the host will still be registered, but will be in an Untrusted state until/unless appropriate Flavors are added to the Verification Service.","title":"Trust Agent Registration"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#importing-the-host_unique-flavor","text":"RHEL and VMWare ESXi hosts have measured components that are unique to each host. This means that a special HOST_UNIQUE flavor part needs to be imported for each RHEL and ESXi host, in addition to any other OS or Platform Flavors. Note Importing a Flavor requires user credentials for a user with sufficient privileges. The minimum role required for creating the HOST_UNIQUE Flavor part is the \u201chost_unique_flavor_creator\u201d role. This role can only create HOST_UNIQUE Flavor parts, and cannot create any other Flavors. On Red Hat Enterprise Linux hosts with the Trust Agent, this can be performed from the Trust Agent command line (this requires the trustagent.env answer file to be present in the current working directory): tagent setup create-host-unique-flavor This can also be performed using a REST API (required for VMWare ESXi hosts): POST https://verification.service.com:8443/hvs/v2/flavors { \"connection_string\": \"<Connection string>\", \"partial_flavor_types\": [\"HOST_UNIQUE\"] }","title":"Importing the HOST_UNIQUE Flavor"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installing-the-intel-secl-kubernetes-extensions-and-integration-hub","text":"Intel\u00ae SecL uses Custom Resource Definitions to add the ability to base orchestration decisions on Intel\u00ae SecL security attributes to Kubernetes. These CRDs allow Kubernetes administrators to configure pods to require specific security attributes so that the Kubernetes Control Plane Node will schedule those pods only on Worker Nodes that match the specified attributes. Two CRDs are required for integration with Intel\u00ae SecL \u2013 an extension for the Control Plane nodes, and a scheduler extension. The extensions are deployed as a Kubernetes deployment in the isecl namespace.","title":"Installing the Intel\u00ae SecL Kubernetes Extensions and Integration Hub"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#deploy-intel-secl-custom-controller","text":"#Install skopeo to load container image for controller and scheduler from archive dnf install -y skopeo Copy isecl-k8s-extensions-*.tar.gz to Kubernetes Control plane machine and extract the contents #Copy scp /<build_path>/binaries/isecl-k8s-extensions-*.tar.gz <user>@<k8s_controller_machine>:/<path>/ #Extract tar -xvzf /<path>/isecl-k8s-extensions-*.tar.gz cd /<path>/isecl-k8s-extensions/ Create hostattributes.crd.isecl.intel.com CRD #1.14<=k8s_version<=1.16 kubectl apply -f yamls/crd-1.14.yaml #1.16<=k8s_version<=1.18 kubectl apply -f yamls/crd-1.17.yaml Check whether the CRD is created kubectl get crds Load the isecl-controller container image cd /<path>/isecl-k8s-extensions/ skopeo copy oci-archive:<isecl-k8s-controller-*.tar> docker://<docker_private_registry_server>:5000/<imageName>:<tagName> Udate image name as above in controller yaml \"/opt/isecl-k8s-extensions/yamls/isecl-controller.yaml\" containers: - name: isecl-controller image: <docker_private_registry_server>:5000/<imageName>:<tagName> Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterRoleBinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole = system:node --user = system:serviceaccount:isecl:isecl Fetch token required for ihub installation kubectl get secrets -n isecl #The below token will be used for ihub installation to update 'KUBERNETES_TOKEN' in ihub.env when configured with Kubernetes Tenant kubectl describe secret default-token-<name> -n isecl Additional Optional Configurable fields for isecl-controller configuration in isecl-controller.yaml Field Required Type Default Description LOG_LEVEL Optional string INFO Determines the log level LOG_MAX_LENGTH Optional int 1500 Determines the maximum length of characters in a line in log file TAG_PREFIX Optional string isecl A custom prefix which can be applied to isecl attributes that are pushed from IH. For example, if the tag-prefix is isecl. and trusted attribute in CRD becomes isecl.trusted . TAINT_UNTRUSTED_NODES Optional string false If set to true. NoExec taint applied to the nodes for which trust status is set to false, Applicable only for HVS based attestation","title":"Deploy Intel\u00ae SecL Custom Controller"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installing-the-intel-secl-integration-hub","text":"Note The Integration Hub is only required to integrate Intel\u00ae SecL with third-party scheduler services, such as OpenStack Nova or Kubernetes. The Hub is not required for usage models that do not require Intel\u00ae SecL security attributes to be pushed to an integration endpoint.","title":"Installing the Intel\u00ae SecL Integration Hub"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#required-for_6","text":"The Hub is REQUIRED for the following use cases. Workload Confidentiality (both VMs and Containers) The Hub is OPTIONAL for the following use cases (used only if orchestration or other integration support is needed): Platform Integrity with Data Sovereignty and Signed Flavors Application Integrity","title":"Required For"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#deployment-architecture-considerations-for-the-hub","text":"A separate Hub instance is REQUIRED for each Cloud environment (also referred to as a Hub \"tenant\"). For example, if a single datacenter will have an OpenStack cluster and also two separate Kubernetes clusters, a total of three Hub instances must be installed, though additional instances of other Intel SecL services are not required (in the same example, only a single Verification Service is required). Each Hub will manage a single orchestrator environment. Each Hub instance should be installed on a separate VM or physical server","title":"Deployment Architecture Considerations for the Hub"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#prerequisites_5","text":"The Intel\u00ae Security Libraries Integration Hub can be run as a VM or as a bare-metal server. The Hub may be installed on the same server (physical or VM) as the Verification Service. The Verification Service must be installed and available The Authentication and Authorization Service must be installed and available The Certificate Management Service must be installed and available (REQUIRED for Kubernetes integration only) The Intel SecL Custom Resource Definitions must be installed and available (see the Integration section for details)","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#package-dependencies_3","text":"The Intel\u00ae SecL Integration Hub requires a number of packages and their dependencies: If these are not already installed, the Integration Hub installer attempts to install these packages automatically using the package manager. Automatic installation requires access to package repositories (the RHEL subscription repositories, the EPEL repository, or a suitable mirror), which may require an Internet connection. If the packages are to be installed from the package repository, be sure to update your repository package lists before installation.","title":"Package Dependencies"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#supported-operating-systems_6","text":"The Intel Security Libraries Integration Hub supports Red Hat Enterprise Linux 8.2","title":"Supported Operating Systems"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#recommended-hardware_4","text":"1 vCPUs RAM: 2 GB 1 GB free space to install the Verification Service services. Additional free space is needed for the Integration Hub database and logs (database and log space requirements are dependent on the number of managed servers). One network interface with network access to the Verification Service. One network interface with network access to any integration endpoints (for example, OpenStack Nova).","title":"Recommended Hardware"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installing-the-integration-hub","text":"To install the Integration Hub, follow these steps: Copy the API Server certificate of the Kubernetes Controller to machine where Integration Hub will be installed to /root/ directory Note In most Kubernetes distributions the Kubernetes certificate and key is normally present under /etc/kubernetes/pki . However this might differ in case of some specific Kubernetes distributions. In ihub.env KUBERNETES_TOKEN token can be retrieved from Kubernetes using the following command: kubectl get secrets -n isecl -o jsonpath=\"{.items[?(@.metadata.annotations['kubernetes\\.io/service-account\\.name']=='default')].data.token}\"|base64 --decode KUBERNETES_CERT_FILE=/<any_path>/apiserver.crt in this path can be specified any ex: /root which can taken care by IHUB during installation and copied to '/etc/ihub' directory. Create the ihub.env installation answer file. See the sample file below. # Authentication URL and service account credentials AAS_API_URL = https://isecl-aas:8444/aas/v1 IHUB_SERVICE_USERNAME = <Username for the Hub service user> IHUB_SERVICE_PASSWORD = <Password for the Hub service user> # CMS URL and CMS webserivce TLS hash for server verification CMS_BASE_URL = https://isecl-cms:8445/cms/v1 CMS_TLS_CERT_SHA384 = <TLS hash> # TLS Configuration TLS_SAN_LIST = 127 .0.0.1,192.168.1.1,hub.server.com #comma-separated list of IP addresses and hostnames for the Hub to be used in the Subject Alternative Names list in the TLS Certificate # Verification Service URL HVS_BASE_URL = https://isecl-hvs:8443/hvs/v2 ATTESTATION_TYPE = HVS #Integration tenant type. Currently supported values are \"KUBENETES\" or \"OPENSTACK\" TENANT = <KUBERNETES or OPENSTACK> # OpenStack Integration Credentials - required for OpenStack integration only OPENSTACK_AUTH_URL = <OpenStack Keystone URL ; typically http://openstack-ip:5000/> OPENSTACK_PLACEMENT_URL = <OpenStack Nova API URL ; typically http://openstack-ip:8778/> OPENSTACK_USERNAME = <OpenStack username> OPENSTACK_PASSWORD = <OpenStack password> # Kubernetes Integration Credentials - required for Kubernetes integration only KUBERNETES_URL = https://kubernetes:6443/ KUBERNETES_CRD = custom-isecl KUBERNETES_CERT_FILE = /root/apiserver.crt KUBERNETES_TOKEN = eyJhbGciOiJSUzI1NiIsImtpZCI6Ik...... # Installation admin bearer token for CSR approval request to CMS - mandatory BEARER_TOKEN = eyJhbGciOiJSUzM4NCIsImtpZCI6ImE\u2026 * Update the token obtained in Step 8 of Deploy Intel\u00ae SecL Custom Controller along with other relevant tenant configuration options in ihub.env Copy the Integration Hub installation binary to the /root directory & execute the installer binary. ./ihub-v4.0.0.bin Copy the /etc/ihub/ihub_public_key.pem to Kubernetes Controller machine to /<path>/secrets/ directory #On K8s-Controller machine mkdir -p /<path>/secrets #On IHUB machine, copy scp /etc/ihub/ihub_public_key.pem <user>@<k8s_controller_machine>:/<path>/secrets/hvs_ihub_public_key.pem After installation, the Hub must be configured to integrate with a Cloud orchestration platform (for example, OpenStack or Kubernetes). See the Integration section for details.","title":"Installing the Integration Hub"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#deploy-intel-secl-extended-scheduler","text":"Install cfssl and cfssljson on Kubernetes Control Plane #Install wget dnf install wget -y #Download cfssl to /usr/local/bin/ wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl #Download cfssljson to /usr/local/bin wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create TLS key-pair for isecl-scheduler service which is signed by Kubernetes apiserver.crt cd /<path>/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh #Set K8s_CONTROLLER_IP,HOSTNAME export CONTROLLER_IP = <k8s_machine_ip> export HOSTNAME = <k8s_machine_hostname> #Create TLS key-pair ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \" $CONTROLLER_IP \" , \" $HOSTNAME \" -c <k8s_ca_authority_cert> -k <k8s_ca_authority_key> Note In most Kubernetes distributions the Kubernetes certificate and key is normally present under /etc/kubernetes/pki . However this might differ in case of some specific Kubernetes distributions. Copy the TLS key-pair generated to /<path>/secrets/ directory cp /<path>/isecl-k8s-extensions/server.key /<path>/secrets/ cp /<path>/isecl-k8s-extensions/server.crt /<path>/secrets/ Load the isecl-scheduler container image cd /<path>/isecl-k8s-extensions/ skopeo copy oci-archive:<isecl-k8s-scheduler-*.tar> docker://<docker_private_registry_server>:5000/<imageName>:<tagName> Update image name as above in scheduler yaml \"/opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml\" containers: - name: isecl-scheduler image: <docker_private_registry_server>:5000/<imageName>:<tagName> Create scheduler-secret for isecl-scheduler cd /<path>/ kubectl create secret generic scheduler-certs --namespace isecl --from-file = secrets The isecl-scheduler.yaml file includes support for both SGX and Workload Security put together. For only working with Workload Security scenarios , the following line needs to be made empty in the yaml file. The scheduler and controller yaml files are located under /<path>/isecl-k8s-extensions/yamls - name : SGX_IHUB_PUBLIC_KEY_PATH value : \"\" Deploy isecl-scheduler cd /<path>/isecl-k8s-extensions/ kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl Additional optional fields for isecl-scheduler configuration in isecl-scheduler.yaml Field Required Type Default Description LOG_LEVEL Optional string INFO Determines the log level LOG_MAX_LENGTH Optional int 1500 Determines the maximum length of characters in a line in log file TAG_PREFIX Optional string isecl. A custom prefix which can be applied to isecl attributes that are pushed from IH. For example, if the tag-prefix is *isecl.* and *trusted* attribute in CRD becomes *isecl.trusted* . PORT Optional int 8888 ISecl scheduler service port HVS_IHUB_PUBLIC_KEY_PATH Required string Required for IHub with HVS Attestation SGX_IHUB_PUBLIC_KEY_PATH Required string Required for IHub with SGX Attestation TLS_CERT_PATH Required string Path of tls certificate signed by kubernetes CA TLS_KEY_PATH Required string Path of tls key","title":"Deploy Intel\u00ae SecL Extended Scheduler"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#configuring-kube-scheduler-to-establish-communication-with-isecl-scheduler","text":"Note The below is a sample when using kubeadm as the Kubernetes distribution, the scheduler configuration files would be different for any other Kubernetes distributions being used. Add a mount path to the /etc/kubernetes/manifests/kube-scheduler.yaml file for the Intel SecL scheduler extension: - mountPath : /<path>/isecl-k8s-extensions/ name : extendedsched readOnly : true Add a volume path to the /etc/kubernetes/manifests/kube-scheduler.yaml file for the Intel SecL scheduler extension: - hostPath : path : /<path>/isecl-k8s-extensions/ type : \"\" name : extendedsched Add policy-config-file path in the /etc/kubernetes/manifests/kube-scheduler.yaml file under command section: - command : - kube-scheduler - --policy-config-file=/<path>/isecl-k8s-extensions/scheduler-policy.json - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=true Restart kubelet systemctl restart kubelet Logs will be appended to older logs in /var/log/isecl-k8s-extensions Whenever the CRD's are deleted and restarted for updates, the CRD's using the yaml files present under /opt/isecl-k8s-extensions/yamls/ . Kubernetes Version 1.14-1.15 uses crd-1.14.yaml and 1.16-1.17 uses crd-1.17.yaml kubectl delete crd hostattributes.crd.isecl.intel.com kubectl apply -f /opt/isecl-k8s-extensions/yamls/crd-<version>.yaml (Optional) Verify that the Intel \u00ae SecL K8s extensions have been started: To verify the Intel SecL CRDs have been deployed: kubectl get -o json hostattributes.crd.isecl.intel.com","title":"Configuring kube-scheduler to establish communication with isecl-scheduler"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installing-the-key-broker-service","text":"","title":"Installing the Key Broker Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#required-for_7","text":"The KBS is REQUIRED for the following use cases: Workload Confidentiality (both VMs and Containers)","title":"Required For"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#prerequisites_6","text":"The following must be completed before installing the Key Broker: The Verification Service must be installed and available The Authentication and Authorization Service must be installed and available The Certificate Management Service must be installed and available (Recommended; Required if a 3 rd -party Key Management Server will be used) A KMIP 2.0-compliant 3 rd -party Key management Server must be available. The Key Broker will require the KMIP server\u2019s client certificate, client key and root ca certificate. The KMIP server's client certificate must contain a Subject Alternative Name that includes the KMIP server's hostname. The Key Broker uses the gemalto kmip-go client to connect to a KMIP server The Key Broker has been validated using the pykmip 0.9.1 KMIP server as a 3 rd -party Key Management Server. While any general KMIP 2.0-compliant Key Management Server should work, implementation differences among KMIP providers may prevent functionality with specific providers.","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#package-dependencies_4","text":"","title":"Package Dependencies"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#supported-operating-systems_7","text":"The Intel\u00ae Security Libraries Key Broker Service supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04","title":"Supported Operating Systems"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#recommended-hardware_5","text":"","title":"Recommended Hardware"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installation_6","text":"Copy the Key Broker installation binary to the /root/ directory. Create the installation answer file kbs.env: AAS_API_URL = https://<AAS IP or hostname>:8444/aas/v1 CMS_BASE_URL = https://<CMS IP or hostname>:8445/cms/v1/ ENDPOINT_URL = https://<KBS IP or hostname>:9443/kbs/v1/ SAN_LIST = <comma-separated list of hostnames and IP addresses for the Key Broker> CMS_TLS_CERT_SHA384 = <SHA384 hash of CMS TLS certificate> BEARER_TOKEN = <Installation token from populate-users script> ### OPTIONAL - KMIP configuration only KEY_MANAGER = KMIP KMIP_SERVER_IP = <IP address of KMIP server> KMIP_SERVER_PORT = <Port number of KMIP server> KMIP_VERSION = <KMIP protocol version> KMIP_USERNAME = <Username of KMIP server> KMIP_PASSWORD = <Password of KMIP server> ### KMIP_HOSTNAME must be used to provide, KMIP server certificate's SAN(IP/DNS) or valid COMMON NAME. Only FQDN names are allowed. KMIP_HOSTNAME = <Hostname of KMIP server> ### Retrieve the following certificates and keys from the KMIP server KMIP_CLIENT_KEY_PATH = <path>/client_key.pem KMIP_ROOT_CERT_PATH = <path>/root_certificate.pem KMIP_CLIENT_CERT_PATH = <path>/client_certificate.pem Execute the KBS installer. ./kbs-4.0.0.bin","title":"Installation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#configure-the-key-broker-to-use-a-kmip-compliant-key-management-server","text":"The Key Broker must be configured to use a 3 rd -party KMIP key manager as part of installation using kbs.env installation variables. To configure the Key Broker to point to a 3 rd -party KMIP-compliant Key Management Server: Copy the KMIP server\u2019s client certificate, client key and root ca certificate to the Key Broker system Configure the variables in kbs.env for kmip support as below during installation KEY_MANAGER = KMIP KMIP_SERVER_IP = <IP address of KMIP server> KMIP_SERVER_PORT = <Port number of KMIP server> KMIP_HOSTNAME = <hostname of the KMIP server. Must match the hostname used in the Subject Alternative Name fort eh KMIP server client certificate.> ## KMIP_VERSION variable can be used to mention KMIP protocol version. ## This is an OPTIONAL field, default value is set to '2.0'. KBS supports KMIP version '1.4' and '2.0'. KMIP_VERSION = <KMIP protocol version> ## KMIP_HOSTNAME can be used to configure TLS config with ServerName. ## KMIP server certificate should contain SAN(IP/DNS) or valid COMMON NAME and this value can be provided in KMIP_HOSTNAME. Only FQDN names are allowed. ## This is an OPTIONAL field, if KMIP_HOSTNAME is not provided then KMIP_SERVER_IP will be considered as ServerName in TLS configuration. KMIP_HOSTNAME = <Hostname of KMIP server> ## KMIP supports authentication mechanism to authenticate requestor. This is an OPTIONAL field. ## This feature can be added to KBS by updating kbs.env with KMIP_USERNAME and KMIP_PASSWORD. ## These are OPTIONAL variables. PyKMIP doesn't supports this feature. This feature is validated in Thales cipher trust manager. KMIP_USERNAME = <Username of KMIP server> KMIP_PASSWORD = <Password of KMIP server> ### Retrieve the following certificates and keys from the KMIP server KMIP_CLIENT_KEY_PATH = <path>/client_key.pem KMIP_ROOT_CERT_PATH = <path>/root_certificate.pem KMIP_CLIENT_CERT_PATH = <path>/client_certificate.pem The KBS configuration can be found in /etc/kbs/config.yml , KMIP configuration can be updated in this configuration shell kmip: version: \"2.0\" server-ip: \"127.0.0.1\" server-port: \"5696\" hostname: \"localhost\" kmip-username: \"<kmip-username>\" kmip-password: \"<kmip-password>\" client-key-path: \"<path>/client-key.pem\" client-cert-path: \"<path>/client-certificate.pem\" root-cert-path: \"<path>/root-certificate.pem\" Restart the Key Broker for the settings to take effect kbs stop kbs start","title":"Configure the Key Broker to use a KMIP-compliant Key Management Server"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#importing-verification-service-certificates","text":"After installation, the Key Broker must import the SAML and PrivacyCA certificates from any Verification Services it will trust. This provides the Key Broker a way to ensure that only attestations that come from a \u201cknown\u201d Verification Service. The SAML and PrivacyCA certificates needed can be found on the Verification Service.","title":"Importing Verification Service Certificates"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#importing-a-saml-certificate","text":"Display the SAML certificate: cat /etc/hvs/certs/trustedca/saml-crt.pem Use the SAML certificate output in the following POST call to the Key Broker: POST https://<Key Broker IP address or hostname>:9443/kbs/v1/saml-certificates Content-Type: application/x-pem-file -----BEGIN CERTIFICATE----- MIID9TCCAl2gAwIBAgIBCTANBgkqhkiG9w0BAQwFADBQMQswCQYDVQQGEwJVUzEL MAkGA1UECBMCU0YxCzAJBgNVBAcTAlNDMQ4wDAYDVQQKEwVJTlRFTDEXMBUGA1UE AxMOQ01TIFNpZ25pbmcgQ0EwHhcNMTkxMjExMTkzOTU1WhcNMjAxMjExMTkzOTU1 WjAYMRYwFAYDVQQDEw1tdHdpbHNvbi1zYW1sMIIBojANBgkqhkiG9w0BAQEFAAOC AY8AMIIBigKCAYEArbrDpzR4Ry0MVhSJULHZoiVL020YqtyRH+R2NlVXTpJzqmEA Ep2utfcP8+mSCT7DLpGBO6KACPCz3pmqj3wZyqZNTrG7IF2Z4Fuf641fPcxA3WVH 3lXz0L5Ep4jOUdfT8kj4hHxHJVJhDsW4J2fds2RGnn8bZG/QbmmGNRfqdxht0zMh 63ik8jBWNWHxYSRbck27FyTj9hDU+z+rFfIdNv1SiQ9FyndgOytK/m7ijoAetkSF bCsauzUL7DFdRzTmB2GCF/Zd957V51GNpvan6uwqDTL6T4NFX2sqoVduu/WIyTpO /6D2aA741CR3Bmk9945TSeDKZNz2HkihuE+d8ES68W1t4rvox/Noi74e0k35AqcQ Q3P0DZpD+XaRapz5CHcOPwOpZ3A/8wN2f+CS2HqDx8FwABkh7l8OdiIWs8+TDQZe 1x4e/50jE/8zMR/tsAy1EXkm3OTOVxih0u18J84x4OT+rHAIcoQ+TOJ40aHrWGHg kVCfiCUzYYT/W/RBAgMBAAGjEjAQMA4GA1UdDwEB/wQEAwIGwDANBgkqhkiG9w0B AQwFAAOCAYEAP/ABHdPquBNrMOCU+v7SfMLmIfJymA15mCorMEiKZ1d7oNnoPP0G pfyRA4TUiyFLCOLi4jIXWuu4Lt6RUz6bnzn8JRWD5ocIJGGxWjOA66xyS3o9iG7G otOh1pzp5wlwPG7r8ZJ7Q26J+NuHpN1GW5U5Vjww1J9rEKnsKp45QHkG2nXEujdx YXmKXtEG2gOMVjaLiqromf6VxbdNoKHZGEfqU3H5ymMgqIrnXl3MivA30CymCDLl rJGRQSwOfzywPCnUOAVptBwLs2kwOtdvnq+BTK3q/dKKoNiFURj/mQ70egquW9ly TOkYivmKqMZxZlq0//cre4K35aCW3ZArzGgNM8Pk0V/hZp8ZHrWLNAdo4w/Pj1oC Yq7R0RQ8jQerkewYBfrv3O3e9c22h48fsHnun6F3sbcDjws/sWJIOcrPyqJE26HY DmIKpvjqc0jI31ndBBwkb+RIBFkz1Ycob9rsW16uVqbjBFDjJ5QKOdXxhqulyboa JAF53vmU+1jE -----END CERTIFICATE-----","title":"Importing a SAML certificate"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#importing-a-privacyca-certificate","text":"Use OpenSSL to display the PrivacyCA certificate content: openssl x509 -in /etc/hvs/certs/trustedca/privacy-ca/privacy-ca-cert.pem Use the PrivacyCA certificate output in the following POST call to the Key Broker: POST https://<Key Broker IP address or hostname>:9443/kbs/v1/tpm-identity-certificates Content-Type: application/x-pem-file -----BEGIN CERTIFICATE----- MIIHaDCCBdCgAwIBAgIGAW72eWZ9MA0GCSqGSIb3DQEBCwUAMBsxGTAXBgNVBAMT EG10d2lsc29uLXBjYS1haWswHhcNMTkxMjExMTkzOTQxWhcNMjkxMjEwMTkzOTQx WjAbMRkwFwYDVQQDExBtdHdpbHNvbi1wY2EtYWlrMIIBojANBgkqhkiG9w0BAQEF AAOCAY8AMIIBigKCAYEAmWqBr2YiycZbF/QgFbxTr4YiHtueWBdW0sibtH1QRSbI KtkbFsmr6J6QiLBaXcF7KVN6DaD0j5sU4cZSttqKwlSUnn07xjWJRP1EcvSaufO1 MarewgBpFQcI2T6aTs1ziV77BoKz0kWteURz1jT1KSwuattxTelpmgucDp98MqW/ uWsliHUVxh51JTE1yn7Vf1QCWz3a+NDH98Lgr5ks337yx3VBK59Dwtsmfsrd5tMn IuV9Jw0Y2UEdDi004FXI4q64MsMpWA7t5ONRAU+VNU0Y3saXeNBDg9J363imOHIH haP8ixDhqZ+Xb/TGafgFeEHBkJTv6bWpDqodbWVDbgZloxJzcNgtimQw3RbyrB3C KijlEo5BQY6bOcdMG7gCq77u/fbOvLb5IXzS8ZDpwuwCQNnBP4UJXwAflO7COG7P mpj9bTV1OtFiPtYFc4JdGdaf1Pl2zWGeR0c3PIzYQxqvtTVtFX+oRWRsgaEdxKf7 LJx4aIjXwP2s6PIiOSalAgMBAAGjggOwMIIDrDCCAbMGA1UdDgSCAaoEggGmMIIB ojANBgkqhkiG9w0BAQEFAAOCAY8AMIIBigKCAYEAmWqBr2YiycZbF/QgFbxTr4Yi HtueWBdW0sibtH1QRSbIKtkbFsmr6J6QiLBaXcF7KVN6DaD0j5sU4cZSttqKwlSU nn07xjWJRP1EcvSaufO1MarewgBpFQcI2T6aTs1ziV77BoKz0kWteURz1jT1KSwu attxTelpmgucDp98MqW/uWsliHUVxh51JTE1yn7Vf1QCWz3a+NDH98Lgr5ks337y x3VBK59Dwtsmfsrd5tMnIuV9Jw0Y2UEdDi004FXI4q64MsMpWA7t5ONRAU+VNU0Y 3saXeNBDg9J363imOHIHhaP8ixDhqZ+Xb/TGafgFeEHBkJTv6bWpDqodbWVDbgZl oxJzcNgtimQw3RbyrB3CKijlEo5BQY6bOcdMG7gCq77u/fbOvLb5IXzS8ZDpwuwC QNnBP4UJXwAflO7COG7Pmpj9bTV1OtFiPtYFc4JdGdaf1Pl2zWGeR0c3PIzYQxqv tTVtFX+oRWRsgaEdxKf7LJx4aIjXwP2s6PIiOSalAgMBAAEwDwYDVR0TAQH/BAUw AwEB/zCCAeAGA1UdIwSCAdcwggHTgIIBpjCCAaIwDQYJKoZIhvcNAQEBBQADggGP ADCCAYoCggGBAJlqga9mIsnGWxf0IBW8U6+GIh7bnlgXVtLIm7R9UEUmyCrZGxbJ q+iekIiwWl3BeylTeg2g9I+bFOHGUrbaisJUlJ59O8Y1iUT9RHL0mrnztTGq3sIA aRUHCNk+mk7Nc4le+waCs9JFrXlEc9Y09SksLmrbcU3paZoLnA6ffDKlv7lrJYh1 FcYedSUxNcp+1X9UAls92vjQx/fC4K+ZLN9+8sd1QSufQ8LbJn7K3ebTJyLlfScN GNlBHQ4tNOBVyOKuuDLDKVgO7eTjUQFPlTVNGN7Gl3jQQ4PSd+t4pjhyB4Wj/IsQ 4amfl2/0xmn4BXhBwZCU7+m1qQ6qHW1lQ24GZaMSc3DYLYpkMN0W8qwdwioo5RKO QUGOmznHTBu4Aqu+7v32zry2+SF80vGQ6cLsAkDZwT+FCV8AH5Tuwjhuz5qY/W01 dTrRYj7WBXOCXRnWn9T5ds1hnkdHNzyM2EMar7U1bRV/qEVkbIGhHcSn+yyceGiI 18D9rOjyIjkmpQIDAQABoR+kHTAbMRkwFwYDVQQDExBtdHdpbHNvbi1wY2EtYWlr ggYBbvZ5Zn0wDQYJKoZIhvcNAQELBQADggGBAC3PEB8Av0PBJgrJMxzMbuf1FCdD AUrfYmP81Hs0/v70efviMEF2s3GAyLHD9v+1nNFCQrjcNCar18k45BlcodBEmxKA DZoioFykRtlha6ByVvuN6wD93KQbKsXPKhUp8X67fLuOcQgfc3BoDRlw/Ha1Ib6X fliE+rQzLCOgClK7ZdTwl9Ok0VbR7Mbal/xShIqr2WopjBtal9p4RsnIxilTHI+m qzbV8zvZXYfYtEb3MMMT5EnjIV8O498KKOjxohD2vqaxqItd58pOi6z/q5f4pLHc DvdsJecJEoWb2bxWQdBgthMjX6AUV/B5G/LTfaPwVbTLdEc+S6Nrobf/TFYV0pvG OzF3ltYag0fupuYJ991s/JhVwgJhCGq7YourDGkNIWAjt0Z2FWuQKnxWvmResgkS WTeXt+1HCFSo5WcAZWV8R9FYv7tzFxPY8aoLj82sgrOE4IwRqaA8KMbq3anF4RCk +D8k6etqMcNHFS8Fj6GlCd80mb4Q3sxuCiBvZw== -----END CERTIFICATE-----","title":"Importing a PrivacyCA Certificate"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installing-the-workload-policy-manager","text":"","title":"Installing the Workload Policy Manager"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#required-for_8","text":"The WPM is REQUIRED for the following use cases. Workload Confidentiality (both VMs and Containers)","title":"Required For"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#package-dependencies_5","text":"","title":"Package Dependencies"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#supported-operating-systems_8","text":"The Intel\u00ae Security Libraries Workload Policy Manager supports: Red Hat Enterprise Linux 8.2 Ubuntu 18.04","title":"Supported Operating Systems"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#recommended-hardware_6","text":"2 vCPUs RAM: 8 GB 100 GB One network interface with network access to the Key Broker and Workload Service Additional memory and disk space may be required depending on the size of images to be encrypted","title":"Recommended Hardware"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/2Intel%C2%AE%20Security%20Libraries%20Components/#installation_7","text":"Copy the WPM installer to the /root directory Create the wpm.env answer file: KBS_BASE_URL = https://<IP address or hostname of the KBS>:9443/v1/ WPM_SERVICE_USERNAME = <WPM_Service username from populate-users script> WPM_SERVICE_PASSWORD = <WPM Service password from populate-users script> CMS_TLS_CERT_SHA384 = <Sha384 hash of the CMS TLS certificate> CMS_BASE_URL = https://<IP address or hostname for CMS>:8445/cms/v1/ AAS_API_URL = https://<Hostname or IP address of the AAS>:8444/aas/v1 BEARER_TOKEN = <Installation token from populate-users script> For Container Encryption only, add the following line to the wpm.env installation answer file: ##For the CRI-O container runtime: WPM_WITH_CONTAINER_SECURITY_CRIO = yes Execute the WPM installer: ./wpm-v4.0.0.bin","title":"Installation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/30Trusted%20Virtual%20Kubernetes%20Worker%20Nodes/","text":"Trusted Virtual Kubernetes Worker Nodes While the existing Platform Integrity Attestation functions support bare-metal Kubernetes Worker Nodes, using Virtual Machines to host the Worker Nodes is a common deployment architecture. This feature aims to help extend the Chain of Trust to protect the integrity of Virtual Machines, including virtual Kubernetes Worker Nodes. This feature requires the foundational Platform integrity Attestation feature as a prerequisite for the bare-metal servers hosting the virtual Worker Nodes. Note This feature requires a degree of separation between the VM and Kubernetes infrastructure. All physical, bare-metal servers should be virtualization hosts, and all Kubernetes Worker Nodes should be Virtual Machines running on those physical virtualization hosts. Kubernetes clusters should not use a mixture of both virtual and bare-metal Workers. The physical virtualization clusters should not include a mixture of hosts protected by Intel\u00ae SecL Platform integrity Attestation and hosts that are not protected. VM trust reports can only be generated for VM instances launched on hosts with Intel\u00ae SecL services enabled. Also important to note is that this feature alone will not prevent any VMs from launching . VMs will still be launched on Untrusted platforms unless additional steps are taken (for example, using OpenStack orchestration integration with Intel\u00ae SecL, or using the Workload Confidentiality feature to encrypt the Kubernetes Worker Node VM image). This feature generates VM attestation reports that can be used to audit compliance and extend the Chain of Trust, and relies on other datacenter policies and/or Intel\u00ae SecL features to enforce compliance. When libvirt initiates a VM Start, the Intel\u00ae SecL-DC Workload Agent will create a report for the VM that associates the VM\u2019s trust status with the trust status of the host launching the VM. This VM report will be retrievable via the Workload Service, and contains the hardware UUID of the physical server hosting the VM. This UUID can be correlated to the Trust Report of that server at the time of VM launch, creating an audit trail validating that the VM launched on a trusted platform. A new report is created for every VM Start, which includes actions like VM migrations, so that each time a VM is launched or moved a new report is generated ensuring an accurate trust status. By using Platform Integrity and Data Sovereignty-based orchestration (or Workload Confidentiality with encrypted worker VMs) for the Virtual Machines to ensure that the virtual Kubernetes Worker nodes only launch on trusted hardware, these VM trust reports provide an auditing capability to extend the Chain of Trust to the virtual Worker Nodes. Optionally, the Kubernetes Worker Node VM images can be encrypted and protected as per the Workload Confidentiality feature of Intel\u00ae SecL. This adds a layer of enforcement \u2013 rather than simply reporting whether the VM started on a Trusted platform (and is therefore Trusted), Workload Confidentiality ensures that the Worker Node VM image can only be decrypted on compliant platforms. In both cases (with VM image encryption and without), the VM Trust Reports are accessed through the Workload Service: GET https://<Workload Service IP or Hostname>:5000/wls/reports?instance_id=<instance ID> Authorization: Bearer <token> This query will return the latest VM trust report for the provided Instance ID (the Instance ID is the VM\u2019s ID as it is identified by Libvirt; in OpenStack this would correspond directly to the OpenStack Instance ID). As a best practice, Intel\u00ae recommends using an orchestration layer (such as OpenStack) integrated with Intel\u00ae SecL to launch VMs only on Trusted platforms. See the previous section, \u201cIntegration\u201d under the \u201cPlatform Integrity Attestation\u201d feature for details. As an additional layer of protection, the Kubernetes Worker Node VM images can be encrypted using the Workload Confidentiality feature. This adds cryptographic enforcement to the workload orchestration and ensures instances of the Worker Node images will only be launched on Trusted platforms. Prerequisites All physical, bare-metal servers should be virtualization hosts. Virtualization hosts must be Linux platforms using Libvirt. All Kubernetes Worker Nodes should be Virtual Machines running on those physical virtualization hosts. Kubernetes clusters must not use a mixture of both virtual and bare-metal Workers. The physical virtualization clusters must not include a mixture of hosts protected by Intel\u00ae SecL Platform integrity Attestation and hosts that are not protected. VM trust reports can only be generated for VM instances launched on hosts with Intel\u00ae SecL services enabled. The Intel\u00ae SecL Platform integrity Attestation feature must be used to protect all physical virtualization hosts. These platforms must all be registered with the Verification Service, must have the Trust Agent installed and running, and must be Trusted. See the Platform integrity Attestation section for details. In addition to the services required by Platform Integrity Attestation, the Workload Agent must be installed on each physical virtualization host, and the Workload Service must be installed on the management plane. (Optional; recommended) Virtual Machines should be orchestrated using an Intel\u00ae SecL-supported orchestrator, such as OpenStack. This will help launch the VMs only on compliant platforms. (Optional) Virtual Machine Images may be encrypted using the Workload Confidentiality feature. This adds a layer of cryptographic enforcement to the orchestration of virtual worker VMs, ensuring that the VMs can only be launched on compliant platforms. Workflow There are no additional steps required to enable this feature; if the Workload Agent is running on the physical virtualization host, VM trust reports will automatically be generated at every VM Start. Intel\u00ae strongly recommends using an orchestration integration for the VM management layer (for example, the provided Integration Hub integration with OpenStack) to help ensure that the worker node VMs only launch on Trusted physical hosts. If no orchestration is used, the platform service provider should ensure that all physical hosts are always in a Trusted state and take action to ensure Untrusted platforms cannot launch VMs. The primary benefit of the Trusted Virtual Kubernetes Worker Node feature is auditability of the Chain of Trust. By retrieving the VM Trust Report from the Workload Service for a given Worker Node instance, auditors can verify that the VM launched on a Trusted platform. The VM trust report also includes the hardware UUID of the physical host. This UUID, along with the time that the VM instance was launched, can be used to pull the correlating physical host trust report from the Verification Service to provide proof of compliance. To retrieve a VM trust report from the Workload Service: GET https://<Workload Service IP or Hostname>:5000/wls/reports?instance_id=<instance ID> Authorization: Bearer <token> This will return the latest report for the specified instance ID. Sample VM Trust Report A sample VM Trust Report from the Workload Service is below. The report is generated by the Workload Agent and signed using the host\u2019s TPM, then stored in the Workload Service. The report contains some key attributes: instance_id : This is the ID of the instance. In OpenStack, this would correlate directly to the Instance ID for the VM. image_id : This is the ID for the source image used to launch the instance. In OpenStack, this correlates directly to the Image ID for the VM. host_hardware_uuid : The hardware UUID of the physical host that started the VM. This attribute identifies which host performed the VM start and attested the VM. This UUID can be used to query the Verification Service to retrieve attestations of the host. By correlating the VM Trust Report with the Host Trust Report, we can verify that this instance was started on a Trusted platform. image_encrypted : True or False based on whether the source image was protected using the Workload Confidentiality feature. trusted : True or False, based on whether the VM instance was started on a Trusted platform. Because the report is generated at every vm start through Libvirt, a new report will be generated whenever the VM is turned on or migrated, reflecting the state of the VM and its host at every opportunity for the state to change. <Response xmlns= \"http://wls.server.com/wls/reports\" > <instance_manifest> <instance_info> <instance_id> bd06385a-5530-4644-a510-e384b8c3323a </instance_id> <host_hardware_uuid> 00964993-89c1-e711-906e-00163566263e </host_hardware_uuid> <image_id> 773e22da-f687-47ca-89e7-5df655c60b7b </image_id> </instance_info> <image_encrypted> true </image_encrypted> </instance_manifest> <policy_name> Intel VM Policy </policy_name> <results> <e> <rule> <rule_name> EncryptionMatches </rule_name> <markers> <e> IMAGE </e> </markers> <expected> <name> encryption_required </name> <value> true </value> </expected> </rule> <flavor_id> 3a3e1ccf-2618-4a0d-8426-fb7acb1ebabc </flavor_id> <trusted> true </trusted> </e> </results> <trusted> true </trusted> <data> eyJpbnN0YW5jZV9tYW5pZmVzdC\u2026data> <hash_alg> SHA-256 </hash_alg> <cert> -----BEGIN CERTIFICATE----- \u2026 -----END CERTIFICATE----- </cert> <signature> \u2026 </signature> </Response>","title":"Trusted Virtual Kubernetes Worker Nodes"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/30Trusted%20Virtual%20Kubernetes%20Worker%20Nodes/#trusted-virtual-kubernetes-worker-nodes","text":"While the existing Platform Integrity Attestation functions support bare-metal Kubernetes Worker Nodes, using Virtual Machines to host the Worker Nodes is a common deployment architecture. This feature aims to help extend the Chain of Trust to protect the integrity of Virtual Machines, including virtual Kubernetes Worker Nodes. This feature requires the foundational Platform integrity Attestation feature as a prerequisite for the bare-metal servers hosting the virtual Worker Nodes. Note This feature requires a degree of separation between the VM and Kubernetes infrastructure. All physical, bare-metal servers should be virtualization hosts, and all Kubernetes Worker Nodes should be Virtual Machines running on those physical virtualization hosts. Kubernetes clusters should not use a mixture of both virtual and bare-metal Workers. The physical virtualization clusters should not include a mixture of hosts protected by Intel\u00ae SecL Platform integrity Attestation and hosts that are not protected. VM trust reports can only be generated for VM instances launched on hosts with Intel\u00ae SecL services enabled. Also important to note is that this feature alone will not prevent any VMs from launching . VMs will still be launched on Untrusted platforms unless additional steps are taken (for example, using OpenStack orchestration integration with Intel\u00ae SecL, or using the Workload Confidentiality feature to encrypt the Kubernetes Worker Node VM image). This feature generates VM attestation reports that can be used to audit compliance and extend the Chain of Trust, and relies on other datacenter policies and/or Intel\u00ae SecL features to enforce compliance. When libvirt initiates a VM Start, the Intel\u00ae SecL-DC Workload Agent will create a report for the VM that associates the VM\u2019s trust status with the trust status of the host launching the VM. This VM report will be retrievable via the Workload Service, and contains the hardware UUID of the physical server hosting the VM. This UUID can be correlated to the Trust Report of that server at the time of VM launch, creating an audit trail validating that the VM launched on a trusted platform. A new report is created for every VM Start, which includes actions like VM migrations, so that each time a VM is launched or moved a new report is generated ensuring an accurate trust status. By using Platform Integrity and Data Sovereignty-based orchestration (or Workload Confidentiality with encrypted worker VMs) for the Virtual Machines to ensure that the virtual Kubernetes Worker nodes only launch on trusted hardware, these VM trust reports provide an auditing capability to extend the Chain of Trust to the virtual Worker Nodes. Optionally, the Kubernetes Worker Node VM images can be encrypted and protected as per the Workload Confidentiality feature of Intel\u00ae SecL. This adds a layer of enforcement \u2013 rather than simply reporting whether the VM started on a Trusted platform (and is therefore Trusted), Workload Confidentiality ensures that the Worker Node VM image can only be decrypted on compliant platforms. In both cases (with VM image encryption and without), the VM Trust Reports are accessed through the Workload Service: GET https://<Workload Service IP or Hostname>:5000/wls/reports?instance_id=<instance ID> Authorization: Bearer <token> This query will return the latest VM trust report for the provided Instance ID (the Instance ID is the VM\u2019s ID as it is identified by Libvirt; in OpenStack this would correspond directly to the OpenStack Instance ID). As a best practice, Intel\u00ae recommends using an orchestration layer (such as OpenStack) integrated with Intel\u00ae SecL to launch VMs only on Trusted platforms. See the previous section, \u201cIntegration\u201d under the \u201cPlatform Integrity Attestation\u201d feature for details. As an additional layer of protection, the Kubernetes Worker Node VM images can be encrypted using the Workload Confidentiality feature. This adds cryptographic enforcement to the workload orchestration and ensures instances of the Worker Node images will only be launched on Trusted platforms.","title":"Trusted Virtual Kubernetes Worker Nodes"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/30Trusted%20Virtual%20Kubernetes%20Worker%20Nodes/#prerequisites","text":"All physical, bare-metal servers should be virtualization hosts. Virtualization hosts must be Linux platforms using Libvirt. All Kubernetes Worker Nodes should be Virtual Machines running on those physical virtualization hosts. Kubernetes clusters must not use a mixture of both virtual and bare-metal Workers. The physical virtualization clusters must not include a mixture of hosts protected by Intel\u00ae SecL Platform integrity Attestation and hosts that are not protected. VM trust reports can only be generated for VM instances launched on hosts with Intel\u00ae SecL services enabled. The Intel\u00ae SecL Platform integrity Attestation feature must be used to protect all physical virtualization hosts. These platforms must all be registered with the Verification Service, must have the Trust Agent installed and running, and must be Trusted. See the Platform integrity Attestation section for details. In addition to the services required by Platform Integrity Attestation, the Workload Agent must be installed on each physical virtualization host, and the Workload Service must be installed on the management plane. (Optional; recommended) Virtual Machines should be orchestrated using an Intel\u00ae SecL-supported orchestrator, such as OpenStack. This will help launch the VMs only on compliant platforms. (Optional) Virtual Machine Images may be encrypted using the Workload Confidentiality feature. This adds a layer of cryptographic enforcement to the orchestration of virtual worker VMs, ensuring that the VMs can only be launched on compliant platforms.","title":"Prerequisites"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/30Trusted%20Virtual%20Kubernetes%20Worker%20Nodes/#workflow","text":"There are no additional steps required to enable this feature; if the Workload Agent is running on the physical virtualization host, VM trust reports will automatically be generated at every VM Start. Intel\u00ae strongly recommends using an orchestration integration for the VM management layer (for example, the provided Integration Hub integration with OpenStack) to help ensure that the worker node VMs only launch on Trusted physical hosts. If no orchestration is used, the platform service provider should ensure that all physical hosts are always in a Trusted state and take action to ensure Untrusted platforms cannot launch VMs. The primary benefit of the Trusted Virtual Kubernetes Worker Node feature is auditability of the Chain of Trust. By retrieving the VM Trust Report from the Workload Service for a given Worker Node instance, auditors can verify that the VM launched on a Trusted platform. The VM trust report also includes the hardware UUID of the physical host. This UUID, along with the time that the VM instance was launched, can be used to pull the correlating physical host trust report from the Verification Service to provide proof of compliance. To retrieve a VM trust report from the Workload Service: GET https://<Workload Service IP or Hostname>:5000/wls/reports?instance_id=<instance ID> Authorization: Bearer <token> This will return the latest report for the specified instance ID.","title":"Workflow"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/30Trusted%20Virtual%20Kubernetes%20Worker%20Nodes/#sample-vm-trust-report","text":"A sample VM Trust Report from the Workload Service is below. The report is generated by the Workload Agent and signed using the host\u2019s TPM, then stored in the Workload Service. The report contains some key attributes: instance_id : This is the ID of the instance. In OpenStack, this would correlate directly to the Instance ID for the VM. image_id : This is the ID for the source image used to launch the instance. In OpenStack, this correlates directly to the Image ID for the VM. host_hardware_uuid : The hardware UUID of the physical host that started the VM. This attribute identifies which host performed the VM start and attested the VM. This UUID can be used to query the Verification Service to retrieve attestations of the host. By correlating the VM Trust Report with the Host Trust Report, we can verify that this instance was started on a Trusted platform. image_encrypted : True or False based on whether the source image was protected using the Workload Confidentiality feature. trusted : True or False, based on whether the VM instance was started on a Trusted platform. Because the report is generated at every vm start through Libvirt, a new report will be generated whenever the VM is turned on or migrated, reflecting the state of the VM and its host at every opportunity for the state to change. <Response xmlns= \"http://wls.server.com/wls/reports\" > <instance_manifest> <instance_info> <instance_id> bd06385a-5530-4644-a510-e384b8c3323a </instance_id> <host_hardware_uuid> 00964993-89c1-e711-906e-00163566263e </host_hardware_uuid> <image_id> 773e22da-f687-47ca-89e7-5df655c60b7b </image_id> </instance_info> <image_encrypted> true </image_encrypted> </instance_manifest> <policy_name> Intel VM Policy </policy_name> <results> <e> <rule> <rule_name> EncryptionMatches </rule_name> <markers> <e> IMAGE </e> </markers> <expected> <name> encryption_required </name> <value> true </value> </expected> </rule> <flavor_id> 3a3e1ccf-2618-4a0d-8426-fb7acb1ebabc </flavor_id> <trusted> true </trusted> </e> </results> <trusted> true </trusted> <data> eyJpbnN0YW5jZV9tYW5pZmVzdC\u2026data> <hash_alg> SHA-256 </hash_alg> <cert> -----BEGIN CERTIFICATE----- \u2026 -----END CERTIFICATE----- </cert> <signature> \u2026 </signature> </Response>","title":"Sample VM Trust Report"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/","text":"Flavor Management Flavor Format Definitions A Flavor is a standardized set of expectations that determines what platform measurements will be considered \u201ctrusted.\u201d Flavors are constructed in a specific format, containing a metadata section describing the Flavor, and then various other sections depending on the Flavor type or Flavor part. Meta The first part of a Flavor is the meta section: \"meta\" :{ \"vendor\" : \"INTEL\" , \"description\" : { \"flavor_part\" : \"PLATFORM\" , \"bios_name\" : \"Intel Corporation\" , \"bios_version\" : \"SE5C620.86B.00.01.0004.071220170215\" , \"tpm_version\" : \"2.0\" } } This section defines the Flavor part and any versioning information. Note Even when the BIOS or OS version remains the same, the actual measurements in the measured boot process will be different between TPM 1.2 and TPM 2.0, and so the TPM version is captured here as well. The attributes in the Meta section are used by the Flavor matching engine when matching Flavors to Hosts. Note that TPM 1.2 is supported only for VMware ESXi hosts. Hardware The hardware section is unique to PLATFORM flavor parts: \"hardware\" : { \"processor_info\" : \"54 06 05 00 FF FB EB BF\" , \"processor_flags\" : \"fpu vme de \u2026\" , \"feature\" : { \"tpm\" : { \"enabled\" : true , \"pcr_banks\" : [ \"SHA1\" , \"SHA256\" ] }, \"txt\" : { \"enabled\" : true } } } This part of the Flavor defines expected hardware attributes of the host, and contains processor and TPM-related attributes. PCR banks (Algorithms) TPMs can have one or more PCR banks enabled with different hash algorithms. Intel SecL will always attempt to use the most secure algorithm available in the enabled PCR banks. For example, if a given TPM has the following PCR banks enabled: SHA1 SHA256 SHA384 The HVS will prefer the SHA384 PCR bank when creating flavors and performing attestations. The TPM vendor and version, platform OEM, and BIOS version and configuration each impact which PCR banks can potentially be enabled. Some manufacturers will allow users to configure which banks are enabled/disabled in the BIOS. Other manufacturers will enable only one PCR bank, and others will be permanently disabled. Flavors will only utilize a single PCR bank, and when importing from a sample host the HVS will always prefer the strongest algorithm supported by the enabled TPM PCR banks. In teh above example, a flavor imported from that host would use the SHA384 bank for all hash values. This means that all hosts that will be attested using this flavor must also have SHA284 banks enabled in their TPMs. Typically, among otherwise-identical servers this will not be an issue. However, in a mixed environment it can be possible to have an OS flavor, for example, that needs to apply for some hosts that have SHA384 banks enabled, and other servers that only have SHA256 enabled and do not support SHA384. In this circumstance, multiple flavors for the same OS version would need to be created - one for SHA384, and another for SHA256. PCRs The last section of a Flavor is the \u201cPCRs\u201d section, which contains the actual expected measurements for any PCRs. This section will contain PCR measurements for each applicable algorithm supported by the TPM (SHA1 only for TPM 1.2, SHA256 and SHA1 sections for TPM 2.0). Some PCRs simply have a value and nothing else. Other PCRs, however, contain different event measurements. This indicates that separate individual platform or OS components are independently measured and extended to the same PCR. PCRs with event measurements will contain an Event array that lists, in the correct order, all of the events in the measurement event log that are extended to this PCR. When the Verification Service attests a host against a given Flavor, each measurement event is compared to the Flavor value, and all of the events are replayed to confirm that a replay of all of the measurement extensions do in fact result in the hash seen in the PCR value. In this way, the Verification Service can ensure that the measurement event log contents are secure, and the individual measurements can be attested so that the cause for an Untrusted attestation can easily be seen. The full PCRs section is not shown here due to length; see the sample Flavor sections for a full sample. \"pcrs\" : { \"SHA1\" : { \"pcr_0\" : { \"value\" : \"d2ed125942726641a7260c4f92beb67d531a0def\" }, \"pcr_17\" : { \"value\" : \"1ec12004b371e3afd43d04155abde7476a3794fa\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"2fb7d57dcc5455af9ac08d82bdf315dbcc59a044\" , \"label\" : \"HASH_START\" , \"info\" : { \"ComponentName\" : \"HASH_START\" , \"EventName\" : \"OpenSource.EventName\" } }, ... Sample PLATFORM Flavor The PLATFORM Flavor part encompasses measurements that are unique to a specific platform, including the server OEM, BIOS version, etc. A PLATFORM Flavor can be shared across all hosts of the same model that have the same BIOS version. { \"flavor_collection\" : { \"flavors\" : [ { \"meta\" : { \"vendor\" : \"INTEL\" , \"description\" : { \"flavor_part\" : \" PLATFORM\" , \"bios_name\" : \"Intel Corporation\" , \"bios_version\" : \"SE5C620.86B.00.01.0004.071220170215\" , \"tpm_version\" : \"2.0\" } }, \"hardware\" : { \"processor_info\" : \"54 06 05 00 FF FB EB BF\" , \"processor_flags\" : \"fpu vme de \u2026\" , \"feature\" : { \"tpm\" : { \"enabled\" : true , \"pcr_banks\" : [ \"SHA1\" , \"SHA256\" ] }, \"txt\" : { \"enabled\" : true } } }, \"pcrs\" : { \"SHA1\" : { \"pcr_0\" : { \"value\" : \"d2ed125942726641a7260c4f92beb67d531a0def\" }, \"pcr_17\" : { \"value\" : \"1ec12004b371e3afd43d04155abde7476a3794fa\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"2fb7d57dcc5455af9ac08d82bdf315dbcc59a044\" , \"label\" : \"HASH_START\" , \"info\" : { \"ComponentName\" : \"HASH_START\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"ffb1806465d2de1b7531fd5a2a6effaad7c5a047\" , \"label\" : \"BIOSAC_REG_DATA\" , \"info\" : { \"ComponentName\" : \"BIOSAC_REG_DATA\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3c585604e87f855973731fea83e21fab9392d2fc\" , \"label\" : \"CPU_SCRTM_STAT\" , \"info\" : { \"ComponentName\" : \"CPU_SCRTM_STAT\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"9069ca78e7450a285173431b3e52c5c25299e473\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"5ba93c9db0cff93f52b521d7420e43f6eda2784f\" , \"label\" : \"LCP_DETAILS_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_DETAILS_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"5ba93c9db0cff93f52b521d7420e43f6eda2784f\" , \"label\" : \"STM_HASH\" , \"info\" : { \"ComponentName\" : \"STM_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3c585604e87f855973731fea83e21fab9392d2fc\" , \"label\" : \"OSSINITDATA_CAP_HASH\" , \"info\" : { \"ComponentName\" : \"OSSINITDATA_CAP_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3d42560dcf165a5557b3156a21583f2c6dbef10e\" , \"label\" : \"MLE_HASH\" , \"info\" : { \"ComponentName\" : \"MLE_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"274f929dbab8b98a7031bbcd9ea5613c2a28e5e6\" , \"label\" : \"NV_INFO_HASH\" , \"info\" : { \"ComponentName\" : \"NV_INFO_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"ca96de412b4e8c062e570d3013d2fccb4b20250a\" , \"label\" : \"tb_policy\" , \"info\" : { \"ComponentName\" : \"tb_policy\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"d123e2f2b30f1effa8d9522f667af0dac4f48cfb\" , \"label\" : \"vmlinuz\" , \"info\" : { \"ComponentName\" : \"vmlinuz\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"f3742133e1a0deb48177a74ed225418e5cf73fd1\" , \"label\" : \"initrd\" , \"info\" : { \"ComponentName\" : \"initrd\" , \"EventName\" : \"OpenSource.EventName\" } } ] } }, \"SHA256\" : { \"pcr_0\" : { \"value\" : \"db83f0e8a1773c21164c17986037cdf8afc1bbdc1b815772c6da1befb1a7f8a3\" }, \"pcr_17\" : { \"value\" : \"50bd58407a1893056eacff493245cfe785f045b2c0e1cc3e6e9eb5812d8d91bd\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"9301981c093654d5aa3430ba05c880a52eb22b9e18248f5f93e1fe1dab1cb947\" , \"label\" : \"HASH_START\" , \"info\" : { \"ComponentName\" : \"HASH_START\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"2785d1ed65f6b5d4b555dc24ce5e068a44ce8740fe77e01e15a10b1ff66cca90\" , \"label\" : \"BIOSAC_REG_DATA\" , \"info\" : { \"ComponentName\" : \"BIOSAC_REG_DATA\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"67abdd721024f0ff4e0b3f4c2fc13bc5bad42d0b7851d456d88d203d15aaa450\" , \"label\" : \"CPU_SCRTM_STAT\" , \"info\" : { \"ComponentName\" : \"CPU_SCRTM_STAT\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"df3f619804a92fdb4057192dc43dd748ea778adc52bc498ce80524c014b81119\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } } ] } } } Sample OS Flavor An OS Flavor encompasses all of the measurements unique to a given OS. This includes the OS kernel and other measurements. { \"flavor_collection\" : { \"flavors\" : [ { \"meta\" : { \"vendor\" : \"INTEL\" , \"description\" : { \"flavor_part\" : \"OS\" , \"os_name\" : \"RedHatEnterpriseServer\" , \"os_version\" : \"7.3\" , \"vmm_name\" : \"\" , \"vmm_version\" : \"\" , \"tpm_version\" : \"2.0\" } }, \"pcrs\" : { \"SHA1\" : { \"pcr_17\" : { \"value\" : \"1ec12004b371e3afd43d04155abde7476a3794fa\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"2fb7d57dcc5455af9ac08d82bdf315dbcc59a044\" , \"label\" : \"HASH_START\" , \"info\" : { \"ComponentName\" : \"HASH_START\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"ffb1806465d2de1b7531fd5a2a6effaad7c5a047\" , \"label\" : \"BIOSAC_REG_DATA\" , \"info\" : { \"ComponentName\" : \"BIOSAC_REG_DATA\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3c585604e87f855973731fea83e21fab9392d2fc\" , \"label\" : \"CPU_SCRTM_STAT\" , \"info\" : { \"ComponentName\" : \"CPU_SCRTM_STAT\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"9069ca78e7450a285173431b3e52c5c25299e473\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"5ba93c9db0cff93f52b521d7420e43f6eda2784f\" , \"label\" : \"LCP_DETAILS_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_DETAILS_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"5ba93c9db0cff93f52b521d7420e43f6eda2784f\" , \"label\" : \"STM_HASH\" , \"info\" : { \"ComponentName\" : \"STM_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3c585604e87f855973731fea83e21fab9392d2fc\" , \"label\" : \"OSSINITDATA_CAP_HASH\" , \"info\" : { \"ComponentName\" : \"OSSINITDATA_CAP_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3d42560dcf165a5557b3156a21583f2c6dbef10e\" , \"label\" : \"MLE_HASH\" , \"info\" : { \"ComponentName\" : \"MLE_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"274f929dbab8b98a7031bbcd9ea5613c2a28e5e6\" , \"label\" : \"NV_INFO_HASH\" , \"info\" : { \"ComponentName\" : \"NV_INFO_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"ca96de412b4e8c062e570d3013d2fccb4b20250a\" , \"label\" : \"tb_policy\" , \"info\" : { \"ComponentName\" : \"tb_policy\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"d123e2f2b30f1effa8d9522f667af0dac4f48cfb\" , \"label\" : \"vmlinuz\" , \"info\" : { \"ComponentName\" : \"vmlinuz\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"f3742133e1a0deb48177a74ed225418e5cf73fd1\" , \"label\" : \"initrd\" , \"info\" : { \"ComponentName\" : \"initrd\" , \"EventName\" : \"OpenSource.EventName\" } } ] } }, \"SHA256\" : { \"pcr_17\" : { \"value\" : \"50bd58407a1893056eacff493245cfe785f045b2c0e1cc3e6e9eb5812d8d91bd\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"9301981c093654d5aa3430ba05c880a52eb22b9e18248f5f93e1fe1dab1cb947\" , \"label\" : \"HASH_START\" , \"info\" : { \"ComponentName\" : \"HASH_START\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"2785d1ed65f6b5d4b555dc24ce5e068a44ce8740fe77e01e15a10b1ff66cca90\" , \"label\" : \"BIOSAC_REG_DATA\" , \"info\" : { \"ComponentName\" : \"BIOSAC_REG_DATA\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"67abdd721024f0ff4e0b3f4c2fc13bc5bad42d0b7851d456d88d203d15aaa450\" , \"label\" : \"CPU_SCRTM_STAT\" , \"info\" : { \"ComponentName\" : \"CPU_SCRTM_STAT\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"df3f619804a92fdb4057192dc43dd748ea778adc52bc498ce80524c014b81119\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"6e340b9cffb37a989ca544e6bb780a2c78901d3fb33738768511a30617afa01d\" , \"label\" : \"LCP_DETAILS_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_DETAILS_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"6e340b9cffb37a989ca544e6bb780a2c78901d3fb33738768511a30617afa01d\" , \"label\" : \"STM_HASH\" , \"info\" : { \"ComponentName\" : \"STM_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"67abdd721024f0ff4e0b3f4c2fc13bc5bad42d0b7851d456d88d203d15aaa450\" , \"label\" : \"OSSINITDATA_CAP_HASH\" , \"info\" : { \"ComponentName\" : \"OSSINITDATA_CAP_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"26e1d98742f79c950dc637f8c067b0b72a1b0e8ff75db4e609c7e17321acf3f4\" , \"label\" : \"MLE_HASH\" , \"info\" : { \"ComponentName\" : \"MLE_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"0f6e0c7a5944963d7081ea494ddff1e9afa689e148e39f684db06578869ea38b\" , \"label\" : \"NV_INFO_HASH\" , \"info\" : { \"ComponentName\" : \"NV_INFO_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"27808f64e6383982cd3bcc10cfcb3457c0b65f465f779d89b668839eaf263a67\" , \"label\" : \"tb_policy\" , \"info\" : { \"ComponentName\" : \"tb_policy\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"c89ad1d1e9adaa7ecfee2abce763b92472685f7d1b9f3799bf49974b66ed9638\" , \"label\" : \"vmlinuz\" , \"info\" : { \"ComponentName\" : \"vmlinuz\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"81b88e268e697ccf1790d41b9de748a8f395acfb47aa67c9845479d4e8456f77\" , \"label\" : \"initrd\" , \"info\" : { \"ComponentName\" : \"initrd\" , \"EventName\" : \"OpenSource.EventName\" } } ] } } } } ] }, \"flavorgroup_name\" : \"automatic\" } Sample HOST_UNIQUE Flavor Host-Unique flavors define measurements for a specific host. This can be either a single large flavor that incorporates all of the host measurements into a single flavor document used only to attest a single host, or can be a small subset of measurements that are specific to a single host. For example, some VMWare module measurements will change from one host to the next, while most others will be shared assuming the same ESXi build is used. The full Flavor requirement for such a host would include Host-Unique flavors to cover the measurements that are unique to only this one host, and would still use a generic PLATFORM and OS flavor for the other measurements that would be identical for other similarly configured hosts. Note The HOST_UNIQUE Flavors are unique to a specific host, and should always be imported directly from the specific host. { \"flavors\" : [ { \"meta\" : { \"id\" : \"4d387cbd-f72b-4742-b4e5-c5b0ffed59e0\" , \"vendor\" : \"INTEL\" , \"description\" : { \"flavor_part\" : \"HOST_UNIQUE\" , \"source\" : \"Purley11\" , \"bios_name\" : \"Intel Corporation\" , \"bios_version\" : \"SE5C620.86B.00.01.0004.071220170215\" , \"os_name\" : \"RedHatEnterpriseServer\" , \"os_version\" : \"7.4\" , \"tpm_version\" : \"2.0\" , \"hardware_uuid\" : \"00448C61-46F2-E711-906E-001560A04062\" } }, \"pcrs\" : { \"SHA256\" : { \"pcr_17\" : { \"value\" : \"f9ef8c53ddfc8096d36eda5506436c52b4bfa2bd451a89aaa102f03181722176\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"df3f619804a92fdb4057192dc43dd748ea778adc52bc498ce80524c014b81119\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"09f468dfc1d98a1fee86eb7297a56b0e097d57be66db4eae539061332da2e723\" , \"label\" : \"initrd\" , \"info\" : { \"ComponentName\" : \"initrd\" , \"EventName\" : \"OpenSource.EventName\" } } ] }, \"pcr_18\" : { \"value\" : \"c1f7bfdae5f270d9f13aa9620b8977951d6b759f1131fe9f9289317f3a56efa1\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"df3f619804a92fdb4057192dc43dd748ea778adc52bc498ce80524c014b81119\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } } ] } }, \"SHA1\" : { \"pcr_17\" : { \"value\" : \"48695f747a3d494710bd14d20cb0a93c78a485cc\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"9069ca78e7450a285173431b3e52c5c25299e473\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"b1f8db372e396bb128280821b7e0ac54a5ec2791\" , \"label\" : \"initrd\" , \"info\" : { \"ComponentName\" : \"initrd\" , \"EventName\" : \"OpenSource.EventName\" } } ] }, \"pcr_18\" : { \"value\" : \"983ec7db975ed31e2c85ef8e375c038d6d307efb\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"9069ca78e7450a285173431b3e52c5c25299e473\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } } ] } } } } ] } Sample ASSET_TAG Flavor Asset Tag flavor parts are unique to Asset Tag attestation. These flavors verify that the Asset Tag data in the host\u2019s TPM correctly matches the most recently created, currently valid Asset Tag certificate that has been deployed to that host. { \"meta\" : { \"id\" : \"b3e0c056-5b6c-4b6b-95c4-de5f1473cac0\" , \"description\" : { \"flavor_part\" : \"ASSET_TAG\" , \"hardware_uuid\" : \"<Hardware UUID of the server to be tagged>\" } }, \"external\" : { \"asset_tag\" : { \"tag_certificate\" : { \"encoded\" : \"<Tag certificate in base64 encoded format>\" , \"issuer\" : \"CN=assetTagService\" , \"serial_number\" : 1519153541461 , \"subject\" : \"<Hardware UUID of the server to be tagged>\" , \"not_before\" : \"2018-02-20T11:05:41-0800\" , \"not_after\" : \"2019-02-20T11:05:41-0800\" , \"fingerprint_sha384\" : \"46001d8472e56de423aac7c55f061404d27d50e497dfc21a861ef1965d7ac1e44887aee918fb5805385a3cbdf820899d\" , \"attribute\" : [ { \"attr_type\" : { \"id\" : \"2.5.4.789.2\" }, \"attribute_values\" : [ { \"objects\" : {} } ] }, { \"attr_type\" : { \"id\" : \"2.5.4.789.2\" }, \"attribute_values\" : [ { \"objects\" : {} } ] }, { \"attr_type\" : { \"id\" : \"2.5.4.789.2\" }, \"attribute_values\" : [ { \"objects\" : {} } ] } ] } } } } Flavor Templates Added in the Intel SecL-DC 4.0 release, Flavor Templates expose the backend logic that determines which PCRs and event log measurements will be used for specific Flavor parts. Where previously these rules were hardcoded on the backend, this new feature allows new templates to be added, and allows the customization or deletion of existing templates to suit specific business needs. Flavor Templates are conditional rules that apply to a Flavor part cumulatively based on defined conditions. For example a PLATFORM Flavor for a Linux host with Intel TXT enabled would normally include PCR0. If tboot is also enabled, elements from PCR 17 and 18 will be added to the PLATFORM flavor. These are cumulative based on which conditions are true on a given host. By default, Flavor Templates will come pre-populated in the HVS database to meet the same default behavior for previous releases. Flavor Templates can be added, removed, or edited to create customized rules. For example, if there is a specific event log measurement that a user would like to add to an OS flavor, a new Flavor Template can be added for the OS Flavor part that defines a condition for applying the Template, along with the specific event log measurement that should be used when that condition is satisfied. Flavor Templates are cumulative. If a given host matches all of the conditions defined for Flavor Template A and Flavor Template B, both Templates will be applied when importing Flavors from that host. Sample Flavor Template Below is a sample default Flavor Template used for RedHat Enterprise Linux servers with TPM2.0 and tboot enabled: { \"id\" : \"8798fb0c-2dfa-4464-8281-e650a30da7e6\" , \"label\" : \"default-linux-rhel-tpm20-tboot\" , \"condition\" :[ \"//host_info/os_name//*[text()='RedHatEnterprise']\" , \"//host_info/hardware_features/TPM/meta/tpm_version//*[text()='2.0']\" , \"//host_info/tboot_installed//*[text()='true']\" ], \"flavor_parts\" :{ \"OS\" :{ \"meta\" :{ \"tpm_version\" : \"2.0\" , \"tboot_installed\" : true }, \"pcr_rules\" :[ { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 17 }, \"eventlog_includes\" :[ \"vmlinuz\" ] } ] }, \"PLATFORM\" :{ \"meta\" :{ \"tpm_version\" : \"2.0\" , \"tboot_installed\" : true }, \"pcr_rules\" :[ { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 0 }, \"pcr_matches\" : true }, { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 17 }, \"eventlog_equals\" :{ \"excluding_tags\" :[ \"LCP_CONTROL_HASH\" , \"initrd\" , \"vmlinuz\" ] } }, { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 18 }, \"eventlog_equals\" :{ \"excluding_tags\" :[ \"LCP_CONTROL_HASH\" , \"initrd\" , \"vmlinuz\" ] } } ] }, \"HOST_UNIQUE\" :{ \"meta\" :{ \"tpm_version\" : \"2.0\" , \"tboot_installed\" : true }, \"pcr_rules\" :[ { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 17 }, \"eventlog_includes\" :[ \"LCP_CONTROL_HASH\" , \"initrd\" ] }, { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 18 }, \"eventlog_includes\" :[ \"LCP_CONTROL_HASH\" ] } ] } } } Flavor Template Definitions A Flavor Template consists of several sections: The \"id\" and \"label\" keys are unique identifiers. The ID is generated automatically by the HVS when the Template is created; the label is user-specified and must be unique. Conditions The \"condition\" section contains a map of host-info elements to match when deciding to apply the Template. For example: \"condition\" :[ \"//host_info/os_name//*[text()='RedHatEnterprise']\" , \"//host_info/hardware_features/TPM/meta/tpm_version//*[text()='2.0']\" , \"//host_info/tboot_installed//*[text()='true']\" ] This sample contains three conditions, each of which must be true for this Template to apply: os_name: 'RedHatEnterprise' tpm_version: 2.0 tboot_installed: true This will apply for RedHat hosts with TPM2.0 and tboot enabled. If a Flavor is imported from a VMware ESXi host, this template will not apply. The condition paths directly refer to host-info elements collected from the host. The full host-info details for a host can be viewed using the /hvs/v2/host-status API; below is a snippet of the host-info section (note that additional host-info elements may be added as new platform features are incorporated): \"host_info\" : { \"os_name\" : \"RedHatEnterprise\" , \"os_version\" : \"8.3\" , \"os_type\" : \"linux\" , \"bios_version\" : \"SE11111.111.11.11.1111.11111111111\" , \"vmm_name\" : \"\" , \"vmm_version\" : \"\" , \"processor_info\" : \"54 06 05 00 FF FB EB BF\" , \"host_name\" : \"hostname\" , \"bios_name\" : \"Intel Corporation\" , \"hardware_uuid\" : \"<UUID>\" , \"process_flags\" : \"FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE-36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE\" , \"no_of_sockets\" : \"72\" , \"tboot_installed\" : \"false\" , \"is_docker_env\" : \"false\" , \"hardware_features\" : { \"TXT\" : { \"enabled\" : \"true\" }, \"TPM\" : { \"enabled\" : \"true\" , \"meta\" : { \"tpm_version\" : \"2.0\" } }, \"CBNT\" : { \"enabled\" : \"false\" , \"meta\" : { \"profile\" : \"\" , \"msr\" : \"\" } }, \"UEFI\" : { \"enabled\" : \"false\" , \"meta\" : { \"secure_boot_enabled\" : true } } }, \"installed_components\" : [ \"tagent\" ] } Flavor Parts This section of the template will define behaviors for each Flavor part. Each different Flavor part is optional; if the new Template will only affect the OS Flavor part, only the OS Flavor part needs to be defined here. Each Flavor part specified will have its own \"meta\" section where conditional host attributes will be defined. These must match with host-info attributes; for example, in the sample above the OS part uses the following \"meta\" section elements: { \"tpm_version\" : \"2.0\" , \"tboot_installed\" : true } These directly correspond to host-info elements from the hosts this Template will apply to. PCR Rules Each Flavor part section of a Flavor Template may contain 0 or more PCR rules that define PCRs to include. Again using the OS Flavor part example from above, the default Template defines SHA1, SHA256, or SHA384 PCR banks; this tells the HVS to use the \"best\" available PCR bank algorithm, but that each of these algorithms is acceptable. Alternatively, if the Template listed only the SHA384 PCR bank, the resulting Flavor would require the SHA384 PCR bank and would disregard any SHA256 or SHA1 banks, even if the SHA384 bank is unavailable and the SHA256 bank is enabled on the server. The PCR Rules will also contain at least one PCR index, indicating which PCR the rule applies to. pcr_matches PCRs can require a direct PCR value match (when event logs are unnecessary and the final PCR hash is required to match a specific value), and/or can contain event log include/exclude/equals rules. A direct PCR value match requirement is the easiest definitions, but should only be used when a specific PCR is known to always be the same on all hosts that the resulting Flavor will apply to: { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 0 }, \"pcr_matches\" : true } This requires the value of PCR0 to exactly match, and will not examine specific event log details for this PCR index. eventlog_includes The following example shows how to require a specific event log entry to exist: { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 17 }, \"eventlog_includes\" :[ \"vmlinuz\" ] } This rule will require the \"vmlinuz\" event log measurement to be present in the PCR17 event log. excluding_tags Specific event logs can also be excluded; in the below example, all events from PCR17 will be part of the resulting Flavor, but will exclude the LCP_CONTROL_HASH, initrd, and vmlinuz measurement events specifically. This is often used when a specific PCR contains measurements that should apply to different Flavor parts; different rules need to be defined to ensure that the correct events are included in the right Flavor part, and events that will apply for different Flavor parts must be excluded. { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 17 }, \"eventlog_equals\" :{ \"excluding_tags\" :[ \"LCP_CONTROL_HASH\" , \"initrd\" , \"vmlinuz\" ] } } Flavor Matching Flavors are matched to host by objects called Flavor Groups A Flavor Group represents a set of rules to satisfy for a set of flavors to be matched to a host for attestation. For example, a Flavor Group can require that a PLATFORM Flavor and an OS Flavor be used for attestation. Without this level of association, a host that matches measurements for only a PLATFORM flavor, for example, can be attested as Trusted, even though the OS Flavor would attest the host as Untrusted. Flavor matching can be automatic (the default), or can explicitly specify a host to which the Flavor Group must apply. Automatic flavor matching allows for more ease in datacenter lifecycle management with updates and patches that may cause the appropriate flavors to change over time. Automatic flavor matching will trigger a new matching action when a new flavor is added, when an existing flavor is deleted, or when a host is initially attested as Untrusted. The system will automatically attempt to find a new set of flavors that match the Flavor Group rules that will attest the host as Trusted. For example, if a host in your datacenter has recently had a BIOS update, the next attestation will cause the host to appear Untrusted (because the PLATFORM measurements will now differ). Using automatic flavor matching, the Verification Service will automatically search for a new PLATFORM flavor that matches the actual BIOS version and measurement seen on the host. If a new BIOS version is successfully found, the Verification Service will use the new version for attestation, and the host will appear Trusted. If no matching PLATFORM flavor is found, the host will appear Untrusted. When automatic flavor matching is used, think of the various flavors in the Verification Service as a collection of valid configurations, and an attested host matching any combination of those configurations (within the confines of the Flavor Group requirements for which flavor types must be present) will be attested as Trusted. Host-based flavor matching explicitly maps a specific host to a flavor. Host-based attestation requires that a host saves its entire configuration in a composite flavor document in the system, and then later validates against this flavor to detect any changes. In this case, if a host received a BIOS upgrade, the host will attest as Untrusted, and no attempt will be made to re-match a new flavor. An administrator will need to explicitly specify a new flavor to be used for that host. When Does Flavor Matching Happen? Generally speaking, a new Flavor match operation is triggered whenever a host is registered, whenever a host is attested and would be untrusted, and whenever a Flavor is added to or removed from a Flavor group. When a new host is registered, the Verification Service will retrieve the Host Report and derive the platform information needed for Flavor matching (BIOS version, server OEM, OS type and version, TPM version, etc.). The Verification Service then searches through the Flavors in the same Flavor group that the host is in, and finds any Flavors that match the platform information. If a Flavor is deleted, the Verification Service finds any hosts that are currently associated with that Flavor, and attempts to match them to alternative Flavors. If a Flavor is added, the Verification Service looks for any hosts in the same Flavor group that are not currently matched to a Flavor of the appropriate Flavor part, and checks to see whether those hosts should be mapped to the new Flavor. If a new Report is generated for a host and would not result in a Trusted attestation, the Verification Service will first repeat the Flavor matching process to be sure that no matching Flavors exist in the host\u2019s Flavor group that would result in a Trusted attestation. If the Service still finds no matching Flavors, the host will appear as Untrusted. Flavor Matching Performance Flavor matching causes affected hosts to be moved into the QUEUE state while the host and Flavor are evaluated to determine whether the host and Flavor should be linked. Hosts can remain in the QUEUE state for varying amounts of time based on the extent of the Flavor match required. This means that the trust status of a host will not be actually updated to reflect a new Flavor until after the process finishes, which may take a few seconds or minutes depending on the number of registered hosts, Flavors in the same Flavorgroup, etc. If a new host is registered, only that host will be added to the queue, and other hosts will be unaffected. The Verification Service will look for only the HOST\\_UNIQUE flavor part applicable to that specific host, and then will look at all PLATFORM and OS Flavors in the same Flavorgroup has the host, using the Flavor metadata and host info to narrow the results. The Service will match the new host to the most similar Flavors, and then move the host to the CONNECTED state and generate a new trust report. When a new PLATFORM or OS Flavor is created, the Service will instead add all hosts in the same Flavorgroup as the new Flavors to the queue. Each host in the queue will then be re-evaluated against every PLATFORM and OS Flavor in the Flavorgroup to determine the closest match. This means that adding a new Flavor can cause more hosts to each spend more time in the QUEUE state, as compared to adding a new host. For this reason, as a best practice for initial population of Flavors and hosts for a new deployment, it is suggested that Flavors be created before registering hosts. This is not a concern after the initial population of Flavors and hosts. Flavor Groups Flavor Groups represent a collection of one or more Flavors that are possible matches for a collection of one or more hosts. Flavor Groups link to both Flavors and hosts \u2013 a host in Flavor Group \"ABC\" will only be matched to Flavors in Flavor Group \"ABC\" Default Flavor Group By default the Verification Service includes a Flavor Group named automatic and another named unique During host registration, the automatic Flavor Group is used as a default selection if no other Flavor Group is specified. automatic The automatic Flavor Group is used as the default Flavor Group for all hosts and all Flavor parts. If no other Flavor Groups are specified when creating Flavors or Hosts, all Hosts and Flavors will be added to this group. This is useful for datacenters that want to manage a single set of acceptable configurations for all hosts. unique The unique Flavor Group is used to contain HOST\\_UNIQUE Flavors. This Flavorgroup is used by the backend software and should not be managed manually. Flavor Match Policies Flavor Match Policies are used to define how the Flavor Match engine will match Flavors to hosts for attestation for a given Flavor Group. Each Flavor part can have defined Flavor Match Policies within a given Flavor Group. { \"PLATFORM\" : { \"any_of\" , \"required\" }, \"OS\" : { \"all_of\" , \"required_if_defined\" }, \"HOST_UNIQUE\" : { \"latest\" , \"required_if_defined\" }, \"ASSET_TAG\" : { \"latest\" , \"required_if_defined\" }, \"SOFTWARE\" : { \"all_of\" , \"required_if_defined\" } } The sample Policy above would require that a PLATFORM Flavor part be matched, but any PLATFORM Flavor part in the Flavor Group may be matched. The OS Flavor Part will only be required if there is an OS Flavor part in the Flavor Group; if there are no OS Flavor parts in the Group, the match will not be required. If more than one OS Flavor part exists in the Group, all of those OS parts will be required to match for a host to be Trusted. Default Flavor Match Policy The automatic Flavor Group, and any Flavor Group created without explicitly defining a Flavor Match Policy, will be created using the following Flavor Match Policy. This is the default behavior for Flavor Matching: { \"PLATFORM\" : { \"any_of\" , \"required\" }, \"OS\" : { \"any_of\" , \"required\" }, \"HOST_UNIQUE\" : { \"latest\" , \"required_if_defined\" }, \"ASSET_TAG\" : { \"latest\" , \"required_if_defined\" }, \"SOFTWARE\" : { \"all_of\" , \"required_if_defined\" } } ANY_OF The ANY_OF Policy allows any Flavor of the specified Flavor part to be matched. If the Flavor Group contains OS Flavor 1 and OS Flavor 2, a host will be Trusted if it matches either OS Flavor 1 or OS Flavor 2. ALL_OF The ALL_OF Policy requires all Flavors of the specified Flavor Part in the Flavor Group to be matched. For example, if Flavor Group X contains PLATFORM Flavor Part 1 and PLATFORM Flavor Part 2, a host in Flavor Group X will need to match both PLATFORM Flavor 1 and PLATFORM Flavor 2 to attest as Trusted. If the host matches only one of the Flavors, or neither of them, the host will be attested as Untrusted. LATEST The LATEST Policy requires that the most recently created Flavor of the specified Flavor part be used when matching to a host. For example: \"ASSET_TAG\" : { \"latest\" , \"required_if_defined\" } ASSET_TAG Flavor parts by default use the above Policy. This means that if Asset Tag Flavors are in the Flavor Group, the most recently created Asset Tag Flavor will be used. If no Asset Tag Flavors are present in the Flavor Group, then this Flavor part will be ignored. REQUIRED The REQUIRED Policy requires a Flavor of the specified part to be matched. For example: \"PLATFORM\" : { \"any_of\" , \"required\" } This policy means that a PLATFORM Flavor part must be used; if the Flavor Group contains no PLATFORM Flavor parts, hosts in this Flavor Group will always count as Untrusted. REQUIRED_IF_DEFINED The REQUIRED_IF_DEFINED Policy requires that a Flavor part be used if a Flavor of that part exists. If no Flavor part of this type exists in the Flavor Group, the Flavor part will not be required. \"ASSET_TAG\" : { \"latest\" , \"required_if_defined\" } ASSET_TAG Flavor parts by default use the above Policy. This means that if Asset Tag Flavors are in the Flavor Group, the most recently created Asset Tag Flavor will be used. If no Asset Tag Flavors are present in the Flavor Group, then this Flavor part will be ignored. Flavor Match Event Triggers Several events will cause the background queue service to attempt to re-match Flavors and hosts: Host registration This event is the first time a host will be attempted to be matched to appropriate Flavors in the same Flavor Group, and affects only the host that was added (other hosts will not be re-matched to Flavors when you add a new host). Flavor creation When a new Flavor is added to a Flavor Group, the queue system will repeat the Flavor match operation for all hosts in the same Flavor Group as the new Flavor. Flavor deletion When a Flavor is deleted, the queue system will repeat the Flavor match operation for all hosts in the same Flavor Group as the deleted Flavor. Creation of a new Attestation Report When a new Attestation Report is generated, if the host would attest as Untrusted with the currently-matched Flavors, the host being attested will be re-matched as part of the Report generation process. This ensures that Reports are always generated using the best possible Flavor matches available in the database. Sample Flavorgroup API Calls Create a New Flavorgroup POST h tt ps : //<Veri f ica t io n Service IP or Hos tna me> : 8443 /hvs/v 2 / fla vorgroups Au t horiza t io n : Bearer < t oke n > { \"name\" : \"firstTest\" , \"flavor_match_policy_collection\" : { \"flavor_match_policies\" : [ { \"flavor_part\" : \" PLATFORM\" , \"match_policy\" : { \"match_type\" : \"ANY_OF\" , \"required\" : \"REQUIRED\" } } ] } } Response: { \"id\" : \"a0950923-596b-41f7-b9ad-09f525929ba1\" , \"name\" : \"firstTest\" , \"flavor_match_policy_collection\" : { \"flavor_match_policies\" : [ { \"flavor_part\" : \" PLATFORM\" , \"match_policy\" : { \"match_type\" : \"ANY_OF\" , \"required\" : \"REQUIRED\" } } ] } } SOFTWARE Flavor Management What is a SOFTWARE Flavor? A SOFTWARE Flavor part defines the measurements expected for a specific application, or a specific set of files and folders on the physical host. SOFTWARE Flavors can be used to attest the boot-time integrity of any static files or folders on a physical server. A single server can have multiple SOFTWARE Flavors associated. Intel\u00ae SecL-DC provides a default SOFTWARE Flavor that is deployed to each Trust Agent server during the provisioning step. This default Flavor includes the static files and folders of the Trust Agent itself, so that the Trust Agent is measured during the server boot process, and its integrity is included in the attestation of the other server measurements. Using SOFTWARE Flavors consists of two parts \u2013 creating the actual SOFTWARE Flavor, and deploying the SOFTWARE Flavor manifest to the host. Creating a SOFTWARE Flavor part Creating a new SOFTWARE Flavor requires creating a manifest of the files and folders that need to be measured. There are three different types of entries for the manifest: Directories , Symlinks and Files . Directories A Directory defines measurement rules for measuring a directory. Effectively this involves listing the contents of the directory and hashing the results; in this way, a Directory measurement can verify that no files have been added or removed from the directory specified, but will not measure the integrity of individual files (ie, files can change within the directory, but cannot be renamed, added, or removed). Directory entries can use regular expressions to define explicit Include and Exclude filters. For example, Exclude=\\*.log would exclude all files ending with .log from the measurement, meaning files with the .log extension can be added or removed from the directory. <Dir Type= \"dir\" Include= \".*\" Exclude= \"\" Path= \"/opt/trustagent/hypertext/WEB-INF\" > Symlinks A Symlink entry defines a symbolic link that will be measured. The actual symbolic link is hashed, not the file or folder the symlink points to. In this way, the measurement will detect the symbolic link being modified to point to a different location, but the actual file or folder pointed to can have its contents change. <Symlink Path= \"/opt/trustagent/bin/tpm_nvinfo\" > Files Individual files can be explicitly specified for measurement as well. Each file listed will be hashed and extended separately. This means that if any file explicitly listed this way changes its contents or is deleted or moved, the measurement will change, and the host will become Untrusted. <File Path= \"/opt/trustagent/bin/module_analysis_da.sh\" > Sample SOFTWARE Flavor Creation Call Creating a new SOFTWARE Flavor requires specifying a sample host where the application, files or folders that will be measured are currently present. The measurements specified in the manifest will be captures when this call is executed, and the Verification Service will communicate with the Trust Agent and create a SOFTWARE Flavor based on the file measurements. The Connection String must point to the sample Trust Agent host. The Label defines the name of the new Flavor (ideally this should be the name of the application being measured for easier management). POST https:// <Verification Service IP or Hostname > :8443/hvs/v2/flavor-from-app-manifest Authorization: Bearer <token> <ManifestRequest xmlns= \"lib:wml:manifests-req:1.0\" > <connectionString> intel:https://trustagent.server.com:1443;u=trustagentUsername;p=trustagentPassword </connectionString> <Manifest xmlns= \"lib:wml:manifests:1.0\" DigestAlg= \"SHA384\" Label= \"Tomcat\" Uuid= \"\" > + <Dir Type= \"dir\" Include= \".*\" Exclude= \"\" Path= \"/opt/trustagent/hypertext/WEB-INF\" /> <Symlink Path= \"/opt/trustagent/bin/tpm_nvinfo\" /> <File Path= \"/opt/trustagent/bin/module_analysis_da.sh\" /> </Manifest> </ManifestRequest> Deploying a SOFTWARE Flavor Manifest to a Host Once the SOFTWARE Flavor has been created, it can be deployed to any number of Trust Agent servers. This requires the Flavor ID (returned from Flavor creation) and the Host ID (returned from host registration). The Verification Service will send a request to the appropriate Trust Agent and create the manifest. Note After the SOFTWARE Flavor manifest is deployed to a host, the host must be rebooted. This will allow the measurements specified in the Flavor to be taken and extended to the TPM. Until the host is rebooted, the host will now appear Untrusted, as it now requires measurements from a SOFTWARE Flavor that have not yet been extended to the TPM. POST h tt ps : //<Veri f ica t io n Service IP or Hos tna me> : 8443 /hvs/v 2 /rpc/deploy - so ft ware - ma n i fest Au t horiza t io n : Bearer < t oke n > { \"flavor_id\" : \"a6544ff4-6dc7-4c74-82be-578592e7e3ba\" , \"host_id\" : \"a6544ff4-6dc7-4c74-82be-578592e7e3ba\" } SOFTWARE Flavor Matching The default Flavor Match Policy for SOFTWARE Flavor parts is ALL_OF , REQUIRED_IF_DEFINED . This means that all Software Flavors defined in a Flavorgroup must match to all hosts in that Flavorgroup. If no SOFTWARE Flavors are in the Flavorgroup, then hosts can still be considered Trusted. Because the default uses the ALL_OF Policy, it\u2019s recommended to use Flavorgroups dedicated to specific software loadouts. For example, if a number of hosts will act as virtualization hosts and will have SOFTWARE Flavors for the hypervisor and VM management applications, those hosts should be placed in their own Flavorgroup as they will all run similar or identical application loadouts. If another group of servers in the datacenter will act as container hosts, these hosts might need SOFTWARE Flavors that include attestation of container runtimes and management applications, and will have a very different application loadout from the VM-based hosts. These should be placed in their own Flavorgroup, so that the VM hosts are attested using the hypervisor-related SOFTWARE Flavors, and the container hosts are attested using the container-related SOFTWARE Flavors. As with other Flavor parts, hosts will be matched to Flavors in the same Flavorgroup that the host is added to, and will not be matched to Flavors in different Flavorgroups. Flavor matching will happen on the same events as for other Flavor parts. Kernel Upgrades Because the Application Integrity functionality involves adding a measurement agent ( tbootXM ) to initrd , an additional process must be followed when updating the OS kernel to ensure the new initrd also contains the measurement agent. This is not required if Application Integrity will not be used. Update grub to have the boot menu-entry created for the new kernel version in grub.cfg ( grub2-mkconfig -o \\<path to grub file\\> ) Reboot the host and boot into new kernel menu-entry. Generate a new initrd with tbootXM. ( /opt/tbootxm/bin/generate\\_initrd.sh ) Copy the generated initrd to the boot drectory. ( cp /var/tbootxm/\\<generated initrd file name\\> /boot/ ) Update the TCB protection menu-entry with the new kernel version. Source rustagent.env , or expor t GRUB_FILE=/boo t /e f i/EFI/redha t /grub.c f g Run the configure_host script: cd /opt/tbootxm/bin ./configure_host.sh Update the default boot menu-entry to have new kernel version. (edit /etc/default/grub ) Update the grub to reflect the updates. ( grub2-mkconfig -o \\<path to grub file\\> ) Reboot the host and boot into TCB protection menu-entry. After updating the system with the new initrd , the Software Flavor should attest as Trusted. Note that changing grub and initrd does result in a new OS Flavor measurements, so an updated OS Flavor should be imported after updating the kernel and regenerating initrd .","title":"Flavor Management"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#flavor-management","text":"","title":"Flavor Management"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#flavor-format-definitions","text":"A Flavor is a standardized set of expectations that determines what platform measurements will be considered \u201ctrusted.\u201d Flavors are constructed in a specific format, containing a metadata section describing the Flavor, and then various other sections depending on the Flavor type or Flavor part.","title":"Flavor Format Definitions"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#meta","text":"The first part of a Flavor is the meta section: \"meta\" :{ \"vendor\" : \"INTEL\" , \"description\" : { \"flavor_part\" : \"PLATFORM\" , \"bios_name\" : \"Intel Corporation\" , \"bios_version\" : \"SE5C620.86B.00.01.0004.071220170215\" , \"tpm_version\" : \"2.0\" } } This section defines the Flavor part and any versioning information. Note Even when the BIOS or OS version remains the same, the actual measurements in the measured boot process will be different between TPM 1.2 and TPM 2.0, and so the TPM version is captured here as well. The attributes in the Meta section are used by the Flavor matching engine when matching Flavors to Hosts. Note that TPM 1.2 is supported only for VMware ESXi hosts.","title":"Meta"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#hardware","text":"The hardware section is unique to PLATFORM flavor parts: \"hardware\" : { \"processor_info\" : \"54 06 05 00 FF FB EB BF\" , \"processor_flags\" : \"fpu vme de \u2026\" , \"feature\" : { \"tpm\" : { \"enabled\" : true , \"pcr_banks\" : [ \"SHA1\" , \"SHA256\" ] }, \"txt\" : { \"enabled\" : true } } } This part of the Flavor defines expected hardware attributes of the host, and contains processor and TPM-related attributes.","title":"Hardware"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#pcr-banks-algorithms","text":"TPMs can have one or more PCR banks enabled with different hash algorithms. Intel SecL will always attempt to use the most secure algorithm available in the enabled PCR banks. For example, if a given TPM has the following PCR banks enabled: SHA1 SHA256 SHA384 The HVS will prefer the SHA384 PCR bank when creating flavors and performing attestations. The TPM vendor and version, platform OEM, and BIOS version and configuration each impact which PCR banks can potentially be enabled. Some manufacturers will allow users to configure which banks are enabled/disabled in the BIOS. Other manufacturers will enable only one PCR bank, and others will be permanently disabled. Flavors will only utilize a single PCR bank, and when importing from a sample host the HVS will always prefer the strongest algorithm supported by the enabled TPM PCR banks. In teh above example, a flavor imported from that host would use the SHA384 bank for all hash values. This means that all hosts that will be attested using this flavor must also have SHA284 banks enabled in their TPMs. Typically, among otherwise-identical servers this will not be an issue. However, in a mixed environment it can be possible to have an OS flavor, for example, that needs to apply for some hosts that have SHA384 banks enabled, and other servers that only have SHA256 enabled and do not support SHA384. In this circumstance, multiple flavors for the same OS version would need to be created - one for SHA384, and another for SHA256.","title":"PCR banks (Algorithms)"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#pcrs","text":"The last section of a Flavor is the \u201cPCRs\u201d section, which contains the actual expected measurements for any PCRs. This section will contain PCR measurements for each applicable algorithm supported by the TPM (SHA1 only for TPM 1.2, SHA256 and SHA1 sections for TPM 2.0). Some PCRs simply have a value and nothing else. Other PCRs, however, contain different event measurements. This indicates that separate individual platform or OS components are independently measured and extended to the same PCR. PCRs with event measurements will contain an Event array that lists, in the correct order, all of the events in the measurement event log that are extended to this PCR. When the Verification Service attests a host against a given Flavor, each measurement event is compared to the Flavor value, and all of the events are replayed to confirm that a replay of all of the measurement extensions do in fact result in the hash seen in the PCR value. In this way, the Verification Service can ensure that the measurement event log contents are secure, and the individual measurements can be attested so that the cause for an Untrusted attestation can easily be seen. The full PCRs section is not shown here due to length; see the sample Flavor sections for a full sample. \"pcrs\" : { \"SHA1\" : { \"pcr_0\" : { \"value\" : \"d2ed125942726641a7260c4f92beb67d531a0def\" }, \"pcr_17\" : { \"value\" : \"1ec12004b371e3afd43d04155abde7476a3794fa\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"2fb7d57dcc5455af9ac08d82bdf315dbcc59a044\" , \"label\" : \"HASH_START\" , \"info\" : { \"ComponentName\" : \"HASH_START\" , \"EventName\" : \"OpenSource.EventName\" } }, ...","title":"PCRs"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#sample-platform-flavor","text":"The PLATFORM Flavor part encompasses measurements that are unique to a specific platform, including the server OEM, BIOS version, etc. A PLATFORM Flavor can be shared across all hosts of the same model that have the same BIOS version. { \"flavor_collection\" : { \"flavors\" : [ { \"meta\" : { \"vendor\" : \"INTEL\" , \"description\" : { \"flavor_part\" : \" PLATFORM\" , \"bios_name\" : \"Intel Corporation\" , \"bios_version\" : \"SE5C620.86B.00.01.0004.071220170215\" , \"tpm_version\" : \"2.0\" } }, \"hardware\" : { \"processor_info\" : \"54 06 05 00 FF FB EB BF\" , \"processor_flags\" : \"fpu vme de \u2026\" , \"feature\" : { \"tpm\" : { \"enabled\" : true , \"pcr_banks\" : [ \"SHA1\" , \"SHA256\" ] }, \"txt\" : { \"enabled\" : true } } }, \"pcrs\" : { \"SHA1\" : { \"pcr_0\" : { \"value\" : \"d2ed125942726641a7260c4f92beb67d531a0def\" }, \"pcr_17\" : { \"value\" : \"1ec12004b371e3afd43d04155abde7476a3794fa\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"2fb7d57dcc5455af9ac08d82bdf315dbcc59a044\" , \"label\" : \"HASH_START\" , \"info\" : { \"ComponentName\" : \"HASH_START\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"ffb1806465d2de1b7531fd5a2a6effaad7c5a047\" , \"label\" : \"BIOSAC_REG_DATA\" , \"info\" : { \"ComponentName\" : \"BIOSAC_REG_DATA\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3c585604e87f855973731fea83e21fab9392d2fc\" , \"label\" : \"CPU_SCRTM_STAT\" , \"info\" : { \"ComponentName\" : \"CPU_SCRTM_STAT\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"9069ca78e7450a285173431b3e52c5c25299e473\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"5ba93c9db0cff93f52b521d7420e43f6eda2784f\" , \"label\" : \"LCP_DETAILS_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_DETAILS_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"5ba93c9db0cff93f52b521d7420e43f6eda2784f\" , \"label\" : \"STM_HASH\" , \"info\" : { \"ComponentName\" : \"STM_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3c585604e87f855973731fea83e21fab9392d2fc\" , \"label\" : \"OSSINITDATA_CAP_HASH\" , \"info\" : { \"ComponentName\" : \"OSSINITDATA_CAP_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3d42560dcf165a5557b3156a21583f2c6dbef10e\" , \"label\" : \"MLE_HASH\" , \"info\" : { \"ComponentName\" : \"MLE_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"274f929dbab8b98a7031bbcd9ea5613c2a28e5e6\" , \"label\" : \"NV_INFO_HASH\" , \"info\" : { \"ComponentName\" : \"NV_INFO_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"ca96de412b4e8c062e570d3013d2fccb4b20250a\" , \"label\" : \"tb_policy\" , \"info\" : { \"ComponentName\" : \"tb_policy\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"d123e2f2b30f1effa8d9522f667af0dac4f48cfb\" , \"label\" : \"vmlinuz\" , \"info\" : { \"ComponentName\" : \"vmlinuz\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"f3742133e1a0deb48177a74ed225418e5cf73fd1\" , \"label\" : \"initrd\" , \"info\" : { \"ComponentName\" : \"initrd\" , \"EventName\" : \"OpenSource.EventName\" } } ] } }, \"SHA256\" : { \"pcr_0\" : { \"value\" : \"db83f0e8a1773c21164c17986037cdf8afc1bbdc1b815772c6da1befb1a7f8a3\" }, \"pcr_17\" : { \"value\" : \"50bd58407a1893056eacff493245cfe785f045b2c0e1cc3e6e9eb5812d8d91bd\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"9301981c093654d5aa3430ba05c880a52eb22b9e18248f5f93e1fe1dab1cb947\" , \"label\" : \"HASH_START\" , \"info\" : { \"ComponentName\" : \"HASH_START\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"2785d1ed65f6b5d4b555dc24ce5e068a44ce8740fe77e01e15a10b1ff66cca90\" , \"label\" : \"BIOSAC_REG_DATA\" , \"info\" : { \"ComponentName\" : \"BIOSAC_REG_DATA\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"67abdd721024f0ff4e0b3f4c2fc13bc5bad42d0b7851d456d88d203d15aaa450\" , \"label\" : \"CPU_SCRTM_STAT\" , \"info\" : { \"ComponentName\" : \"CPU_SCRTM_STAT\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"df3f619804a92fdb4057192dc43dd748ea778adc52bc498ce80524c014b81119\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } } ] } } }","title":"Sample PLATFORM Flavor"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#sample-os-flavor","text":"An OS Flavor encompasses all of the measurements unique to a given OS. This includes the OS kernel and other measurements. { \"flavor_collection\" : { \"flavors\" : [ { \"meta\" : { \"vendor\" : \"INTEL\" , \"description\" : { \"flavor_part\" : \"OS\" , \"os_name\" : \"RedHatEnterpriseServer\" , \"os_version\" : \"7.3\" , \"vmm_name\" : \"\" , \"vmm_version\" : \"\" , \"tpm_version\" : \"2.0\" } }, \"pcrs\" : { \"SHA1\" : { \"pcr_17\" : { \"value\" : \"1ec12004b371e3afd43d04155abde7476a3794fa\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"2fb7d57dcc5455af9ac08d82bdf315dbcc59a044\" , \"label\" : \"HASH_START\" , \"info\" : { \"ComponentName\" : \"HASH_START\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"ffb1806465d2de1b7531fd5a2a6effaad7c5a047\" , \"label\" : \"BIOSAC_REG_DATA\" , \"info\" : { \"ComponentName\" : \"BIOSAC_REG_DATA\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3c585604e87f855973731fea83e21fab9392d2fc\" , \"label\" : \"CPU_SCRTM_STAT\" , \"info\" : { \"ComponentName\" : \"CPU_SCRTM_STAT\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"9069ca78e7450a285173431b3e52c5c25299e473\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"5ba93c9db0cff93f52b521d7420e43f6eda2784f\" , \"label\" : \"LCP_DETAILS_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_DETAILS_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"5ba93c9db0cff93f52b521d7420e43f6eda2784f\" , \"label\" : \"STM_HASH\" , \"info\" : { \"ComponentName\" : \"STM_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3c585604e87f855973731fea83e21fab9392d2fc\" , \"label\" : \"OSSINITDATA_CAP_HASH\" , \"info\" : { \"ComponentName\" : \"OSSINITDATA_CAP_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"3d42560dcf165a5557b3156a21583f2c6dbef10e\" , \"label\" : \"MLE_HASH\" , \"info\" : { \"ComponentName\" : \"MLE_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"274f929dbab8b98a7031bbcd9ea5613c2a28e5e6\" , \"label\" : \"NV_INFO_HASH\" , \"info\" : { \"ComponentName\" : \"NV_INFO_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"ca96de412b4e8c062e570d3013d2fccb4b20250a\" , \"label\" : \"tb_policy\" , \"info\" : { \"ComponentName\" : \"tb_policy\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"d123e2f2b30f1effa8d9522f667af0dac4f48cfb\" , \"label\" : \"vmlinuz\" , \"info\" : { \"ComponentName\" : \"vmlinuz\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"f3742133e1a0deb48177a74ed225418e5cf73fd1\" , \"label\" : \"initrd\" , \"info\" : { \"ComponentName\" : \"initrd\" , \"EventName\" : \"OpenSource.EventName\" } } ] } }, \"SHA256\" : { \"pcr_17\" : { \"value\" : \"50bd58407a1893056eacff493245cfe785f045b2c0e1cc3e6e9eb5812d8d91bd\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"9301981c093654d5aa3430ba05c880a52eb22b9e18248f5f93e1fe1dab1cb947\" , \"label\" : \"HASH_START\" , \"info\" : { \"ComponentName\" : \"HASH_START\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"2785d1ed65f6b5d4b555dc24ce5e068a44ce8740fe77e01e15a10b1ff66cca90\" , \"label\" : \"BIOSAC_REG_DATA\" , \"info\" : { \"ComponentName\" : \"BIOSAC_REG_DATA\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"67abdd721024f0ff4e0b3f4c2fc13bc5bad42d0b7851d456d88d203d15aaa450\" , \"label\" : \"CPU_SCRTM_STAT\" , \"info\" : { \"ComponentName\" : \"CPU_SCRTM_STAT\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"df3f619804a92fdb4057192dc43dd748ea778adc52bc498ce80524c014b81119\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"6e340b9cffb37a989ca544e6bb780a2c78901d3fb33738768511a30617afa01d\" , \"label\" : \"LCP_DETAILS_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_DETAILS_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"6e340b9cffb37a989ca544e6bb780a2c78901d3fb33738768511a30617afa01d\" , \"label\" : \"STM_HASH\" , \"info\" : { \"ComponentName\" : \"STM_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"67abdd721024f0ff4e0b3f4c2fc13bc5bad42d0b7851d456d88d203d15aaa450\" , \"label\" : \"OSSINITDATA_CAP_HASH\" , \"info\" : { \"ComponentName\" : \"OSSINITDATA_CAP_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"26e1d98742f79c950dc637f8c067b0b72a1b0e8ff75db4e609c7e17321acf3f4\" , \"label\" : \"MLE_HASH\" , \"info\" : { \"ComponentName\" : \"MLE_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"0f6e0c7a5944963d7081ea494ddff1e9afa689e148e39f684db06578869ea38b\" , \"label\" : \"NV_INFO_HASH\" , \"info\" : { \"ComponentName\" : \"NV_INFO_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"27808f64e6383982cd3bcc10cfcb3457c0b65f465f779d89b668839eaf263a67\" , \"label\" : \"tb_policy\" , \"info\" : { \"ComponentName\" : \"tb_policy\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"c89ad1d1e9adaa7ecfee2abce763b92472685f7d1b9f3799bf49974b66ed9638\" , \"label\" : \"vmlinuz\" , \"info\" : { \"ComponentName\" : \"vmlinuz\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"81b88e268e697ccf1790d41b9de748a8f395acfb47aa67c9845479d4e8456f77\" , \"label\" : \"initrd\" , \"info\" : { \"ComponentName\" : \"initrd\" , \"EventName\" : \"OpenSource.EventName\" } } ] } } } } ] }, \"flavorgroup_name\" : \"automatic\" }","title":"Sample OS Flavor"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#sample-host_unique-flavor","text":"Host-Unique flavors define measurements for a specific host. This can be either a single large flavor that incorporates all of the host measurements into a single flavor document used only to attest a single host, or can be a small subset of measurements that are specific to a single host. For example, some VMWare module measurements will change from one host to the next, while most others will be shared assuming the same ESXi build is used. The full Flavor requirement for such a host would include Host-Unique flavors to cover the measurements that are unique to only this one host, and would still use a generic PLATFORM and OS flavor for the other measurements that would be identical for other similarly configured hosts. Note The HOST_UNIQUE Flavors are unique to a specific host, and should always be imported directly from the specific host. { \"flavors\" : [ { \"meta\" : { \"id\" : \"4d387cbd-f72b-4742-b4e5-c5b0ffed59e0\" , \"vendor\" : \"INTEL\" , \"description\" : { \"flavor_part\" : \"HOST_UNIQUE\" , \"source\" : \"Purley11\" , \"bios_name\" : \"Intel Corporation\" , \"bios_version\" : \"SE5C620.86B.00.01.0004.071220170215\" , \"os_name\" : \"RedHatEnterpriseServer\" , \"os_version\" : \"7.4\" , \"tpm_version\" : \"2.0\" , \"hardware_uuid\" : \"00448C61-46F2-E711-906E-001560A04062\" } }, \"pcrs\" : { \"SHA256\" : { \"pcr_17\" : { \"value\" : \"f9ef8c53ddfc8096d36eda5506436c52b4bfa2bd451a89aaa102f03181722176\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"df3f619804a92fdb4057192dc43dd748ea778adc52bc498ce80524c014b81119\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"09f468dfc1d98a1fee86eb7297a56b0e097d57be66db4eae539061332da2e723\" , \"label\" : \"initrd\" , \"info\" : { \"ComponentName\" : \"initrd\" , \"EventName\" : \"OpenSource.EventName\" } } ] }, \"pcr_18\" : { \"value\" : \"c1f7bfdae5f270d9f13aa9620b8977951d6b759f1131fe9f9289317f3a56efa1\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha256\" , \"value\" : \"df3f619804a92fdb4057192dc43dd748ea778adc52bc498ce80524c014b81119\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } } ] } }, \"SHA1\" : { \"pcr_17\" : { \"value\" : \"48695f747a3d494710bd14d20cb0a93c78a485cc\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"9069ca78e7450a285173431b3e52c5c25299e473\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } }, { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"b1f8db372e396bb128280821b7e0ac54a5ec2791\" , \"label\" : \"initrd\" , \"info\" : { \"ComponentName\" : \"initrd\" , \"EventName\" : \"OpenSource.EventName\" } } ] }, \"pcr_18\" : { \"value\" : \"983ec7db975ed31e2c85ef8e375c038d6d307efb\" , \"event\" : [ { \"digest_type\" : \"com.intel.mtwilson.lib.common.model.MeasurementSha1\" , \"value\" : \"9069ca78e7450a285173431b3e52c5c25299e473\" , \"label\" : \"LCP_CONTROL_HASH\" , \"info\" : { \"ComponentName\" : \"LCP_CONTROL_HASH\" , \"EventName\" : \"OpenSource.EventName\" } } ] } } } } ] }","title":"Sample HOST_UNIQUE Flavor"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#sample-asset_tag-flavor","text":"Asset Tag flavor parts are unique to Asset Tag attestation. These flavors verify that the Asset Tag data in the host\u2019s TPM correctly matches the most recently created, currently valid Asset Tag certificate that has been deployed to that host. { \"meta\" : { \"id\" : \"b3e0c056-5b6c-4b6b-95c4-de5f1473cac0\" , \"description\" : { \"flavor_part\" : \"ASSET_TAG\" , \"hardware_uuid\" : \"<Hardware UUID of the server to be tagged>\" } }, \"external\" : { \"asset_tag\" : { \"tag_certificate\" : { \"encoded\" : \"<Tag certificate in base64 encoded format>\" , \"issuer\" : \"CN=assetTagService\" , \"serial_number\" : 1519153541461 , \"subject\" : \"<Hardware UUID of the server to be tagged>\" , \"not_before\" : \"2018-02-20T11:05:41-0800\" , \"not_after\" : \"2019-02-20T11:05:41-0800\" , \"fingerprint_sha384\" : \"46001d8472e56de423aac7c55f061404d27d50e497dfc21a861ef1965d7ac1e44887aee918fb5805385a3cbdf820899d\" , \"attribute\" : [ { \"attr_type\" : { \"id\" : \"2.5.4.789.2\" }, \"attribute_values\" : [ { \"objects\" : {} } ] }, { \"attr_type\" : { \"id\" : \"2.5.4.789.2\" }, \"attribute_values\" : [ { \"objects\" : {} } ] }, { \"attr_type\" : { \"id\" : \"2.5.4.789.2\" }, \"attribute_values\" : [ { \"objects\" : {} } ] } ] } } } }","title":"Sample ASSET_TAG Flavor"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#flavor-templates","text":"Added in the Intel SecL-DC 4.0 release, Flavor Templates expose the backend logic that determines which PCRs and event log measurements will be used for specific Flavor parts. Where previously these rules were hardcoded on the backend, this new feature allows new templates to be added, and allows the customization or deletion of existing templates to suit specific business needs. Flavor Templates are conditional rules that apply to a Flavor part cumulatively based on defined conditions. For example a PLATFORM Flavor for a Linux host with Intel TXT enabled would normally include PCR0. If tboot is also enabled, elements from PCR 17 and 18 will be added to the PLATFORM flavor. These are cumulative based on which conditions are true on a given host. By default, Flavor Templates will come pre-populated in the HVS database to meet the same default behavior for previous releases. Flavor Templates can be added, removed, or edited to create customized rules. For example, if there is a specific event log measurement that a user would like to add to an OS flavor, a new Flavor Template can be added for the OS Flavor part that defines a condition for applying the Template, along with the specific event log measurement that should be used when that condition is satisfied. Flavor Templates are cumulative. If a given host matches all of the conditions defined for Flavor Template A and Flavor Template B, both Templates will be applied when importing Flavors from that host.","title":"Flavor Templates"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#sample-flavor-template","text":"Below is a sample default Flavor Template used for RedHat Enterprise Linux servers with TPM2.0 and tboot enabled: { \"id\" : \"8798fb0c-2dfa-4464-8281-e650a30da7e6\" , \"label\" : \"default-linux-rhel-tpm20-tboot\" , \"condition\" :[ \"//host_info/os_name//*[text()='RedHatEnterprise']\" , \"//host_info/hardware_features/TPM/meta/tpm_version//*[text()='2.0']\" , \"//host_info/tboot_installed//*[text()='true']\" ], \"flavor_parts\" :{ \"OS\" :{ \"meta\" :{ \"tpm_version\" : \"2.0\" , \"tboot_installed\" : true }, \"pcr_rules\" :[ { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 17 }, \"eventlog_includes\" :[ \"vmlinuz\" ] } ] }, \"PLATFORM\" :{ \"meta\" :{ \"tpm_version\" : \"2.0\" , \"tboot_installed\" : true }, \"pcr_rules\" :[ { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 0 }, \"pcr_matches\" : true }, { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 17 }, \"eventlog_equals\" :{ \"excluding_tags\" :[ \"LCP_CONTROL_HASH\" , \"initrd\" , \"vmlinuz\" ] } }, { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 18 }, \"eventlog_equals\" :{ \"excluding_tags\" :[ \"LCP_CONTROL_HASH\" , \"initrd\" , \"vmlinuz\" ] } } ] }, \"HOST_UNIQUE\" :{ \"meta\" :{ \"tpm_version\" : \"2.0\" , \"tboot_installed\" : true }, \"pcr_rules\" :[ { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 17 }, \"eventlog_includes\" :[ \"LCP_CONTROL_HASH\" , \"initrd\" ] }, { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 18 }, \"eventlog_includes\" :[ \"LCP_CONTROL_HASH\" ] } ] } } }","title":"Sample Flavor Template"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#flavor-template-definitions","text":"A Flavor Template consists of several sections: The \"id\" and \"label\" keys are unique identifiers. The ID is generated automatically by the HVS when the Template is created; the label is user-specified and must be unique.","title":"Flavor Template Definitions"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#conditions","text":"The \"condition\" section contains a map of host-info elements to match when deciding to apply the Template. For example: \"condition\" :[ \"//host_info/os_name//*[text()='RedHatEnterprise']\" , \"//host_info/hardware_features/TPM/meta/tpm_version//*[text()='2.0']\" , \"//host_info/tboot_installed//*[text()='true']\" ] This sample contains three conditions, each of which must be true for this Template to apply: os_name: 'RedHatEnterprise' tpm_version: 2.0 tboot_installed: true This will apply for RedHat hosts with TPM2.0 and tboot enabled. If a Flavor is imported from a VMware ESXi host, this template will not apply. The condition paths directly refer to host-info elements collected from the host. The full host-info details for a host can be viewed using the /hvs/v2/host-status API; below is a snippet of the host-info section (note that additional host-info elements may be added as new platform features are incorporated): \"host_info\" : { \"os_name\" : \"RedHatEnterprise\" , \"os_version\" : \"8.3\" , \"os_type\" : \"linux\" , \"bios_version\" : \"SE11111.111.11.11.1111.11111111111\" , \"vmm_name\" : \"\" , \"vmm_version\" : \"\" , \"processor_info\" : \"54 06 05 00 FF FB EB BF\" , \"host_name\" : \"hostname\" , \"bios_name\" : \"Intel Corporation\" , \"hardware_uuid\" : \"<UUID>\" , \"process_flags\" : \"FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE-36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE\" , \"no_of_sockets\" : \"72\" , \"tboot_installed\" : \"false\" , \"is_docker_env\" : \"false\" , \"hardware_features\" : { \"TXT\" : { \"enabled\" : \"true\" }, \"TPM\" : { \"enabled\" : \"true\" , \"meta\" : { \"tpm_version\" : \"2.0\" } }, \"CBNT\" : { \"enabled\" : \"false\" , \"meta\" : { \"profile\" : \"\" , \"msr\" : \"\" } }, \"UEFI\" : { \"enabled\" : \"false\" , \"meta\" : { \"secure_boot_enabled\" : true } } }, \"installed_components\" : [ \"tagent\" ] }","title":"Conditions"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#flavor-parts","text":"This section of the template will define behaviors for each Flavor part. Each different Flavor part is optional; if the new Template will only affect the OS Flavor part, only the OS Flavor part needs to be defined here. Each Flavor part specified will have its own \"meta\" section where conditional host attributes will be defined. These must match with host-info attributes; for example, in the sample above the OS part uses the following \"meta\" section elements: { \"tpm_version\" : \"2.0\" , \"tboot_installed\" : true } These directly correspond to host-info elements from the hosts this Template will apply to.","title":"Flavor Parts"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#pcr-rules","text":"Each Flavor part section of a Flavor Template may contain 0 or more PCR rules that define PCRs to include. Again using the OS Flavor part example from above, the default Template defines SHA1, SHA256, or SHA384 PCR banks; this tells the HVS to use the \"best\" available PCR bank algorithm, but that each of these algorithms is acceptable. Alternatively, if the Template listed only the SHA384 PCR bank, the resulting Flavor would require the SHA384 PCR bank and would disregard any SHA256 or SHA1 banks, even if the SHA384 bank is unavailable and the SHA256 bank is enabled on the server. The PCR Rules will also contain at least one PCR index, indicating which PCR the rule applies to.","title":"PCR Rules"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#pcr_matches","text":"PCRs can require a direct PCR value match (when event logs are unnecessary and the final PCR hash is required to match a specific value), and/or can contain event log include/exclude/equals rules. A direct PCR value match requirement is the easiest definitions, but should only be used when a specific PCR is known to always be the same on all hosts that the resulting Flavor will apply to: { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 0 }, \"pcr_matches\" : true } This requires the value of PCR0 to exactly match, and will not examine specific event log details for this PCR index.","title":"pcr_matches"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#eventlog_includes","text":"The following example shows how to require a specific event log entry to exist: { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 17 }, \"eventlog_includes\" :[ \"vmlinuz\" ] } This rule will require the \"vmlinuz\" event log measurement to be present in the PCR17 event log.","title":"eventlog_includes"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#excluding_tags","text":"Specific event logs can also be excluded; in the below example, all events from PCR17 will be part of the resulting Flavor, but will exclude the LCP_CONTROL_HASH, initrd, and vmlinuz measurement events specifically. This is often used when a specific PCR contains measurements that should apply to different Flavor parts; different rules need to be defined to ensure that the correct events are included in the right Flavor part, and events that will apply for different Flavor parts must be excluded. { \"pcr\" :{ \"bank\" :[ \"SHA384\" , \"SHA256\" , \"SHA1\" ], \"index\" : 17 }, \"eventlog_equals\" :{ \"excluding_tags\" :[ \"LCP_CONTROL_HASH\" , \"initrd\" , \"vmlinuz\" ] } }","title":"excluding_tags"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#flavor-matching","text":"Flavors are matched to host by objects called Flavor Groups A Flavor Group represents a set of rules to satisfy for a set of flavors to be matched to a host for attestation. For example, a Flavor Group can require that a PLATFORM Flavor and an OS Flavor be used for attestation. Without this level of association, a host that matches measurements for only a PLATFORM flavor, for example, can be attested as Trusted, even though the OS Flavor would attest the host as Untrusted. Flavor matching can be automatic (the default), or can explicitly specify a host to which the Flavor Group must apply. Automatic flavor matching allows for more ease in datacenter lifecycle management with updates and patches that may cause the appropriate flavors to change over time. Automatic flavor matching will trigger a new matching action when a new flavor is added, when an existing flavor is deleted, or when a host is initially attested as Untrusted. The system will automatically attempt to find a new set of flavors that match the Flavor Group rules that will attest the host as Trusted. For example, if a host in your datacenter has recently had a BIOS update, the next attestation will cause the host to appear Untrusted (because the PLATFORM measurements will now differ). Using automatic flavor matching, the Verification Service will automatically search for a new PLATFORM flavor that matches the actual BIOS version and measurement seen on the host. If a new BIOS version is successfully found, the Verification Service will use the new version for attestation, and the host will appear Trusted. If no matching PLATFORM flavor is found, the host will appear Untrusted. When automatic flavor matching is used, think of the various flavors in the Verification Service as a collection of valid configurations, and an attested host matching any combination of those configurations (within the confines of the Flavor Group requirements for which flavor types must be present) will be attested as Trusted. Host-based flavor matching explicitly maps a specific host to a flavor. Host-based attestation requires that a host saves its entire configuration in a composite flavor document in the system, and then later validates against this flavor to detect any changes. In this case, if a host received a BIOS upgrade, the host will attest as Untrusted, and no attempt will be made to re-match a new flavor. An administrator will need to explicitly specify a new flavor to be used for that host.","title":"Flavor Matching"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#when-does-flavor-matching-happen","text":"Generally speaking, a new Flavor match operation is triggered whenever a host is registered, whenever a host is attested and would be untrusted, and whenever a Flavor is added to or removed from a Flavor group. When a new host is registered, the Verification Service will retrieve the Host Report and derive the platform information needed for Flavor matching (BIOS version, server OEM, OS type and version, TPM version, etc.). The Verification Service then searches through the Flavors in the same Flavor group that the host is in, and finds any Flavors that match the platform information. If a Flavor is deleted, the Verification Service finds any hosts that are currently associated with that Flavor, and attempts to match them to alternative Flavors. If a Flavor is added, the Verification Service looks for any hosts in the same Flavor group that are not currently matched to a Flavor of the appropriate Flavor part, and checks to see whether those hosts should be mapped to the new Flavor. If a new Report is generated for a host and would not result in a Trusted attestation, the Verification Service will first repeat the Flavor matching process to be sure that no matching Flavors exist in the host\u2019s Flavor group that would result in a Trusted attestation. If the Service still finds no matching Flavors, the host will appear as Untrusted.","title":"When Does Flavor Matching Happen?"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#flavor-matching-performance","text":"Flavor matching causes affected hosts to be moved into the QUEUE state while the host and Flavor are evaluated to determine whether the host and Flavor should be linked. Hosts can remain in the QUEUE state for varying amounts of time based on the extent of the Flavor match required. This means that the trust status of a host will not be actually updated to reflect a new Flavor until after the process finishes, which may take a few seconds or minutes depending on the number of registered hosts, Flavors in the same Flavorgroup, etc. If a new host is registered, only that host will be added to the queue, and other hosts will be unaffected. The Verification Service will look for only the HOST\\_UNIQUE flavor part applicable to that specific host, and then will look at all PLATFORM and OS Flavors in the same Flavorgroup has the host, using the Flavor metadata and host info to narrow the results. The Service will match the new host to the most similar Flavors, and then move the host to the CONNECTED state and generate a new trust report. When a new PLATFORM or OS Flavor is created, the Service will instead add all hosts in the same Flavorgroup as the new Flavors to the queue. Each host in the queue will then be re-evaluated against every PLATFORM and OS Flavor in the Flavorgroup to determine the closest match. This means that adding a new Flavor can cause more hosts to each spend more time in the QUEUE state, as compared to adding a new host. For this reason, as a best practice for initial population of Flavors and hosts for a new deployment, it is suggested that Flavors be created before registering hosts. This is not a concern after the initial population of Flavors and hosts.","title":"Flavor Matching Performance"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#flavor-groups","text":"Flavor Groups represent a collection of one or more Flavors that are possible matches for a collection of one or more hosts. Flavor Groups link to both Flavors and hosts \u2013 a host in Flavor Group \"ABC\" will only be matched to Flavors in Flavor Group \"ABC\"","title":"Flavor Groups"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#default-flavor-group","text":"By default the Verification Service includes a Flavor Group named automatic and another named unique During host registration, the automatic Flavor Group is used as a default selection if no other Flavor Group is specified.","title":"Default Flavor Group"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#automatic","text":"The automatic Flavor Group is used as the default Flavor Group for all hosts and all Flavor parts. If no other Flavor Groups are specified when creating Flavors or Hosts, all Hosts and Flavors will be added to this group. This is useful for datacenters that want to manage a single set of acceptable configurations for all hosts.","title":"automatic"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#unique","text":"The unique Flavor Group is used to contain HOST\\_UNIQUE Flavors. This Flavorgroup is used by the backend software and should not be managed manually.","title":"unique"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#flavor-match-policies","text":"Flavor Match Policies are used to define how the Flavor Match engine will match Flavors to hosts for attestation for a given Flavor Group. Each Flavor part can have defined Flavor Match Policies within a given Flavor Group. { \"PLATFORM\" : { \"any_of\" , \"required\" }, \"OS\" : { \"all_of\" , \"required_if_defined\" }, \"HOST_UNIQUE\" : { \"latest\" , \"required_if_defined\" }, \"ASSET_TAG\" : { \"latest\" , \"required_if_defined\" }, \"SOFTWARE\" : { \"all_of\" , \"required_if_defined\" } } The sample Policy above would require that a PLATFORM Flavor part be matched, but any PLATFORM Flavor part in the Flavor Group may be matched. The OS Flavor Part will only be required if there is an OS Flavor part in the Flavor Group; if there are no OS Flavor parts in the Group, the match will not be required. If more than one OS Flavor part exists in the Group, all of those OS parts will be required to match for a host to be Trusted.","title":"Flavor Match Policies"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#default-flavor-match-policy","text":"The automatic Flavor Group, and any Flavor Group created without explicitly defining a Flavor Match Policy, will be created using the following Flavor Match Policy. This is the default behavior for Flavor Matching: { \"PLATFORM\" : { \"any_of\" , \"required\" }, \"OS\" : { \"any_of\" , \"required\" }, \"HOST_UNIQUE\" : { \"latest\" , \"required_if_defined\" }, \"ASSET_TAG\" : { \"latest\" , \"required_if_defined\" }, \"SOFTWARE\" : { \"all_of\" , \"required_if_defined\" } }","title":"Default Flavor Match Policy"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#any_of","text":"The ANY_OF Policy allows any Flavor of the specified Flavor part to be matched. If the Flavor Group contains OS Flavor 1 and OS Flavor 2, a host will be Trusted if it matches either OS Flavor 1 or OS Flavor 2.","title":"ANY_OF"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#all_of","text":"The ALL_OF Policy requires all Flavors of the specified Flavor Part in the Flavor Group to be matched. For example, if Flavor Group X contains PLATFORM Flavor Part 1 and PLATFORM Flavor Part 2, a host in Flavor Group X will need to match both PLATFORM Flavor 1 and PLATFORM Flavor 2 to attest as Trusted. If the host matches only one of the Flavors, or neither of them, the host will be attested as Untrusted.","title":"ALL_OF"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#latest","text":"The LATEST Policy requires that the most recently created Flavor of the specified Flavor part be used when matching to a host. For example: \"ASSET_TAG\" : { \"latest\" , \"required_if_defined\" } ASSET_TAG Flavor parts by default use the above Policy. This means that if Asset Tag Flavors are in the Flavor Group, the most recently created Asset Tag Flavor will be used. If no Asset Tag Flavors are present in the Flavor Group, then this Flavor part will be ignored.","title":"LATEST"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#required","text":"The REQUIRED Policy requires a Flavor of the specified part to be matched. For example: \"PLATFORM\" : { \"any_of\" , \"required\" } This policy means that a PLATFORM Flavor part must be used; if the Flavor Group contains no PLATFORM Flavor parts, hosts in this Flavor Group will always count as Untrusted.","title":"REQUIRED"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#required_if_defined","text":"The REQUIRED_IF_DEFINED Policy requires that a Flavor part be used if a Flavor of that part exists. If no Flavor part of this type exists in the Flavor Group, the Flavor part will not be required. \"ASSET_TAG\" : { \"latest\" , \"required_if_defined\" } ASSET_TAG Flavor parts by default use the above Policy. This means that if Asset Tag Flavors are in the Flavor Group, the most recently created Asset Tag Flavor will be used. If no Asset Tag Flavors are present in the Flavor Group, then this Flavor part will be ignored.","title":"REQUIRED_IF_DEFINED"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#flavor-match-event-triggers","text":"Several events will cause the background queue service to attempt to re-match Flavors and hosts: Host registration This event is the first time a host will be attempted to be matched to appropriate Flavors in the same Flavor Group, and affects only the host that was added (other hosts will not be re-matched to Flavors when you add a new host). Flavor creation When a new Flavor is added to a Flavor Group, the queue system will repeat the Flavor match operation for all hosts in the same Flavor Group as the new Flavor. Flavor deletion When a Flavor is deleted, the queue system will repeat the Flavor match operation for all hosts in the same Flavor Group as the deleted Flavor. Creation of a new Attestation Report When a new Attestation Report is generated, if the host would attest as Untrusted with the currently-matched Flavors, the host being attested will be re-matched as part of the Report generation process. This ensures that Reports are always generated using the best possible Flavor matches available in the database.","title":"Flavor Match Event Triggers"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#sample-flavorgroup-api-calls","text":"","title":"Sample Flavorgroup API Calls"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#create-a-new-flavorgroup","text":"POST h tt ps : //<Veri f ica t io n Service IP or Hos tna me> : 8443 /hvs/v 2 / fla vorgroups Au t horiza t io n : Bearer < t oke n > { \"name\" : \"firstTest\" , \"flavor_match_policy_collection\" : { \"flavor_match_policies\" : [ { \"flavor_part\" : \" PLATFORM\" , \"match_policy\" : { \"match_type\" : \"ANY_OF\" , \"required\" : \"REQUIRED\" } } ] } } Response: { \"id\" : \"a0950923-596b-41f7-b9ad-09f525929ba1\" , \"name\" : \"firstTest\" , \"flavor_match_policy_collection\" : { \"flavor_match_policies\" : [ { \"flavor_part\" : \" PLATFORM\" , \"match_policy\" : { \"match_type\" : \"ANY_OF\" , \"required\" : \"REQUIRED\" } } ] } }","title":"Create a New Flavorgroup"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#software-flavor-management","text":"","title":"SOFTWARE Flavor Management"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#what-is-a-software-flavor","text":"A SOFTWARE Flavor part defines the measurements expected for a specific application, or a specific set of files and folders on the physical host. SOFTWARE Flavors can be used to attest the boot-time integrity of any static files or folders on a physical server. A single server can have multiple SOFTWARE Flavors associated. Intel\u00ae SecL-DC provides a default SOFTWARE Flavor that is deployed to each Trust Agent server during the provisioning step. This default Flavor includes the static files and folders of the Trust Agent itself, so that the Trust Agent is measured during the server boot process, and its integrity is included in the attestation of the other server measurements. Using SOFTWARE Flavors consists of two parts \u2013 creating the actual SOFTWARE Flavor, and deploying the SOFTWARE Flavor manifest to the host.","title":"What is a SOFTWARE Flavor?"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#creating-a-software-flavor-part","text":"Creating a new SOFTWARE Flavor requires creating a manifest of the files and folders that need to be measured. There are three different types of entries for the manifest: Directories , Symlinks and Files .","title":"Creating a SOFTWARE Flavor part"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#directories","text":"A Directory defines measurement rules for measuring a directory. Effectively this involves listing the contents of the directory and hashing the results; in this way, a Directory measurement can verify that no files have been added or removed from the directory specified, but will not measure the integrity of individual files (ie, files can change within the directory, but cannot be renamed, added, or removed). Directory entries can use regular expressions to define explicit Include and Exclude filters. For example, Exclude=\\*.log would exclude all files ending with .log from the measurement, meaning files with the .log extension can be added or removed from the directory. <Dir Type= \"dir\" Include= \".*\" Exclude= \"\" Path= \"/opt/trustagent/hypertext/WEB-INF\" >","title":"Directories"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#symlinks","text":"A Symlink entry defines a symbolic link that will be measured. The actual symbolic link is hashed, not the file or folder the symlink points to. In this way, the measurement will detect the symbolic link being modified to point to a different location, but the actual file or folder pointed to can have its contents change. <Symlink Path= \"/opt/trustagent/bin/tpm_nvinfo\" >","title":"Symlinks"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#files","text":"Individual files can be explicitly specified for measurement as well. Each file listed will be hashed and extended separately. This means that if any file explicitly listed this way changes its contents or is deleted or moved, the measurement will change, and the host will become Untrusted. <File Path= \"/opt/trustagent/bin/module_analysis_da.sh\" >","title":"Files"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#sample-software-flavor-creation-call","text":"Creating a new SOFTWARE Flavor requires specifying a sample host where the application, files or folders that will be measured are currently present. The measurements specified in the manifest will be captures when this call is executed, and the Verification Service will communicate with the Trust Agent and create a SOFTWARE Flavor based on the file measurements. The Connection String must point to the sample Trust Agent host. The Label defines the name of the new Flavor (ideally this should be the name of the application being measured for easier management). POST https:// <Verification Service IP or Hostname > :8443/hvs/v2/flavor-from-app-manifest Authorization: Bearer <token> <ManifestRequest xmlns= \"lib:wml:manifests-req:1.0\" > <connectionString> intel:https://trustagent.server.com:1443;u=trustagentUsername;p=trustagentPassword </connectionString> <Manifest xmlns= \"lib:wml:manifests:1.0\" DigestAlg= \"SHA384\" Label= \"Tomcat\" Uuid= \"\" > + <Dir Type= \"dir\" Include= \".*\" Exclude= \"\" Path= \"/opt/trustagent/hypertext/WEB-INF\" /> <Symlink Path= \"/opt/trustagent/bin/tpm_nvinfo\" /> <File Path= \"/opt/trustagent/bin/module_analysis_da.sh\" /> </Manifest> </ManifestRequest>","title":"Sample SOFTWARE Flavor Creation Call"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#deploying-a-software-flavor-manifest-to-a-host","text":"Once the SOFTWARE Flavor has been created, it can be deployed to any number of Trust Agent servers. This requires the Flavor ID (returned from Flavor creation) and the Host ID (returned from host registration). The Verification Service will send a request to the appropriate Trust Agent and create the manifest. Note After the SOFTWARE Flavor manifest is deployed to a host, the host must be rebooted. This will allow the measurements specified in the Flavor to be taken and extended to the TPM. Until the host is rebooted, the host will now appear Untrusted, as it now requires measurements from a SOFTWARE Flavor that have not yet been extended to the TPM. POST h tt ps : //<Veri f ica t io n Service IP or Hos tna me> : 8443 /hvs/v 2 /rpc/deploy - so ft ware - ma n i fest Au t horiza t io n : Bearer < t oke n > { \"flavor_id\" : \"a6544ff4-6dc7-4c74-82be-578592e7e3ba\" , \"host_id\" : \"a6544ff4-6dc7-4c74-82be-578592e7e3ba\" }","title":"Deploying a SOFTWARE Flavor Manifest to a Host"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#software-flavor-matching","text":"The default Flavor Match Policy for SOFTWARE Flavor parts is ALL_OF , REQUIRED_IF_DEFINED . This means that all Software Flavors defined in a Flavorgroup must match to all hosts in that Flavorgroup. If no SOFTWARE Flavors are in the Flavorgroup, then hosts can still be considered Trusted. Because the default uses the ALL_OF Policy, it\u2019s recommended to use Flavorgroups dedicated to specific software loadouts. For example, if a number of hosts will act as virtualization hosts and will have SOFTWARE Flavors for the hypervisor and VM management applications, those hosts should be placed in their own Flavorgroup as they will all run similar or identical application loadouts. If another group of servers in the datacenter will act as container hosts, these hosts might need SOFTWARE Flavors that include attestation of container runtimes and management applications, and will have a very different application loadout from the VM-based hosts. These should be placed in their own Flavorgroup, so that the VM hosts are attested using the hypervisor-related SOFTWARE Flavors, and the container hosts are attested using the container-related SOFTWARE Flavors. As with other Flavor parts, hosts will be matched to Flavors in the same Flavorgroup that the host is added to, and will not be matched to Flavors in different Flavorgroups. Flavor matching will happen on the same events as for other Flavor parts.","title":"SOFTWARE Flavor Matching"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/31Flavor%20Management/#kernel-upgrades","text":"Because the Application Integrity functionality involves adding a measurement agent ( tbootXM ) to initrd , an additional process must be followed when updating the OS kernel to ensure the new initrd also contains the measurement agent. This is not required if Application Integrity will not be used. Update grub to have the boot menu-entry created for the new kernel version in grub.cfg ( grub2-mkconfig -o \\<path to grub file\\> ) Reboot the host and boot into new kernel menu-entry. Generate a new initrd with tbootXM. ( /opt/tbootxm/bin/generate\\_initrd.sh ) Copy the generated initrd to the boot drectory. ( cp /var/tbootxm/\\<generated initrd file name\\> /boot/ ) Update the TCB protection menu-entry with the new kernel version. Source rustagent.env , or expor t GRUB_FILE=/boo t /e f i/EFI/redha t /grub.c f g Run the configure_host script: cd /opt/tbootxm/bin ./configure_host.sh Update the default boot menu-entry to have new kernel version. (edit /etc/default/grub ) Update the grub to reflect the updates. ( grub2-mkconfig -o \\<path to grub file\\> ) Reboot the host and boot into TCB protection menu-entry. After updating the system with the new initrd , the Software Flavor should attest as Trusted. Note that changing grub and initrd does result in a new OS Flavor measurements, so an updated OS Flavor should be imported after updating the kernel and regenerating initrd .","title":"Kernel Upgrades"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/32Scalability%20and%20Sizing/","text":"Scalability and Sizing Configuration Maximums Registered Hosts The Intel\u00ae SecL Verification Service can support a maximum of 2000 registered hosts with a single Verification Service instance with default settings and the specified minimum required hardware. The Verification Service can support up to 100,000 registered hosts, but will require a minimum of 16 vCPUs and 16GB RAM. The service is primarily CPU-limited. HDD Space The HDD space recommendations below represent expected log and database growth using default settings. Altering the database or log rotation settings, or the SAML expiration setting, may change the amount of disk space required. For default settings, 100 GB of disk space is recommended. Database Rotation Settings The Intel\u00ae SecL Verification Service database will automatically rotate the audit log table after one million records, and will retain up to ten total rotations. These settings are user-configurable if a longer retention period is needed. mtwilson.audit.log.num.rotations - defines the maximum number of rotations before the oldest rotation is deleted to make space for a new rotation. mtwilson.audit.log.max.row.count \u2013 defines the maximum number of rows in the audit log table before a rotation will occur. Log Rotation The Intel\u00ae SecL services (the Verification Service, Trust Agent, and Integration Hub) use Logrotate to rotate logs automatically during a daily cron job. By default, logs are rotated once per month or when they exceed 1 GB in size, whichever comes first, and 12 total rotations will be retained.","title":"Scalability and Sizing"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/32Scalability%20and%20Sizing/#scalability-and-sizing","text":"","title":"Scalability and Sizing"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/32Scalability%20and%20Sizing/#configuration-maximums","text":"","title":"Configuration Maximums"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/32Scalability%20and%20Sizing/#registered-hosts","text":"The Intel\u00ae SecL Verification Service can support a maximum of 2000 registered hosts with a single Verification Service instance with default settings and the specified minimum required hardware. The Verification Service can support up to 100,000 registered hosts, but will require a minimum of 16 vCPUs and 16GB RAM. The service is primarily CPU-limited.","title":"Registered Hosts"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/32Scalability%20and%20Sizing/#hdd-space","text":"The HDD space recommendations below represent expected log and database growth using default settings. Altering the database or log rotation settings, or the SAML expiration setting, may change the amount of disk space required. For default settings, 100 GB of disk space is recommended.","title":"HDD Space"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/32Scalability%20and%20Sizing/#database-rotation-settings","text":"The Intel\u00ae SecL Verification Service database will automatically rotate the audit log table after one million records, and will retain up to ten total rotations. These settings are user-configurable if a longer retention period is needed. mtwilson.audit.log.num.rotations - defines the maximum number of rotations before the oldest rotation is deleted to make space for a new rotation. mtwilson.audit.log.max.row.count \u2013 defines the maximum number of rows in the audit log table before a rotation will occur.","title":"Database Rotation Settings"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/32Scalability%20and%20Sizing/#log-rotation","text":"The Intel\u00ae SecL services (the Verification Service, Trust Agent, and Integration Hub) use Logrotate to rotate logs automatically during a daily cron job. By default, logs are rotated once per month or when they exceed 1 GB in size, whichever comes first, and 12 total rotations will be retained.","title":"Log Rotation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/","text":"Verification Service Installation Answer File Options # Authentication URL and service account credentials - mandatory AAS_API_URL=https://isecl-aas:8444/aas/v1 HVS_SERVICE_USERNAME=HVS_service HVS_SERVICE_PASSWORD=password # CMS URL and CMS webserivce TLS hash for server verification - mandatory CMS_BASE_URL=https://isecl-cms:8445/cms/v1 CMS_TLS_CERT_SHA384=digest # Installation admin bearer token for CSR approval request to CMS - mandatory BEARER_TOKEN=eyJhbGciOiJSUzM4NCIsImtpZCI6ImE\u2026 # Skip setup - optional HVS_NOSETUP=false #default=false # Logging options - optional HVS_LOGLEVEL=info # options: critical|error|warning|info|debug|trace, default='info' HVS_LOG_MAX_LENGTH=300 # default=300 HVS_ENABLE_CONSOLE_LOG=false # default=false # HRRS configuration - optional HRRS_REFRESH_PERIOD=2m0s # default=2m0s HRRS_REFRESH_LOOK_AHEAD=5m0s # default=5m0s # FVS configuration - optional FVS_NUMBER_OF_VERIFIERS=20 # default=20 FVS_NUMBER_OF_DATA_FETCHERS=20 # default=20 FVS_SKIP_FLAVOR_SIGNATURE_VERIFICATION=false # default=false # In case of trusted flavor storage, flavor signature verification can be skipped # using following flag - optional SKIP_FLAVOR_SIGNATURE_VERIFICATION=false # default=false # TLS certificate configuration - optional TLS_COMMON_NAME=\"HVS TLS Certificate\" # default=\"HVS TLS Certificate\" TLS_SAN_LIST=127.0.0.1,localhost # default=127.0.0.1,localhost # Server configuration - optional HVS_PORT=8443 # default=8443 HVS_SERVER_READ_TIMEOUT=30s # default=30s HVS_SERVER_READ_HEADER_TIMEOUT=10s # default=10s HVS_SERVER_WRITE_TIMEOUT=10s # default=10s HVS_SERVER_IDLE_TIMEOUT=10s # default=10s HVS_SERVER_MAX_HEADER_BYTES=1048576 # default=1048576 # Database - mandatory HVS_DB_USERNAME=runner HVS_DB_PASSWORD=test HVS_DB_SSLCERTSRC=/tmp/dbcert.pem # This doesn't need to be specified if HVS_DB_SSLCERT is given # Database - optional HVS_DB_HOSTNAME=localhost # default=localhost HVS_DB_NAME=hvs-pg-db # default=hvs-pg-db HVS_DB_PORT=5432 # default=5432 HVS_DB_SSLMODE=verify-full # default=verify-full ;other options are like allow, prefer, require, verify-ca HVS_DB_SSLCERT=/etc/hvs/hvsdbcert.pem # default=/etc/hvs/hvsdbcert.pem # Webservice configuration - Optional HVS_PORT=8443 HVS_SERVER_READ_TIMEOUT=30s HVS_SERVER_READ_HEADER_TIMEOUT=10s HVS_SERVER_WRITE_TIMEOUT=10s HVS_SERVER_IDLE_TIMEOUT=10s HVS_SERVER_MAX_HEADER_BYTES=1048576 # Logging - Optional HVS_LOG_MAX_LENGTH=300 HVS_ENABLE_CONSOLE_LOG=false # Flavor Signing Configuration - Optional FLAVOR_SIGNING_KEY_FILE=/etc/hvs/trusted-keys/flavor-signing.key FLAVOR_SIGNING_CERT_FILE=/etc/hvs/certs/trustedca/flavor-signing.pem FLAVOR_SIGNING_COMMON_NAME=HVS Flavor Signing Certificate # SAML Configuration - Optional SAML_KEY_FILE=/etc/hvs/trusted-keys/saml.key SAML_CERT_FILE=/etc/hvs/certs/trustedca/saml-cert.pem SAML_COMMON_NAME=HVS SAML Certificate # Endorsement CA Configuration - Optional ENDORSEMENT_CA_KEY_FILE=/etc/hvs/trusted-keys/endorsement-ca.key ENDORSEMENT_CA_CERT_FILE=/etc/hvs/certs/trustedca/EndorsementCA.pem ENDORSEMENT_CA_COMMON_NAME=HVS Endorsement Certificate ENDORSEMENT_CA_ISSUER=intel-secl ENDORSEMENT_CA_VALIDITY_YEARS=5 # Privacy CA Configuration - Optional PRIVACY_CA_KEY_FILE=/etc/hvs/trusted-keys/privacy-ca.key PRIVACY_CA_CERT_FILE=/etc/hvs/certs/trustedca/privacy-ca-cert.pem PRIVACY_CA_COMMON_NAME=HVS Privacy Certificate PRIVACY_CA_ISSUER=intel-secl PRIVACY_CA_VALIDITY_YEARS=5 # Asset Tag Configuration - Optional TAG_CA_KEY_FILE=/etc/hvs/trusted-keys/tag-ca.key TAG_CA_CERT_FILE=/etc/hvs/certs/trustedca/tag-ca-cert.pem TAG_CA_COMMON_NAME=HVS Tag Certificate TAG_CA_ISSUER=intel-secl TAG_CA_VALIDITY_YEARS=5 # Certificate Revocation Checks - Optional ENABLE_EKCERT_REVOKE_CHECK=false # default=false - if true, revocation checks will be performed on EK certs # proxy settings (if applicable) must be provided - see next stion Note on Certificate Revocation Checks for TPM EK Certs The ENABLE_EKCERT_REVOKE_CHECK setting has been added to toggle revocation checks for Endorsement Key Certs by Verification Service. The revocation check will be performed during the course of the AIK provisioning flow on the Trust Agent ( tagent setup provision-aik ). At this stage, there are 3 possible outcomes: No certs in the chain are found to be revoked: VS responds with a HTTP 200 response code A certificate is found to be revoked: VS responds with a HTTP 400 response code The revocation check failed due to a connection error: VS responds with a HTTP 500 response code If proxy settings are required to source CRL from external CAs, these can be added to the Verification Service service systemd via a drop-in config at /etc/systemd/system/hvs.service.d/proxy.conf or directly to /opt/hvs/hvs.service under the service section like so: [Service] Environment=\"https_proxy=<proxy server url>\" Run systemctl daemon-reload and restart the service after making this change. Configuration Options The Verification Service configuration is stored in the file /etc/hvs/config.yml : tls : cert-file : /etc/hvs/tls-cert.pem key-file : /etc/hvs/tls.key common-name : HVS TLS Certificate san-list : 127.0.0.1,localhost saml : common : cert-file : /etc/hvs/certs/trustedca/saml-cert.pem key-file : /etc/hvs/trusted-keys/saml.key common-name : HVS SAML Certificate issuer : AttestationService validity-days : 1 flavor-signing : cert-file : /etc/hvs/certs/trustedca/flavor-signing.pem key-file : /etc/hvs/trusted-keys/flavor-signing.key common-name : HVS Flavor Signing Certificate privacy-ca : cert-file : /etc/hvs/certs/trustedca/privacy-ca/privacy-ca-cert.pem key-file : /etc/hvs/trusted-keys/privacy-ca.key common-name : HVS Privacy Certificate issuer : intel-secl validity-years : 5 endorsement-ca : cert-file : /etc/hvs/certs/endorsement/EndorsementCA.pem key-file : /etc/hvs/trusted-keys/endorsement-ca.key common-name : HVS Endorsement Certificate issuer : intel-secl validity-years : 5 tag-ca : cert-file : /etc/hvs/certs/trustedca/tag-ca-cert.pem key-file : /etc/hvs/trusted-keys/tag-ca.key common-name : HVS Tag Certificate issuer : intel-secl validity-years : 5 aik-certificate-validity-years : 5 server : port : 8898 read-timeout : 30s read-header-timeout : 10s write-timeout : 30s idle-timeout : 10s max-header-bytes : 1048576 log : max-length : 30000 enable-stdout : true level : TRACE db : vendor : postgres host : localhost port : \"5432\" name : hvs_db username : root password : password ssl-mode : allow ssl-cert : /etc/hvs/hvsdbsslcert.pem conn-retry-attempts : 5 conn-retry-time : 1 hrrs : refresh-period : 2m0s refresh-look-ahead : 5m0s fvs : number-of-verifiers : 20 number-of-data-fetchers : 20 skip-flavor-signature-verification : true enable-ekcert-revoke-check : false Command-Line Options The Verification Service supports several command-line commands that can be executed only as the Root user: Syntax: hvs <command> Help hvs help Displays the list of available CLI commands. Start hvs start Starts the services. Stop hvs stop Stops the services. Status hvs status Reports whether the service is currently running. Uninstall hvs uninstall Uninstalls the service, including the deletion of all files and folders. Database content is not removed. See section 14.1 for additional details. Version hvs version Reports the version of the service. Erase-data hvs erase-data Deletes all non-user information from the database. All data in the following tables will be deleted; the database schema will be preserved: flavor_host flavor_flavorgroup flavorgroup_host queue report host_status flavorgroup host flavor host_credential tag_certificate audit_log_entry tls_policy Setup Usage of hvs setup : hvs setup <task> [--help] [--force] [-f <answer-file>] --help show help message for setup task --force existing configuration will be overwritten if this flag is set -f|--file <answer-file> the answer file with required arguments Available Tasks for setup : all Runs all setup tasks database Setup hvs database create-default-flavorgroup Create default flavor groups in database create-dek Create data encryption key for HVS download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls download-cert-saml Download CA certificate from CMS for saml download-cert-flavor-signing Download CA certificate from CMS for flavor signing create-endorsement-ca Generate self-signed endorsement certificate create-privacy-ca Generate self-signed privacy certificate create-tag-ca Generate self-signed tag certificate update-service-config Sets or Updates the Service configuration Directory Layout The Host Verification Service installs by default to the following folders: /etc/hvs/ This directory contains the config.yml configuration file, the database connection ssl cerificate, and the webservice TLS certificate. /etc/hvs/ \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 endorsement \u2502 \u2502 \u251c\u2500\u2500 EndorsementCA-external.pem \u2502 \u2502 \u2514\u2500\u2500 EndorsementCA.pem \u2502 \u251c\u2500\u2500 trustedca \u2502 \u2502 \u251c\u2500\u2500 flavor-signing.pem \u2502 \u2502 \u251c\u2500\u2500 privacy-ca \u2502 \u2502 \u2502 \u2514\u2500\u2500 privacy-ca-cert.pem \u2502 \u2502 \u251c\u2500\u2500 root \u2502 \u2502 \u2502 \u251c\u2500\u2500 58f6bcfcd.pem \u2502 \u2502 \u2502 \u251c\u2500\u2500 vmware-cert1.pem \u2502 \u2502 \u2502 \u2514\u2500\u2500 vmware-cert2.pem \u2502 \u2502 \u251c\u2500\u2500 saml-cert.pem \u2502 \u2502 \u2514\u2500\u2500 tag-ca-cert.pem \u2502 \u2514\u2500\u2500 trustedjwt \u2502 \u2514\u2500\u2500 f29aa4ab3.pem \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 hvsdbsslcert.pem \u251c\u2500\u2500 tls-cert.pem \u251c\u2500\u2500 tls.key \u2514\u2500\u2500 trusted-keys \u200b \u251c\u2500\u2500 endorsement-ca.key \u200b \u251c\u2500\u2500 flavor-signing.key \u200b \u251c\u2500\u2500 privacy-ca.key \u200b \u251c\u2500\u2500 saml.key \u200b \u2514\u2500\u2500 tag-ca.key","title":"Verification Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#verification-service","text":"","title":"Verification Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#installation-answer-file-options","text":"# Authentication URL and service account credentials - mandatory AAS_API_URL=https://isecl-aas:8444/aas/v1 HVS_SERVICE_USERNAME=HVS_service HVS_SERVICE_PASSWORD=password # CMS URL and CMS webserivce TLS hash for server verification - mandatory CMS_BASE_URL=https://isecl-cms:8445/cms/v1 CMS_TLS_CERT_SHA384=digest # Installation admin bearer token for CSR approval request to CMS - mandatory BEARER_TOKEN=eyJhbGciOiJSUzM4NCIsImtpZCI6ImE\u2026 # Skip setup - optional HVS_NOSETUP=false #default=false # Logging options - optional HVS_LOGLEVEL=info # options: critical|error|warning|info|debug|trace, default='info' HVS_LOG_MAX_LENGTH=300 # default=300 HVS_ENABLE_CONSOLE_LOG=false # default=false # HRRS configuration - optional HRRS_REFRESH_PERIOD=2m0s # default=2m0s HRRS_REFRESH_LOOK_AHEAD=5m0s # default=5m0s # FVS configuration - optional FVS_NUMBER_OF_VERIFIERS=20 # default=20 FVS_NUMBER_OF_DATA_FETCHERS=20 # default=20 FVS_SKIP_FLAVOR_SIGNATURE_VERIFICATION=false # default=false # In case of trusted flavor storage, flavor signature verification can be skipped # using following flag - optional SKIP_FLAVOR_SIGNATURE_VERIFICATION=false # default=false # TLS certificate configuration - optional TLS_COMMON_NAME=\"HVS TLS Certificate\" # default=\"HVS TLS Certificate\" TLS_SAN_LIST=127.0.0.1,localhost # default=127.0.0.1,localhost # Server configuration - optional HVS_PORT=8443 # default=8443 HVS_SERVER_READ_TIMEOUT=30s # default=30s HVS_SERVER_READ_HEADER_TIMEOUT=10s # default=10s HVS_SERVER_WRITE_TIMEOUT=10s # default=10s HVS_SERVER_IDLE_TIMEOUT=10s # default=10s HVS_SERVER_MAX_HEADER_BYTES=1048576 # default=1048576 # Database - mandatory HVS_DB_USERNAME=runner HVS_DB_PASSWORD=test HVS_DB_SSLCERTSRC=/tmp/dbcert.pem # This doesn't need to be specified if HVS_DB_SSLCERT is given # Database - optional HVS_DB_HOSTNAME=localhost # default=localhost HVS_DB_NAME=hvs-pg-db # default=hvs-pg-db HVS_DB_PORT=5432 # default=5432 HVS_DB_SSLMODE=verify-full # default=verify-full ;other options are like allow, prefer, require, verify-ca HVS_DB_SSLCERT=/etc/hvs/hvsdbcert.pem # default=/etc/hvs/hvsdbcert.pem # Webservice configuration - Optional HVS_PORT=8443 HVS_SERVER_READ_TIMEOUT=30s HVS_SERVER_READ_HEADER_TIMEOUT=10s HVS_SERVER_WRITE_TIMEOUT=10s HVS_SERVER_IDLE_TIMEOUT=10s HVS_SERVER_MAX_HEADER_BYTES=1048576 # Logging - Optional HVS_LOG_MAX_LENGTH=300 HVS_ENABLE_CONSOLE_LOG=false # Flavor Signing Configuration - Optional FLAVOR_SIGNING_KEY_FILE=/etc/hvs/trusted-keys/flavor-signing.key FLAVOR_SIGNING_CERT_FILE=/etc/hvs/certs/trustedca/flavor-signing.pem FLAVOR_SIGNING_COMMON_NAME=HVS Flavor Signing Certificate # SAML Configuration - Optional SAML_KEY_FILE=/etc/hvs/trusted-keys/saml.key SAML_CERT_FILE=/etc/hvs/certs/trustedca/saml-cert.pem SAML_COMMON_NAME=HVS SAML Certificate # Endorsement CA Configuration - Optional ENDORSEMENT_CA_KEY_FILE=/etc/hvs/trusted-keys/endorsement-ca.key ENDORSEMENT_CA_CERT_FILE=/etc/hvs/certs/trustedca/EndorsementCA.pem ENDORSEMENT_CA_COMMON_NAME=HVS Endorsement Certificate ENDORSEMENT_CA_ISSUER=intel-secl ENDORSEMENT_CA_VALIDITY_YEARS=5 # Privacy CA Configuration - Optional PRIVACY_CA_KEY_FILE=/etc/hvs/trusted-keys/privacy-ca.key PRIVACY_CA_CERT_FILE=/etc/hvs/certs/trustedca/privacy-ca-cert.pem PRIVACY_CA_COMMON_NAME=HVS Privacy Certificate PRIVACY_CA_ISSUER=intel-secl PRIVACY_CA_VALIDITY_YEARS=5 # Asset Tag Configuration - Optional TAG_CA_KEY_FILE=/etc/hvs/trusted-keys/tag-ca.key TAG_CA_CERT_FILE=/etc/hvs/certs/trustedca/tag-ca-cert.pem TAG_CA_COMMON_NAME=HVS Tag Certificate TAG_CA_ISSUER=intel-secl TAG_CA_VALIDITY_YEARS=5 # Certificate Revocation Checks - Optional ENABLE_EKCERT_REVOKE_CHECK=false # default=false - if true, revocation checks will be performed on EK certs # proxy settings (if applicable) must be provided - see next stion","title":"Installation Answer File Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#note-on-certificate-revocation-checks-for-tpm-ek-certs","text":"The ENABLE_EKCERT_REVOKE_CHECK setting has been added to toggle revocation checks for Endorsement Key Certs by Verification Service. The revocation check will be performed during the course of the AIK provisioning flow on the Trust Agent ( tagent setup provision-aik ). At this stage, there are 3 possible outcomes: No certs in the chain are found to be revoked: VS responds with a HTTP 200 response code A certificate is found to be revoked: VS responds with a HTTP 400 response code The revocation check failed due to a connection error: VS responds with a HTTP 500 response code If proxy settings are required to source CRL from external CAs, these can be added to the Verification Service service systemd via a drop-in config at /etc/systemd/system/hvs.service.d/proxy.conf or directly to /opt/hvs/hvs.service under the service section like so: [Service] Environment=\"https_proxy=<proxy server url>\" Run systemctl daemon-reload and restart the service after making this change.","title":"Note on Certificate Revocation Checks for TPM EK Certs"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#configuration-options","text":"The Verification Service configuration is stored in the file /etc/hvs/config.yml : tls : cert-file : /etc/hvs/tls-cert.pem key-file : /etc/hvs/tls.key common-name : HVS TLS Certificate san-list : 127.0.0.1,localhost saml : common : cert-file : /etc/hvs/certs/trustedca/saml-cert.pem key-file : /etc/hvs/trusted-keys/saml.key common-name : HVS SAML Certificate issuer : AttestationService validity-days : 1 flavor-signing : cert-file : /etc/hvs/certs/trustedca/flavor-signing.pem key-file : /etc/hvs/trusted-keys/flavor-signing.key common-name : HVS Flavor Signing Certificate privacy-ca : cert-file : /etc/hvs/certs/trustedca/privacy-ca/privacy-ca-cert.pem key-file : /etc/hvs/trusted-keys/privacy-ca.key common-name : HVS Privacy Certificate issuer : intel-secl validity-years : 5 endorsement-ca : cert-file : /etc/hvs/certs/endorsement/EndorsementCA.pem key-file : /etc/hvs/trusted-keys/endorsement-ca.key common-name : HVS Endorsement Certificate issuer : intel-secl validity-years : 5 tag-ca : cert-file : /etc/hvs/certs/trustedca/tag-ca-cert.pem key-file : /etc/hvs/trusted-keys/tag-ca.key common-name : HVS Tag Certificate issuer : intel-secl validity-years : 5 aik-certificate-validity-years : 5 server : port : 8898 read-timeout : 30s read-header-timeout : 10s write-timeout : 30s idle-timeout : 10s max-header-bytes : 1048576 log : max-length : 30000 enable-stdout : true level : TRACE db : vendor : postgres host : localhost port : \"5432\" name : hvs_db username : root password : password ssl-mode : allow ssl-cert : /etc/hvs/hvsdbsslcert.pem conn-retry-attempts : 5 conn-retry-time : 1 hrrs : refresh-period : 2m0s refresh-look-ahead : 5m0s fvs : number-of-verifiers : 20 number-of-data-fetchers : 20 skip-flavor-signature-verification : true enable-ekcert-revoke-check : false","title":"Configuration Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#command-line-options","text":"The Verification Service supports several command-line commands that can be executed only as the Root user: Syntax: hvs <command>","title":"Command-Line Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#help","text":"hvs help Displays the list of available CLI commands.","title":"Help"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#start","text":"hvs start Starts the services.","title":"Start"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#stop","text":"hvs stop Stops the services.","title":"Stop"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#status","text":"hvs status Reports whether the service is currently running.","title":"Status"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#uninstall","text":"hvs uninstall Uninstalls the service, including the deletion of all files and folders. Database content is not removed. See section 14.1 for additional details.","title":"Uninstall"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#version","text":"hvs version Reports the version of the service.","title":"Version"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#erase-data","text":"hvs erase-data Deletes all non-user information from the database. All data in the following tables will be deleted; the database schema will be preserved: flavor_host flavor_flavorgroup flavorgroup_host queue report host_status flavorgroup host flavor host_credential tag_certificate audit_log_entry tls_policy","title":"Erase-data"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#setup","text":"Usage of hvs setup : hvs setup <task> [--help] [--force] [-f <answer-file>] --help show help message for setup task --force existing configuration will be overwritten if this flag is set -f|--file <answer-file> the answer file with required arguments Available Tasks for setup : all Runs all setup tasks database Setup hvs database create-default-flavorgroup Create default flavor groups in database create-dek Create data encryption key for HVS download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls download-cert-saml Download CA certificate from CMS for saml download-cert-flavor-signing Download CA certificate from CMS for flavor signing create-endorsement-ca Generate self-signed endorsement certificate create-privacy-ca Generate self-signed privacy certificate create-tag-ca Generate self-signed tag certificate update-service-config Sets or Updates the Service configuration","title":"Setup"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings1/#directory-layout","text":"The Host Verification Service installs by default to the following folders: /etc/hvs/ This directory contains the config.yml configuration file, the database connection ssl cerificate, and the webservice TLS certificate. /etc/hvs/ \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 endorsement \u2502 \u2502 \u251c\u2500\u2500 EndorsementCA-external.pem \u2502 \u2502 \u2514\u2500\u2500 EndorsementCA.pem \u2502 \u251c\u2500\u2500 trustedca \u2502 \u2502 \u251c\u2500\u2500 flavor-signing.pem \u2502 \u2502 \u251c\u2500\u2500 privacy-ca \u2502 \u2502 \u2502 \u2514\u2500\u2500 privacy-ca-cert.pem \u2502 \u2502 \u251c\u2500\u2500 root \u2502 \u2502 \u2502 \u251c\u2500\u2500 58f6bcfcd.pem \u2502 \u2502 \u2502 \u251c\u2500\u2500 vmware-cert1.pem \u2502 \u2502 \u2502 \u2514\u2500\u2500 vmware-cert2.pem \u2502 \u2502 \u251c\u2500\u2500 saml-cert.pem \u2502 \u2502 \u2514\u2500\u2500 tag-ca-cert.pem \u2502 \u2514\u2500\u2500 trustedjwt \u2502 \u2514\u2500\u2500 f29aa4ab3.pem \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 hvsdbsslcert.pem \u251c\u2500\u2500 tls-cert.pem \u251c\u2500\u2500 tls.key \u2514\u2500\u2500 trusted-keys \u200b \u251c\u2500\u2500 endorsement-ca.key \u200b \u251c\u2500\u2500 flavor-signing.key \u200b \u251c\u2500\u2500 privacy-ca.key \u200b \u251c\u2500\u2500 saml.key \u200b \u2514\u2500\u2500 tag-ca.key","title":"Directory Layout"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings2/","text":"Trust Agent Installation Answer File Options Key Description Sample Value AAS_API_URL API URL for Authentication Authorization Service (AAS). AAS_API_URL=https://{host}:{port}/aas/v1 AUTOMATIC_PULL_MANIFEST Instructs the installer to automatically pull application-manifests from HVS similar to tagent setup get-configured-manifest AUTOMATIC_PULL_MANIFEST=Y AUTOMATIC_REGISTRATION Instructs the installer to automatically register the host with HVS similar to running tagent setup create-host and tagent setup create-host-unique-flavor. AUTOMATIC_REGISTRATION=Y BEARER_TOKEN JWT from AAS that contains \"install\" permissions needed to access ISecL services during provisioning and registration BEARER_TOKEN=eyJhbGciOiJSUzM4NCIsjdkMTdiNmUz... CMS_BASE_URL API URL for Certificate Management Service (CMS). CMS_BASE_URL=https://{host}:{port}/cms/v1 CMS_TLS_CERT_SHA384 SHA384 Hash sum for verifying the CMS TLS certificate. CMS_TLS_CERT_SHA384=bd8ebf5091289958b5765da4... HVS_API_URL The url used during setup to request information from HVS. HVS_API_URL=https://{host}:{port}/hvs/v2 PROVISION_ATTESTATION When present, enables/disables whether tagent setup is called during installation. If trustagent.env is not present, the value defaults to no ('N'). PROVISION_ATTESTATION=Y SAN_LIST CSV list that sets the value for SAN list in the TA TLS certificate. Defaults to 127.0.0.1. SAN_LIST=10.123.100.1,201.102.10.22,mya.example.com TA_TLS_CERT_CN Sets the value for Common Name in the TA TLS certificate. Defaults to CN=trustagent. TA_TLS_CERT_CN=Acme Trust Agent 007 TPM_OWNER_SECRET Default is null. Can be any string of characters. Use the \"hex:\" prefix to force hex characters rather than a string. hex:0164837f83...\" TPM_OWNER_SECRET=625d6... Starting in Intel SecL-DC 4.0, this value will now default to null unless a secret is specified. Using a null TPM ownership secret is recommended. The Trust Agent now only requires TPM ownership during Trust Agent provisioning. TPM_QUOTE_IPV4 When enabled (=y), uses the local system's ip address as a salt when processing a quote nonce. This field must align with the configuration of HVS. TPM_QUOTE_IPV4=no TA_SERVER_READ_TIMEOUT Sets tagent server ReadTimeout. Defaults to 30 seconds. TA_SERVER_READ_TIMEOUT=30 TA_SERVER_READ_HEADER_TIMEOUT Sets tagent server ReadHeaderTimeout. Defaults to 30 seconds. TA_SERVER_READ_HEADER_TIMEOUT=10 TA_SERVER_WRITE_TIMEOUT Sets tagent server WriteTimeout. Defaults to 10 seconds. TA_SERVER_WRITE_TIMEOUT=10 TA_SERVER_IDLE_TIMEOUT Sets tagent server IdleTimeout. Defaults to 10 seconds. TA_SERVER_IDLE_TIMEOUT=10 TA_SERVER_MAX_HEADER_BYTES Sets tagent server MaxHeaderBytes. Defaults to 1MB(1048576) TA_SERVER_MAX_HEADER_BYTES=1048576 TA_ENABLE_CONSOLE_LOG When set true, tagent logs are redirected to stdout. Defaults to false TA_ENABLE_CONSOLE_LOG=true TRUSTAGENT_LOG_LEVEL The logging level to be saved in config.yml during installation (\"trace\", \"debug\", \"info\"). TRUSTAGENT_LOG_LEVEL=debug TRUSTAGENT_PORT The port on which the trust-agent service will listen. TRUSTAGENT_PORT=10433 Configuration Options The Trust Agent configuration settings are managed in /opt/trustagent/configuration/config.yml Setting Description tpmquoteipv4: true When enabled, the Trust Agent will perform an additional hash of the nonce using the bytes from the Trust Agent server IP when returning TPM quotes. This should always be set to True. logging: loglevel: info Defines the Trust Agent logging level logenablestdout: false If set to True, the Trust Agent will log to stdout. By default this is False and the logs are sent to /var/log/trustagent/trustagent.log logentrymaxlength: 300 Defines the maximum length of a single log entry webservice: port: 1443 Defines the port on which the Trust Agent API server will listen readtimeout: 30s readheadertimeout: 10s writetimeout: 10s idletimeout: 10s maxheaderbytes: 1048576 hvs: url: https://0.0.0.0:8443/hvs/v2 Defines the baseurl for the Verification Service tpm: aas: baseurl: https://0.0.0.0:8444/aas/v1/ Defines the base URL for the AAS cms: baseurl: https://0.0.0.0:8445/cms/v1 Defines the base URL for the CMS tlscertdigest: 330086b3...ae477c8502 Defines the SHA383 hash of the CMS TLS certificate tls: certsan: 10.1.2.3,server.domain.com,localhost Comma-separated list of hostnames and IP addresses for the Trust Agent. Used in the Agent TLS certificate. certcn: Trust Agent TLS Certificate Common Name for the Trust Agent TLS certificate Command-Line Options Usage: tagent <command> [arguments] Available Commands: help|-h|-help Show this help message. setup [all] [task] Run setup task. uninstall Uninstall trust agent. version Print build version info. start Start the trust agent service. stop Stop the trust agent service. status Get the status of the trust agent service. fetch-ekcert-with-issuer Print Tpm Endorsement Certificate in Base64 encoded string along with issuer Setup command usage : tagent setup [cmd] [-f <env-file>] Available Tasks for 'setup', all commands support env file flag all - Runs all setup tasks to provision the trust agent. This command can be omitted with running only tagent setup Required environment variables [in env/trustagent.env] : - AAS_API_URL=<url> : AAS API URL - CMS_BASE_URL=<url> : CMS API URL - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that TA is communicating with the right CMS instance - BEARER_TOKEN=<token> : for authenticating with CMS and VS - HVS_URL=<url> : VS API URL Optional Environment variables : - TA_ENABLE_CONSOLE_LOG=<true/false> : When 'true', logs are redirected to stdout. Defaults to false. - TA_SERVER_IDLE_TIMEOUT=<t seconds> : Sets the trust agent service's idle timeout. Defaults to 10 seconds. - TA_SERVER_MAX_HEADER_BYTES=<n bytes> : Sets trust agent service's maximum header bytes. Defaults to 1MB. - TA_SERVER_READ_TIMEOUT=<t seconds> : Sets trust agent service's read timeout. Defaults to 30 seconds. - TA_SERVER_READ_HEADER_TIMEOUT=<t seconds> : Sets trust agent service's read header timeout. Defaults to 30 seconds. - TA_SERVER_WRITE_TIMEOUT=<t seconds> : Sets trust agent service's write timeout. Defaults to 10 seconds. - SAN_LIST=<host1,host2.acme.com,...> : CSV list that sets the value for SAN list in the TA TLS certificate. Defaults to \"127.0.0.1,localhost\". - TA_TLS_CERT_CN=<Common Name> : Sets the value for Common Name in the TA TLS certificate. Defaults to \"Trust Agent TLS Certificate\". - TPM_OWNER_SECRET=<40 byte hex> : When provided, setup uses the 40 character hex string for the TPM owner password. Auto-generated when not provided. - TRUSTAGENT_LOG_LEVEL=<trace|debug|info|error> : Sets the verbosity level of logging. Defaults to 'info'. - TRUSTAGENT_PORT=<portnum> : The port on which the trust agent service will listen. Defaults to 1443 download-ca-cert - Fetches the latest CMS Root CA Certificates, overwriting existing files. Required environment variables : - CMS_BASE_URL=<url> : CMS API URL - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that TA is communicating with the right CMS instance download-cert - Fetches a signed TLS Certificate from CMS, overwriting existing files. Required environment variables : - CMS_BASE_URL=<url> : CMS API URL - BEARER_TOKEN=<token> : for authenticating with CMS and VS Optional Environment variables : - SAN_LIST=<host1,host2.acme.com,...> : CSV list that sets the value for SAN list in the TA TLS certificate. Defaults to \"127.0.0.1,localhost\". - TA_TLS_CERT_CN=<Common Name> : Sets the value for Common Name in the TA TLS certificate. Defaults to \"Trust Agent TLS Certificate\". update-certificates - Runs 'download-ca-cert' and 'download-cert' Required environment variables : - CMS_BASE_URL=<url> : CMS API URL - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that TA is communicating with the right CMS instance - BEARER_TOKEN=<token> : for authenticating with CMS Optional Environment variables : - SAN_LIST=<host1,host2.acme.com,...> : CSV list that sets the value for SAN list in the TA TLS certificate. Defaults to \"127.0.0.1,localhost\". - TA_TLS_CERT_CN=<Common Name> : Sets the value for Common Name in the TA TLS certificate. Defaults to \"Trust Agent TLS Certificate\". provision-attestation - Runs setup tasks associated with HVS/TPM provisioning. Required environment variables : - HVS_URL=<url> : VS API URL - BEARER_TOKEN=<token> : for authenticating with VS Optional environment variables : - TPM_OWNER_SECRET=<40 byte hex> : When provided, setup uses the 40 character hex string for the TPM owner password. Auto-generated when not provided. create-host - Registers the trust agent with the verification service. Required environment variables : - HVS_URL=<url> : VS API URL - BEARER_TOKEN=<token> : for authenticating with VS - CURRENT_IP=<ip address of host> : IP or hostname of host with which the host will be registered with HVS Optional environment variables : - TPM_OWNER_SECRET=<40 byte hex> : When provided, setup uses the 40 character hex string for the TPM owner password. Auto-generated when not provided. create-host-unique-flavor - Populates the verification service with the host unique flavor Required environment variables : - HVS_URL=<url> : VS API URL - BEARER_TOKEN=<token> : for authenticating with VS - CURRENT_IP=<ip address of host> : Used to associate the flavor with the host get-configured-manifest - Uses environment variables to pull application-integrity manifests from the verification service. Required environment variables : - HVS_URL=<url> : VS API URL - BEARER_TOKEN=<token> : for authenticating with VS - FLAVOR_UUIDS=<uuid1,uuid2,[...]> : CSV list of flavor UUIDs - FLAVOR_LABELS=<flavorlabel1,flavorlabel2,[...]> : CSV list of flavor labels Directory Layout Linux The Linux Trust Agent installs by default to /opt/trustagent , with the following subfolders: Bin Contains executables and scripts. Configuration Contains the config.yml configuration file, as well as certificates and keystores. This includes the AIK public key blob after provitioning. Var Contains information gathered from the platform and SOFTWARE Flavor manifests. All files with the name manifest_*.xml will be parsed to define measurements during boot. Generally these should be automatically provisioned from the Verification Service when creating/deploying SOFTWARE Flavors.","title":"Trust Agent"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings2/#trust-agent","text":"","title":"Trust Agent"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings2/#installation-answer-file-options","text":"Key Description Sample Value AAS_API_URL API URL for Authentication Authorization Service (AAS). AAS_API_URL=https://{host}:{port}/aas/v1 AUTOMATIC_PULL_MANIFEST Instructs the installer to automatically pull application-manifests from HVS similar to tagent setup get-configured-manifest AUTOMATIC_PULL_MANIFEST=Y AUTOMATIC_REGISTRATION Instructs the installer to automatically register the host with HVS similar to running tagent setup create-host and tagent setup create-host-unique-flavor. AUTOMATIC_REGISTRATION=Y BEARER_TOKEN JWT from AAS that contains \"install\" permissions needed to access ISecL services during provisioning and registration BEARER_TOKEN=eyJhbGciOiJSUzM4NCIsjdkMTdiNmUz... CMS_BASE_URL API URL for Certificate Management Service (CMS). CMS_BASE_URL=https://{host}:{port}/cms/v1 CMS_TLS_CERT_SHA384 SHA384 Hash sum for verifying the CMS TLS certificate. CMS_TLS_CERT_SHA384=bd8ebf5091289958b5765da4... HVS_API_URL The url used during setup to request information from HVS. HVS_API_URL=https://{host}:{port}/hvs/v2 PROVISION_ATTESTATION When present, enables/disables whether tagent setup is called during installation. If trustagent.env is not present, the value defaults to no ('N'). PROVISION_ATTESTATION=Y SAN_LIST CSV list that sets the value for SAN list in the TA TLS certificate. Defaults to 127.0.0.1. SAN_LIST=10.123.100.1,201.102.10.22,mya.example.com TA_TLS_CERT_CN Sets the value for Common Name in the TA TLS certificate. Defaults to CN=trustagent. TA_TLS_CERT_CN=Acme Trust Agent 007 TPM_OWNER_SECRET Default is null. Can be any string of characters. Use the \"hex:\" prefix to force hex characters rather than a string. hex:0164837f83...\" TPM_OWNER_SECRET=625d6... Starting in Intel SecL-DC 4.0, this value will now default to null unless a secret is specified. Using a null TPM ownership secret is recommended. The Trust Agent now only requires TPM ownership during Trust Agent provisioning. TPM_QUOTE_IPV4 When enabled (=y), uses the local system's ip address as a salt when processing a quote nonce. This field must align with the configuration of HVS. TPM_QUOTE_IPV4=no TA_SERVER_READ_TIMEOUT Sets tagent server ReadTimeout. Defaults to 30 seconds. TA_SERVER_READ_TIMEOUT=30 TA_SERVER_READ_HEADER_TIMEOUT Sets tagent server ReadHeaderTimeout. Defaults to 30 seconds. TA_SERVER_READ_HEADER_TIMEOUT=10 TA_SERVER_WRITE_TIMEOUT Sets tagent server WriteTimeout. Defaults to 10 seconds. TA_SERVER_WRITE_TIMEOUT=10 TA_SERVER_IDLE_TIMEOUT Sets tagent server IdleTimeout. Defaults to 10 seconds. TA_SERVER_IDLE_TIMEOUT=10 TA_SERVER_MAX_HEADER_BYTES Sets tagent server MaxHeaderBytes. Defaults to 1MB(1048576) TA_SERVER_MAX_HEADER_BYTES=1048576 TA_ENABLE_CONSOLE_LOG When set true, tagent logs are redirected to stdout. Defaults to false TA_ENABLE_CONSOLE_LOG=true TRUSTAGENT_LOG_LEVEL The logging level to be saved in config.yml during installation (\"trace\", \"debug\", \"info\"). TRUSTAGENT_LOG_LEVEL=debug TRUSTAGENT_PORT The port on which the trust-agent service will listen. TRUSTAGENT_PORT=10433","title":"Installation Answer File Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings2/#configuration-options","text":"The Trust Agent configuration settings are managed in /opt/trustagent/configuration/config.yml Setting Description tpmquoteipv4: true When enabled, the Trust Agent will perform an additional hash of the nonce using the bytes from the Trust Agent server IP when returning TPM quotes. This should always be set to True. logging: loglevel: info Defines the Trust Agent logging level logenablestdout: false If set to True, the Trust Agent will log to stdout. By default this is False and the logs are sent to /var/log/trustagent/trustagent.log logentrymaxlength: 300 Defines the maximum length of a single log entry webservice: port: 1443 Defines the port on which the Trust Agent API server will listen readtimeout: 30s readheadertimeout: 10s writetimeout: 10s idletimeout: 10s maxheaderbytes: 1048576 hvs: url: https://0.0.0.0:8443/hvs/v2 Defines the baseurl for the Verification Service tpm: aas: baseurl: https://0.0.0.0:8444/aas/v1/ Defines the base URL for the AAS cms: baseurl: https://0.0.0.0:8445/cms/v1 Defines the base URL for the CMS tlscertdigest: 330086b3...ae477c8502 Defines the SHA383 hash of the CMS TLS certificate tls: certsan: 10.1.2.3,server.domain.com,localhost Comma-separated list of hostnames and IP addresses for the Trust Agent. Used in the Agent TLS certificate. certcn: Trust Agent TLS Certificate Common Name for the Trust Agent TLS certificate","title":"Configuration Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings2/#command-line-options","text":"Usage: tagent <command> [arguments] Available Commands: help|-h|-help Show this help message. setup [all] [task] Run setup task. uninstall Uninstall trust agent. version Print build version info. start Start the trust agent service. stop Stop the trust agent service. status Get the status of the trust agent service. fetch-ekcert-with-issuer Print Tpm Endorsement Certificate in Base64 encoded string along with issuer Setup command usage : tagent setup [cmd] [-f <env-file>] Available Tasks for 'setup', all commands support env file flag all - Runs all setup tasks to provision the trust agent. This command can be omitted with running only tagent setup Required environment variables [in env/trustagent.env] : - AAS_API_URL=<url> : AAS API URL - CMS_BASE_URL=<url> : CMS API URL - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that TA is communicating with the right CMS instance - BEARER_TOKEN=<token> : for authenticating with CMS and VS - HVS_URL=<url> : VS API URL Optional Environment variables : - TA_ENABLE_CONSOLE_LOG=<true/false> : When 'true', logs are redirected to stdout. Defaults to false. - TA_SERVER_IDLE_TIMEOUT=<t seconds> : Sets the trust agent service's idle timeout. Defaults to 10 seconds. - TA_SERVER_MAX_HEADER_BYTES=<n bytes> : Sets trust agent service's maximum header bytes. Defaults to 1MB. - TA_SERVER_READ_TIMEOUT=<t seconds> : Sets trust agent service's read timeout. Defaults to 30 seconds. - TA_SERVER_READ_HEADER_TIMEOUT=<t seconds> : Sets trust agent service's read header timeout. Defaults to 30 seconds. - TA_SERVER_WRITE_TIMEOUT=<t seconds> : Sets trust agent service's write timeout. Defaults to 10 seconds. - SAN_LIST=<host1,host2.acme.com,...> : CSV list that sets the value for SAN list in the TA TLS certificate. Defaults to \"127.0.0.1,localhost\". - TA_TLS_CERT_CN=<Common Name> : Sets the value for Common Name in the TA TLS certificate. Defaults to \"Trust Agent TLS Certificate\". - TPM_OWNER_SECRET=<40 byte hex> : When provided, setup uses the 40 character hex string for the TPM owner password. Auto-generated when not provided. - TRUSTAGENT_LOG_LEVEL=<trace|debug|info|error> : Sets the verbosity level of logging. Defaults to 'info'. - TRUSTAGENT_PORT=<portnum> : The port on which the trust agent service will listen. Defaults to 1443 download-ca-cert - Fetches the latest CMS Root CA Certificates, overwriting existing files. Required environment variables : - CMS_BASE_URL=<url> : CMS API URL - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that TA is communicating with the right CMS instance download-cert - Fetches a signed TLS Certificate from CMS, overwriting existing files. Required environment variables : - CMS_BASE_URL=<url> : CMS API URL - BEARER_TOKEN=<token> : for authenticating with CMS and VS Optional Environment variables : - SAN_LIST=<host1,host2.acme.com,...> : CSV list that sets the value for SAN list in the TA TLS certificate. Defaults to \"127.0.0.1,localhost\". - TA_TLS_CERT_CN=<Common Name> : Sets the value for Common Name in the TA TLS certificate. Defaults to \"Trust Agent TLS Certificate\". update-certificates - Runs 'download-ca-cert' and 'download-cert' Required environment variables : - CMS_BASE_URL=<url> : CMS API URL - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that TA is communicating with the right CMS instance - BEARER_TOKEN=<token> : for authenticating with CMS Optional Environment variables : - SAN_LIST=<host1,host2.acme.com,...> : CSV list that sets the value for SAN list in the TA TLS certificate. Defaults to \"127.0.0.1,localhost\". - TA_TLS_CERT_CN=<Common Name> : Sets the value for Common Name in the TA TLS certificate. Defaults to \"Trust Agent TLS Certificate\". provision-attestation - Runs setup tasks associated with HVS/TPM provisioning. Required environment variables : - HVS_URL=<url> : VS API URL - BEARER_TOKEN=<token> : for authenticating with VS Optional environment variables : - TPM_OWNER_SECRET=<40 byte hex> : When provided, setup uses the 40 character hex string for the TPM owner password. Auto-generated when not provided. create-host - Registers the trust agent with the verification service. Required environment variables : - HVS_URL=<url> : VS API URL - BEARER_TOKEN=<token> : for authenticating with VS - CURRENT_IP=<ip address of host> : IP or hostname of host with which the host will be registered with HVS Optional environment variables : - TPM_OWNER_SECRET=<40 byte hex> : When provided, setup uses the 40 character hex string for the TPM owner password. Auto-generated when not provided. create-host-unique-flavor - Populates the verification service with the host unique flavor Required environment variables : - HVS_URL=<url> : VS API URL - BEARER_TOKEN=<token> : for authenticating with VS - CURRENT_IP=<ip address of host> : Used to associate the flavor with the host get-configured-manifest - Uses environment variables to pull application-integrity manifests from the verification service. Required environment variables : - HVS_URL=<url> : VS API URL - BEARER_TOKEN=<token> : for authenticating with VS - FLAVOR_UUIDS=<uuid1,uuid2,[...]> : CSV list of flavor UUIDs - FLAVOR_LABELS=<flavorlabel1,flavorlabel2,[...]> : CSV list of flavor labels","title":"Command-Line Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings2/#directory-layout","text":"","title":"Directory Layout"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings2/#linux","text":"The Linux Trust Agent installs by default to /opt/trustagent , with the following subfolders:","title":"Linux"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings2/#bin","text":"Contains executables and scripts.","title":"Bin"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings2/#configuration","text":"Contains the config.yml configuration file, as well as certificates and keystores. This includes the AIK public key blob after provitioning.","title":"Configuration"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings2/#var","text":"Contains information gathered from the platform and SOFTWARE Flavor manifests. All files with the name manifest_*.xml will be parsed to define measurements during boot. Generally these should be automatically provisioned from the Verification Service when creating/deploying SOFTWARE Flavors.","title":"Var"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/","text":"Integration Hub Installation Answer File # Authentication URL and service account credentials AAS_API_URL=https://isecl-aas:8444/aas/v1 IHUB_SERVICE_USERNAME=<Integration Hub Service User username> IHUB_SERVICE_PASSWORD=<Integration Hub Service User password> # CMS URL and CMS webserivce TLS hash for server verification CMS_BASE_URL=https://isecl-cms:8445/cms/v1 CMS_TLS_CERT_SHA384=<TLS hash> # TLS Configuration TLS_SAN_LIST=127.0.0.1,192.168.1.1,hub.server.com #comma-separated list of IP addresses and hostnames for the Hub to be used in the Subject Alternative Names list in the TLS Certificate # Verification Service URL HVS_BASE_URL=https://isecl-hvs:8443/hvs/v2 ATTESTATION_TYPE=HVS #Integration tenant type. Currently supported values are \"KUBENETES\" or \"OPENSTACK\" TENANT=<KUBERNETES or OPENSTACK> # OpenStack Integration Credentials - required for OpenStack integration only OPENSTACK_AUTH_URL=<OpenStack Keystone URL; typically http://openstack-ip:5000/> OPENSTACK_PLACEMENT_URL=<OpenStack Nova API URL; typically http://openstack-ip:8778/> OPENSTACK_USERNAME=<OpenStack username> OPENSTACK_PASSWORD=<OpenStack password> # Kubernetes Integration Credentials - required for Kubernetes integration only KUBERNETES_URL=https://kubernetes:6443/ KUBERNETES_CRD=custom-isecl KUBERNETES_CERT_FILE=/etc/ihub/apiserver.crt KUBERNETES_TOKEN=eyJhbGciOiJSUzI1NiIsImtpZCI6Ik...... # Installation admin bearer token for CSR approval request to CMS - mandatory BEARER_TOKEN=eyJhbGciOiJSUzM4NCIsImtpZCI6ImE\u2026 #Optional, configures the polling interval at which the Hub retrieves attestations from the HVS POLL_INTERVAL_MINUTES=2 #Optional, runs the installer skipping setup IHUB_NO_SETUP=false #Optional, configures the TLS certificate common name TLS_COMMON_NAME=Integration Hub TLS Certificate #Optional, log configuration LOG_MAX_LENGTH=1500 LOG_LEVEL=Info LOG_ENABLE_STDOUT=true Configuration Options config-file : /etc/ihub/config log : max-length : 1500 enable-stdout : true level : trace ihub : service-username : admin@hub service-password : hubAdminPass poll-interval-minutes : 1 aas : url : https://<aas_ip>:8444/aas/v1 cms : url : https://<cms_ip>:8445/cms/v1/ tls-cert-digest : 8a035e3cdd... attestation-service : attestation-url : https://<hvs_ip>:8443/hvs/v2 attestation-type : HVS end-point : type : KUBERNETES or OPENSTACK url : https://<kubernetes_ip>:6443/ or OpenStack Nova URL crd-name : custom-isecl token : eyJhbGciOiJSUzI... username : OpenStack Username password : OpenStack Password auth-url : OpenStack Authentication URL cert-file : /etc/ihub/apiserver.crt tls : cert-file : /etc/ihub/tls-cert.pem key-file : /etc/ihub/tls-key.pem common-name : Integration Hub TLS Certificate san-list : 127.0.0.1,localhost Command-Line Options Available Commands Help ihub -h | --help Displays the list of available CLI commands. Start ihub start Starts the services. Stop ihub stop Stops the services. Status ihub status Reports whether the service is currently running. Uninstall ihub uninstall [--purge] Uninstalls the service, including the deletion of all files and folders. Database content is not removed. If the --purge option is used, database content will be removed during the uninstallation. Version ihub -v | --version Reports the version of the service. Setup ihub setup <task> [--help] [--force] [-f <answer-file>] Usage of ihub setup : ihub setup <task> [--help] [--force] [-f <answer-file>] --help show help message for setup task --force existing configuration will e overwritten if this flag is set -f|--file <answer-file> the answer file with required arguments Available Tasks for setup : all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls attestation-service-connection Establish Attestation service connection tenant-service-connection Establish Tenant service connection create-signing-key Create signing key for IHUB download-saml-cert Download SAML certificate from Attestation service update-service-config Sets or Updates the Service configuration Directory Layout The ihub installs by default to /etc/ihub. This directory contains the config.yaml configuration file, saml certificate, trusted ca, and the webservice TLS certificate. /etc/ihub/ \u251c\u2500\u2500 apiserver.crt \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 saml \u2502 \u2502 \u2514\u2500\u2500 saml-cert.pem \u2502 \u2514\u2500\u2500 trustedca \u2502 \u2514\u2500\u2500 58f6bcfcd.pem \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 ihub_private_key.pem \u251c\u2500\u2500 ihub_public_key.pem \u251c\u2500\u2500 tls-cert.pem \u2514\u2500\u2500 tls-key.pem Logs /var/logs/ihub","title":"Integration Hub"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#integration-hub","text":"","title":"Integration Hub"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#installation-answer-file","text":"# Authentication URL and service account credentials AAS_API_URL=https://isecl-aas:8444/aas/v1 IHUB_SERVICE_USERNAME=<Integration Hub Service User username> IHUB_SERVICE_PASSWORD=<Integration Hub Service User password> # CMS URL and CMS webserivce TLS hash for server verification CMS_BASE_URL=https://isecl-cms:8445/cms/v1 CMS_TLS_CERT_SHA384=<TLS hash> # TLS Configuration TLS_SAN_LIST=127.0.0.1,192.168.1.1,hub.server.com #comma-separated list of IP addresses and hostnames for the Hub to be used in the Subject Alternative Names list in the TLS Certificate # Verification Service URL HVS_BASE_URL=https://isecl-hvs:8443/hvs/v2 ATTESTATION_TYPE=HVS #Integration tenant type. Currently supported values are \"KUBENETES\" or \"OPENSTACK\" TENANT=<KUBERNETES or OPENSTACK> # OpenStack Integration Credentials - required for OpenStack integration only OPENSTACK_AUTH_URL=<OpenStack Keystone URL; typically http://openstack-ip:5000/> OPENSTACK_PLACEMENT_URL=<OpenStack Nova API URL; typically http://openstack-ip:8778/> OPENSTACK_USERNAME=<OpenStack username> OPENSTACK_PASSWORD=<OpenStack password> # Kubernetes Integration Credentials - required for Kubernetes integration only KUBERNETES_URL=https://kubernetes:6443/ KUBERNETES_CRD=custom-isecl KUBERNETES_CERT_FILE=/etc/ihub/apiserver.crt KUBERNETES_TOKEN=eyJhbGciOiJSUzI1NiIsImtpZCI6Ik...... # Installation admin bearer token for CSR approval request to CMS - mandatory BEARER_TOKEN=eyJhbGciOiJSUzM4NCIsImtpZCI6ImE\u2026 #Optional, configures the polling interval at which the Hub retrieves attestations from the HVS POLL_INTERVAL_MINUTES=2 #Optional, runs the installer skipping setup IHUB_NO_SETUP=false #Optional, configures the TLS certificate common name TLS_COMMON_NAME=Integration Hub TLS Certificate #Optional, log configuration LOG_MAX_LENGTH=1500 LOG_LEVEL=Info LOG_ENABLE_STDOUT=true","title":"Installation Answer File"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#configuration-options","text":"config-file : /etc/ihub/config log : max-length : 1500 enable-stdout : true level : trace ihub : service-username : admin@hub service-password : hubAdminPass poll-interval-minutes : 1 aas : url : https://<aas_ip>:8444/aas/v1 cms : url : https://<cms_ip>:8445/cms/v1/ tls-cert-digest : 8a035e3cdd... attestation-service : attestation-url : https://<hvs_ip>:8443/hvs/v2 attestation-type : HVS end-point : type : KUBERNETES or OPENSTACK url : https://<kubernetes_ip>:6443/ or OpenStack Nova URL crd-name : custom-isecl token : eyJhbGciOiJSUzI... username : OpenStack Username password : OpenStack Password auth-url : OpenStack Authentication URL cert-file : /etc/ihub/apiserver.crt tls : cert-file : /etc/ihub/tls-cert.pem key-file : /etc/ihub/tls-key.pem common-name : Integration Hub TLS Certificate san-list : 127.0.0.1,localhost","title":"Configuration Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#command-line-options","text":"","title":"Command-Line Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#available-commands","text":"","title":"Available Commands"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#help","text":"ihub -h | --help Displays the list of available CLI commands.","title":"Help"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#start","text":"ihub start Starts the services.","title":"Start"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#stop","text":"ihub stop Stops the services.","title":"Stop"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#status","text":"ihub status Reports whether the service is currently running.","title":"Status"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#uninstall","text":"ihub uninstall [--purge] Uninstalls the service, including the deletion of all files and folders. Database content is not removed. If the --purge option is used, database content will be removed during the uninstallation.","title":"Uninstall"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#version","text":"ihub -v | --version Reports the version of the service.","title":"Version"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#setup","text":"ihub setup <task> [--help] [--force] [-f <answer-file>] Usage of ihub setup : ihub setup <task> [--help] [--force] [-f <answer-file>] --help show help message for setup task --force existing configuration will e overwritten if this flag is set -f|--file <answer-file> the answer file with required arguments Available Tasks for setup : all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls attestation-service-connection Establish Attestation service connection tenant-service-connection Establish Tenant service connection create-signing-key Create signing key for IHUB download-saml-cert Download SAML certificate from Attestation service update-service-config Sets or Updates the Service configuration","title":"Setup"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#directory-layout","text":"The ihub installs by default to /etc/ihub. This directory contains the config.yaml configuration file, saml certificate, trusted ca, and the webservice TLS certificate. /etc/ihub/ \u251c\u2500\u2500 apiserver.crt \u251c\u2500\u2500 certs \u2502 \u251c\u2500\u2500 saml \u2502 \u2502 \u2514\u2500\u2500 saml-cert.pem \u2502 \u2514\u2500\u2500 trustedca \u2502 \u2514\u2500\u2500 58f6bcfcd.pem \u251c\u2500\u2500 config.yml \u251c\u2500\u2500 ihub_private_key.pem \u251c\u2500\u2500 ihub_public_key.pem \u251c\u2500\u2500 tls-cert.pem \u2514\u2500\u2500 tls-key.pem","title":"Directory Layout"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings3/#logs","text":"/var/logs/ihub","title":"Logs"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/","text":"Certificate Management Service Installation Answer File Options Key Sample Value Description CMS_NOSETUP false Determines whether \u201csetup\u201d will be executed after installation. Typically this is set to \u201cfalse\u201d to install and perform setup in one action. The \u201ctrue\u201d option is intended for building the service as a container, where the installation would be part of the image build, and setup would be performed when the container starts for the first time to generate any persistent data. CMS_PORT 8445 Defines the HTTPS port the service will use. AAS_API_URL https://\\<Hostname or IP address of the AAS>:8444/aas/v1/ URL to connect to the AAS, used during setup for authentication. AAS_TLS_SAN \\<Comma-separated list of IPs/hostnames for the AAS> SAN list populated in special JWT token, this token is used by AAS to get TLS certificate signed from CMS. SAN list in this token and CSR generated by AAS must match. LOG_ROTATION_PERIOD hourly, daily, weekly, monthly, yearly log rotation period, for more details refer- https://linux.die.net/man/8/logrotate LOG_COMPRESS Compress Old versions of log files are compressed with gzip, for more details refer- https://linux.die.net/man/8/logrotate LOG_DELAYCOMPRESS delaycompress Postpone compression of the previous log file to the next rotation cycle, for more details refer- https://linux.die.net/man/8/logrotate LOG_COPYTRUNCATE Copytruncate Truncate the original log file in place after creating a copy,'create' creates new one, for more details refer- https://linux.die.net/man/8/logrotate LOG_SIZE 1K Log files are rotated when they grow bigger than size bytes, for more details refer- https://linux.die.net/man/8/logrotate LOG_OLD 12 Log files are rotated count times before being removed, for more details refer- https://linux.die.net/man/8/logrotate CMS_CA_CERT_VALIDITY 5 CMS Root Certificate Validity in years CMS_CA_ORGANIZATION INTEL CMS Certificate Organization CMS_CA_LOCALITY US CMS Certificate locality CMS_CA_PROVINCE CA CMS Certificate province CMS_CA_COUNTRY USA CMS Certificate country CMS_TLS_SAN_LIST Comma-separated list of IP addresses and hostnames to be added to the SAN list of CMS server CMS_SERVER_READ_TIMEOUT 30s MS server - ReadTimeout is the maximum duration for reading the entire request, including the body. CMS_SERVER_READ_HEADER_TIMEOUT 10s CMS server - ReadHeaderTimeout is the amount of time allowed to read request headers CMS_SERVER_WRITE_TIMEOUT 10s CMS server - WriteTimeout is the maximum duration before timing out writes of the response. CMS_SERVER_IDLE_TIMEOUT 10s CMS server - IdleTimeout is the maximum amount of time to wait for the next request when keep-alives are enabled. CMS_SERVER_MAX_HEADER_BYTES 1048576 CMS server - MaxHeaderBytes controls the maximum number of bytes the server will read parsing the request header's keys and values, including the request line. AAS_JWT_CN AAS JWT Signing Certificate CN of AAS JWT certificate, this gets populated in special JWT token. AAS must send JWT certificate CSR with this CN. AAS_TLS_CN AAS TLS Certificate CN of AAS TLS certificate, this gets populated in special JWT token. AAS must send TLS certificate CSR with this CN. AAS_TLS_SAN SAN list populated in special JWT token, this token is used by AAS to get TLS certificate signed from CMS. SAN list in this token and CSR generated by AAS must match. Configuration Options The CMS configuration can be found in /etc/cms/config.yml port : 8445 loglevel : info authserviceurl : https://<AAS IP or hostname>:8444/aas/v1/ cacertvalidity : 5 organization : INTEL locality : SC province : CA country : US keyalgorithm : rsa keyalgorithmlength : 3072 rootcacertdigest : <sha384> tlscertdigest : <sha384> tokendurationmins : 20 aasjwtcn : \"\" aastlscn : \"\" aastlssan : \"\" authdefender : maxattempts : 5 intervalmins : 5 lockoutdurationmins : 15 Command-Line Options Help cms help Displays the list of available CLI commands. Start cms start Starts the services. Stop cms stop Stops the service. Status cms status Reports whether the service is currently running. Uninstall cms uninstall Uninstalls the service, including the deletion of all files and folders. Version cms version Reports the version of the service. Tlscertsha384 Shows the SHA384 of the TLS certificate. setup [task] Usage of cms setup : cms setup <task> [--help] [--force] [-f <answer-file>] --help show help message for setup task --force existing configuration will be overwritten if this flag is set -f|--file <answer-file> the answer file with required arguments Available Tasks for setup : all Runs all setup tasks root-ca Creates a self signed Root CA key pair in /etc/cms/root-ca/ for quality of life intermediate-ca Creates a Root CA signed intermediate CA key pair(signing, tls-server and tls-client) in /etc/cms/intermediate-ca/ for quality of life tls Creates an intermediate-ca signed TLS key pair in /etc/cms for quality of life cms-auth-token Create its own self signed JWT key pair in /etc/cms/jwt for quality of life update-service-config Sets or Updates the Service configuration Directory Layout The Certificate Management Service installs by default to /opt/cms with the following folders. Bin This folder contains executable scripts. Cacerts This folder contains the CMS root CA certificate.","title":"Certificate Management Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#certificate-management-service","text":"","title":"Certificate Management Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#installation-answer-file-options","text":"Key Sample Value Description CMS_NOSETUP false Determines whether \u201csetup\u201d will be executed after installation. Typically this is set to \u201cfalse\u201d to install and perform setup in one action. The \u201ctrue\u201d option is intended for building the service as a container, where the installation would be part of the image build, and setup would be performed when the container starts for the first time to generate any persistent data. CMS_PORT 8445 Defines the HTTPS port the service will use. AAS_API_URL https://\\<Hostname or IP address of the AAS>:8444/aas/v1/ URL to connect to the AAS, used during setup for authentication. AAS_TLS_SAN \\<Comma-separated list of IPs/hostnames for the AAS> SAN list populated in special JWT token, this token is used by AAS to get TLS certificate signed from CMS. SAN list in this token and CSR generated by AAS must match. LOG_ROTATION_PERIOD hourly, daily, weekly, monthly, yearly log rotation period, for more details refer- https://linux.die.net/man/8/logrotate LOG_COMPRESS Compress Old versions of log files are compressed with gzip, for more details refer- https://linux.die.net/man/8/logrotate LOG_DELAYCOMPRESS delaycompress Postpone compression of the previous log file to the next rotation cycle, for more details refer- https://linux.die.net/man/8/logrotate LOG_COPYTRUNCATE Copytruncate Truncate the original log file in place after creating a copy,'create' creates new one, for more details refer- https://linux.die.net/man/8/logrotate LOG_SIZE 1K Log files are rotated when they grow bigger than size bytes, for more details refer- https://linux.die.net/man/8/logrotate LOG_OLD 12 Log files are rotated count times before being removed, for more details refer- https://linux.die.net/man/8/logrotate CMS_CA_CERT_VALIDITY 5 CMS Root Certificate Validity in years CMS_CA_ORGANIZATION INTEL CMS Certificate Organization CMS_CA_LOCALITY US CMS Certificate locality CMS_CA_PROVINCE CA CMS Certificate province CMS_CA_COUNTRY USA CMS Certificate country CMS_TLS_SAN_LIST Comma-separated list of IP addresses and hostnames to be added to the SAN list of CMS server CMS_SERVER_READ_TIMEOUT 30s MS server - ReadTimeout is the maximum duration for reading the entire request, including the body. CMS_SERVER_READ_HEADER_TIMEOUT 10s CMS server - ReadHeaderTimeout is the amount of time allowed to read request headers CMS_SERVER_WRITE_TIMEOUT 10s CMS server - WriteTimeout is the maximum duration before timing out writes of the response. CMS_SERVER_IDLE_TIMEOUT 10s CMS server - IdleTimeout is the maximum amount of time to wait for the next request when keep-alives are enabled. CMS_SERVER_MAX_HEADER_BYTES 1048576 CMS server - MaxHeaderBytes controls the maximum number of bytes the server will read parsing the request header's keys and values, including the request line. AAS_JWT_CN AAS JWT Signing Certificate CN of AAS JWT certificate, this gets populated in special JWT token. AAS must send JWT certificate CSR with this CN. AAS_TLS_CN AAS TLS Certificate CN of AAS TLS certificate, this gets populated in special JWT token. AAS must send TLS certificate CSR with this CN. AAS_TLS_SAN SAN list populated in special JWT token, this token is used by AAS to get TLS certificate signed from CMS. SAN list in this token and CSR generated by AAS must match.","title":"Installation Answer File Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#configuration-options","text":"The CMS configuration can be found in /etc/cms/config.yml port : 8445 loglevel : info authserviceurl : https://<AAS IP or hostname>:8444/aas/v1/ cacertvalidity : 5 organization : INTEL locality : SC province : CA country : US keyalgorithm : rsa keyalgorithmlength : 3072 rootcacertdigest : <sha384> tlscertdigest : <sha384> tokendurationmins : 20 aasjwtcn : \"\" aastlscn : \"\" aastlssan : \"\" authdefender : maxattempts : 5 intervalmins : 5 lockoutdurationmins : 15","title":"Configuration Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#command-line-options","text":"","title":"Command-Line Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#help","text":"cms help Displays the list of available CLI commands.","title":"Help"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#start","text":"cms start Starts the services.","title":"Start"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#stop","text":"cms stop Stops the service.","title":"Stop"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#status","text":"cms status Reports whether the service is currently running.","title":"Status"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#uninstall","text":"cms uninstall Uninstalls the service, including the deletion of all files and folders.","title":"Uninstall"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#version","text":"cms version Reports the version of the service.","title":"Version"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#tlscertsha384","text":"Shows the SHA384 of the TLS certificate.","title":"Tlscertsha384"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#setup-task","text":"Usage of cms setup : cms setup <task> [--help] [--force] [-f <answer-file>] --help show help message for setup task --force existing configuration will be overwritten if this flag is set -f|--file <answer-file> the answer file with required arguments Available Tasks for setup : all Runs all setup tasks root-ca Creates a self signed Root CA key pair in /etc/cms/root-ca/ for quality of life intermediate-ca Creates a Root CA signed intermediate CA key pair(signing, tls-server and tls-client) in /etc/cms/intermediate-ca/ for quality of life tls Creates an intermediate-ca signed TLS key pair in /etc/cms for quality of life cms-auth-token Create its own self signed JWT key pair in /etc/cms/jwt for quality of life update-service-config Sets or Updates the Service configuration","title":"setup [task]"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#directory-layout","text":"The Certificate Management Service installs by default to /opt/cms with the following folders.","title":"Directory Layout"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#bin","text":"This folder contains executable scripts.","title":"Bin"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings4/#cacerts","text":"This folder contains the CMS root CA certificate.","title":"Cacerts"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings5/","text":"Authentication and Authorization Service Installation Answer File Options Key Sample Value Description CMS_BASE_URL https://<cms IP or hostname>/cms/v1/ Required; Provides the URL for the CMS. AAS_NOSETUP false Optional. Determines whether \u201csetup\u201d will be executed after installation. Typically this is set to \u201cfalse\u201d to install and perform setup in one action. The \u201ctrue\u201d option is intended for building the service as a container, where the installation would be part of the image build, and setup would be performed when the container starts for the first time to generate any persistent data. AAS_DB_HOSTNAME localhost Required. Hostname or IP address of the AAS database AAS_DB_PORT 5432 Required. Database port number AAS_DB_NAME pgdb Required. Database name AAS_DB_USERNAME dbuser Required. Database username AAS_DB_PASSWORD dbpassword Required. Database password AAS_DB_SSLMODE verify-ca Defines the SSL mode for the connection to the database. If not specified, the database connection will not use certificate verification. If specified, certificate verification will be required for database connections. AAS_DB_SSLCERTSRC /usr/local/pgsql/data/server.crt Optional, required if the \u201cAAS_DB_SSLMODE is set to verify-ca Defines the location of the database SSL certificate. AAS_DB_SSLCERT \\<path_to_cert_file_on_system> Optional. The AAS_DB_SSLCERTSRC variable defines the source location of the database SSL certificate; this variable determines the local location. If the former option is used without specifying this option, the service will copy the SSL certificate to the default configuration directory. AAS_ADMIN_USERNAME admin@aas Required. Defines a new AAS administrative user. This user will be able to create new users, new roles, and new role-user mappings. This user will have the AAS:Administrator role. AAS_ADMIN_PASSWORD aasAdminPass Required. Password for the new AAS admin user. AAS_JWT_CERT_SUBJECT \"AAS JWT Signing Certificate\" Optional. Defines the subject of the JWT signing certificate. AAS_JWT_TOKEN_DURATION_MINS 5 Optional. Defines the amount of time in minutes that an issued token will be valid. SAN_LIST 127.0.0.1,localhost,10.x.x.x Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service. BEARER_TOKEN \\<token> Required. Token from the CMS generated during CMS setup that allows the AAS to perform initial setup tasks. LOG_LEVEL Critical, error, warning, info, debug, trace Optional. Defaults to INFO. Changes the log level used. Configuration Options Command-Line Options Usage: authservice <command> [arguments] Available Commands : -h|--help | help Show this help message setup <task> Run setup task start Start authservice status Show the status of authservice stop Stop authservice uninstall [--purge] Uninstall authservice. --purge option needs to be applied to remove configuration and data files -v|--version | version Show the version of authservice Usage of authservice setup : authservice setup [task] [--help] [--force] [-f <answer-file>] --help show help message for setup task --force existing configuration will be overwritten if this flag is set -f|--file <answer-file> the answer file with required arguments Available Tasks for setup : all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls database Setup authservice database admin Add authservice admin username and password to database and assign respective roles to the user jwt Create jwt signing key and jwt certificate signed by CMS update-service-config Sets or Updates the Service configuration Directory Layout The Verification Service installs by default to /opt/authservice with the following folders. Bin Contains executable scripts and binaries. dbscripts Contains database scripts.","title":"Authentication and Authorization Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings5/#authentication-and-authorization-service","text":"","title":"Authentication and Authorization Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings5/#installation-answer-file-options","text":"Key Sample Value Description CMS_BASE_URL https://<cms IP or hostname>/cms/v1/ Required; Provides the URL for the CMS. AAS_NOSETUP false Optional. Determines whether \u201csetup\u201d will be executed after installation. Typically this is set to \u201cfalse\u201d to install and perform setup in one action. The \u201ctrue\u201d option is intended for building the service as a container, where the installation would be part of the image build, and setup would be performed when the container starts for the first time to generate any persistent data. AAS_DB_HOSTNAME localhost Required. Hostname or IP address of the AAS database AAS_DB_PORT 5432 Required. Database port number AAS_DB_NAME pgdb Required. Database name AAS_DB_USERNAME dbuser Required. Database username AAS_DB_PASSWORD dbpassword Required. Database password AAS_DB_SSLMODE verify-ca Defines the SSL mode for the connection to the database. If not specified, the database connection will not use certificate verification. If specified, certificate verification will be required for database connections. AAS_DB_SSLCERTSRC /usr/local/pgsql/data/server.crt Optional, required if the \u201cAAS_DB_SSLMODE is set to verify-ca Defines the location of the database SSL certificate. AAS_DB_SSLCERT \\<path_to_cert_file_on_system> Optional. The AAS_DB_SSLCERTSRC variable defines the source location of the database SSL certificate; this variable determines the local location. If the former option is used without specifying this option, the service will copy the SSL certificate to the default configuration directory. AAS_ADMIN_USERNAME admin@aas Required. Defines a new AAS administrative user. This user will be able to create new users, new roles, and new role-user mappings. This user will have the AAS:Administrator role. AAS_ADMIN_PASSWORD aasAdminPass Required. Password for the new AAS admin user. AAS_JWT_CERT_SUBJECT \"AAS JWT Signing Certificate\" Optional. Defines the subject of the JWT signing certificate. AAS_JWT_TOKEN_DURATION_MINS 5 Optional. Defines the amount of time in minutes that an issued token will be valid. SAN_LIST 127.0.0.1,localhost,10.x.x.x Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service. BEARER_TOKEN \\<token> Required. Token from the CMS generated during CMS setup that allows the AAS to perform initial setup tasks. LOG_LEVEL Critical, error, warning, info, debug, trace Optional. Defaults to INFO. Changes the log level used.","title":"Installation Answer File Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings5/#configuration-options","text":"","title":"Configuration Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings5/#command-line-options","text":"Usage: authservice <command> [arguments] Available Commands : -h|--help | help Show this help message setup <task> Run setup task start Start authservice status Show the status of authservice stop Stop authservice uninstall [--purge] Uninstall authservice. --purge option needs to be applied to remove configuration and data files -v|--version | version Show the version of authservice Usage of authservice setup : authservice setup [task] [--help] [--force] [-f <answer-file>] --help show help message for setup task --force existing configuration will be overwritten if this flag is set -f|--file <answer-file> the answer file with required arguments Available Tasks for setup : all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls database Setup authservice database admin Add authservice admin username and password to database and assign respective roles to the user jwt Create jwt signing key and jwt certificate signed by CMS update-service-config Sets or Updates the Service configuration","title":"Command-Line Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings5/#directory-layout","text":"The Verification Service installs by default to /opt/authservice with the following folders.","title":"Directory Layout"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings5/#bin","text":"Contains executable scripts and binaries.","title":"Bin"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings5/#dbscripts","text":"Contains database scripts.","title":"dbscripts"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/","text":"Workload Service Installation Answer File Options Key Sample Value Description WLS_LOGLEVEL INFO (Optional) Alternatives include WARN and DEBUG. Sets the log level for the service. WLS_NOSETUP false (Optional) Determines whether \u201csetup\u201d will be executed after installation. Typically this is set to \u201cfalse\u201d to install and perform setup in one action. The \u201ctrue\u201d option is intended for building the service as a container, where the installation would be part of the image build, and setup would be performed when the container starts for the first time to generate any persistent data. Defaults to \u201cfalse\u201d if unset. WLS_PORT 5000 (Optional) Defines the HTTPS port used by the service Defaults to 5000 if unset. WLS_DB_HOSTNAME localhost (Required) Database hostname WLS_DB wlsdb (Required) Database name WLS_DB_PORT 5432 (Required) Database port number WLS_DB_USERNAME wlsdbuser (Required) Database username WLS_DB_PASSWORD wlsdbuserpass (Required) Database password HVS_URL https://\\<HVS IP address or hostname>:8443/hvs/v2/ (Required) Base URL for the HVS AAS_API_URL https://\\<AAS IP address or hostname>:8444/aas/v1 Base URL for the AAS SAN_LIST 127.0.0.1,localhost,10.x.x.x Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service. CMS_BASE_URL Base URL for the CMS BEARER_TOKEN \\<token> (Required) Token from the CMS generated during CMS setup that allows the AAS to perform initial setup tasks. WLS_TLS_CERT_CN 'WLS TLS Certificate (Optional) Set the Common name for TLS cert to be downloaded from CMS. Default is 'WLS TLS Certificate'. WLS_CERT_ORG 'INTEL' (Optional) Set the Organization in Subject of CSR. Default is 'INTEL'. WLS_CERT_COUNTRY 'US' (Optional) Set the Country in Subject of CSR. Default is 'US'. WLS_CERT_PROVINCE 'SF' (Optional) Set the Province in Subject of CSR. Default is 'SF'. WLS_CERT_LOCALITY 'SC' (Optional) Set the Locality in Subject of CSR. Default is 'SC'. KEY_CACHE_SECONDS 300 (Optional) Set the time till which the key will be cached. Default is '300 seconds'. WLS_LOGLEVEL Info, debug, error, warn (Optional) Set the log level. KEY_PATH (Optional) Redefines the path to the keystore folder CERT_PATH (Optional) Redefines the path to the certificates folder Configuration Options The Workload Service configuration can be found in /etc/workload-service/config.yml : port : 5000 cmstlscertdigest : <sha384> postgres : dbname : wlsdb user : <database username> password : <database password> hostname : <database IP or hostname> port : 5432 sslmode : false hvs_api_url : https://<HVS IP or hostname>:8443/hvs/v2/ cms_base_url : https://<CMS IP or hostname>:8445:/cms/v1/ aas_api_url : https://<AAS IP or hostname>:8444/aas/v1/ subject : tlscertcommonname : WLS TLS Certificate organization : INTEL country : US province : SF locality : SC wls : user : <username of service account used by WLS to access other services>> password : <password> loglevel : info key_cache_seconds : 300 Command-Line Options The Workload Service supports several command-line commands that can be executed only as the Root user: Syntax: workload-service <command> Help Available Commands: help Show this help message start Start workload-service stop Stop workload-service status Determine if workload-service is running uninstall [--purge] Uninstall workload-service. --purge option needs to be applied to remove configuration and data files setup Setup workload-service for use Setup command usage: workload-service <command> [task...] Available tasks for setup: download_ca_cert - Download CMS root CA certificate - Environment variable CMS_BASE_URL=<url> for CMS API url download_cert TLS - Generates Key pair and CSR, gets it signed from CMS - Environment variable CMS_BASE_URL=<url> for CMS API url - Environment variable BEARER_TOKEN=<token> for authenticating with CMS - Environment variable KEY_PATH=<key_path> to override default specified in config - Environment variable CERT_PATH=<cert_path> to override default specified in config - Environment variable WLS_TLS_CERT_CN=<COMMON NAME> to override default specified in config - Environment variable WLS_CERT_ORG=<CERTIFICATE ORGANIZATION> to override default specified in config - Environment variable WLS_CERT_COUNTRY=<CERTIFICATE COUNTRY> to override default specified in config - Environment variable WLS_CERT_LOCALITY=<CERTIFICATE LOCALITY> to override default specified in config - Environment variable WLS_CERT_PROVINCE=<CERTIFICATE PROVINCE> to override default specified in config server Setup http server on given port -Environment variable WLS_PORT=<port> should be set database Setup workload-service database Required env variables are: - WLS_DB_HOSTNAME : database host name - WLS_DB_PORT : database port number - WLS_DB_USERNAME : database user name - WLS_DB_PASSWORD : database password - WLS_DB : database schema name hvsconnection Setup task for setting up the connection to the Host Verification Service(HVS) Required env variables are: - HVS_URL : HVS URL aasconnection Setup to create workload service user roles in AAS - AAS_API_URL : AAS API URL - BEARER_TOKEN : Bearer Token logs Setup workload-service log level - Environment variable WLS_LOG_LEVEL=<log level> should be set start Start workload-service stop Stop workload-service status Determine if workload-service is running uninstall [--purge] Uninstall workload-service. --purge option needs to be applied to remove configuration and data files setup Setup command usage : workload-service setup [task] [--force] Available tasks for setup : all Runs all setup tasks Required env variables : - get required env variables from all the setup tasks Optional env variables : - get optional env variables from all the setup tasks download_ca_cert Download CMS root CA certificate - Option [--force] overwrites any existing files, and always downloads new root CA cert Required env variables if WLS_NOSETUP=true or variables not set in config.yml : - AAS_API_URL=<url> : AAS API url - HVS_URL=<url> : HVS API Endpoint URL - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password Required env variables specific to setup task are : - CMS_BASE_URL=<url> : for CMS API url - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance download_cert TLS Generates Key pair and CSR, gets it signed from CMS - Option [--force] overwrites any existing files, and always downloads newly signed WLS TLS cert Required env variables if WLS_NOSETUP=true or variable not set in config.yml : - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance - AAS_API_URL=<url> : AAS API url - HVS_URL=<url> : HVS API Endpoint URL - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password Required env variables specific to setup task are : - CMS_BASE_URL=<url> : for CMS API url - BEARER_TOKEN=<token> : for authenticating with CMS - SAN_LIST=<CSV List> : List of FQDNs to be added to the SAN field in TLS cert to override default specified in config Optional env variables specific to setup task are : - KEY_PATH=<key_path> : Path of file where TLS key needs to be stored - CERT_PATH=<cert_path> : Path of file/directory where TLS certificate needs to be stored - WLS_TLS_CERT_CN=<COMMON NAME> : to override default specified in config database Setup workload-service database - Option [--force] overwrites existing database config Required env variables if WLS_NOSETUP=true or variable not set in config.yml : - CMS_BASE_URL=<url> : for CMS API url - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance - AAS_API_URL=<url> : AAS API url - HVS_URL=<url> : HVS API Endpoint URL - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password Required env variables specific to setup task are : - WLS_DB_HOSTNAME=<db host name> : database host name - WLS_DB_PORT=<db port> : database port number - WLS_DB=<db name> : database schema name - WLS_DB_USERNAME=<db user name> : database user name - WLS_DB_PASSWORD=<db password> : database password Optional env variables specific to setup task are : - WLS_DB_SSLMODE=<db sslmode> : database SSL Connection Mode <disable|allow|prefer|require|verify-ca|verify-full> - WLS_DB_SSLCERT=<ssl certificate path> : database SSL Certificate target path. Only applicable for WLS_DB_SSLMODE=<verify-ca|verify-full>. If left empty, the cert will be copied to /etc/workload-service/wlsdbsslcert.pem - WLS_DB_SSLCERTSRC=<ssl certificate source path> : database SSL Certificate source path. Mandatory if WLS_DB_SSLCERT does not already exist server Setup http server on given port - Option [--force] overwrites existing server config Required env variables if WLS_NOSETUP=true or variable not set in config.yml : - CMS_BASE_URL=<url> : for CMS API url - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance - AAS_API_URL=<url> : AAS API url - HVS_URL=<url> : HVS API Endpoint URL Optional env variables specific to setup task are : - WLS_PORT=<port> : WLS API listener port - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password hvsconnection Setup task for setting up the connection to the Host Verification Service(HVS) - Option [--force] overwrites existing HVS config Required env variables if WLS_NOSETUP=true or variable not set in config.yml : - CMS_BASE_URL=<url> : for CMS API url - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance - AAS_API_URL=<url> : AAS API url - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password Required env variable specific to setup task is : - HVS_URL=<url> : HVS API Endpoint URL download_saml_ca_cert Setup to download SAML CA certificates from HVS - Option [--force] overwrites existing certificate Required env variables if WLS_NOSETUP=true or variable not set in config.yml : - CMS_BASE_URL=<url> : for CMS API url - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance - AAS_API_URL=<url> : AAS API url - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password Required env variables specific to setup task are : - HVS_URL=<url> : HVS API Endpoint URL - BEARER_TOKEN=<token> for authenticating with HVS Directory Layout The Workload Service installs by default to /opt/wls with the following folders.","title":"Workload Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/#workload-service","text":"","title":"Workload Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/#installation-answer-file-options","text":"Key Sample Value Description WLS_LOGLEVEL INFO (Optional) Alternatives include WARN and DEBUG. Sets the log level for the service. WLS_NOSETUP false (Optional) Determines whether \u201csetup\u201d will be executed after installation. Typically this is set to \u201cfalse\u201d to install and perform setup in one action. The \u201ctrue\u201d option is intended for building the service as a container, where the installation would be part of the image build, and setup would be performed when the container starts for the first time to generate any persistent data. Defaults to \u201cfalse\u201d if unset. WLS_PORT 5000 (Optional) Defines the HTTPS port used by the service Defaults to 5000 if unset. WLS_DB_HOSTNAME localhost (Required) Database hostname WLS_DB wlsdb (Required) Database name WLS_DB_PORT 5432 (Required) Database port number WLS_DB_USERNAME wlsdbuser (Required) Database username WLS_DB_PASSWORD wlsdbuserpass (Required) Database password HVS_URL https://\\<HVS IP address or hostname>:8443/hvs/v2/ (Required) Base URL for the HVS AAS_API_URL https://\\<AAS IP address or hostname>:8444/aas/v1 Base URL for the AAS SAN_LIST 127.0.0.1,localhost,10.x.x.x Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service. CMS_BASE_URL Base URL for the CMS BEARER_TOKEN \\<token> (Required) Token from the CMS generated during CMS setup that allows the AAS to perform initial setup tasks. WLS_TLS_CERT_CN 'WLS TLS Certificate (Optional) Set the Common name for TLS cert to be downloaded from CMS. Default is 'WLS TLS Certificate'. WLS_CERT_ORG 'INTEL' (Optional) Set the Organization in Subject of CSR. Default is 'INTEL'. WLS_CERT_COUNTRY 'US' (Optional) Set the Country in Subject of CSR. Default is 'US'. WLS_CERT_PROVINCE 'SF' (Optional) Set the Province in Subject of CSR. Default is 'SF'. WLS_CERT_LOCALITY 'SC' (Optional) Set the Locality in Subject of CSR. Default is 'SC'. KEY_CACHE_SECONDS 300 (Optional) Set the time till which the key will be cached. Default is '300 seconds'. WLS_LOGLEVEL Info, debug, error, warn (Optional) Set the log level. KEY_PATH (Optional) Redefines the path to the keystore folder CERT_PATH (Optional) Redefines the path to the certificates folder","title":"Installation Answer File Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/#configuration-options","text":"The Workload Service configuration can be found in /etc/workload-service/config.yml : port : 5000 cmstlscertdigest : <sha384> postgres : dbname : wlsdb user : <database username> password : <database password> hostname : <database IP or hostname> port : 5432 sslmode : false hvs_api_url : https://<HVS IP or hostname>:8443/hvs/v2/ cms_base_url : https://<CMS IP or hostname>:8445:/cms/v1/ aas_api_url : https://<AAS IP or hostname>:8444/aas/v1/ subject : tlscertcommonname : WLS TLS Certificate organization : INTEL country : US province : SF locality : SC wls : user : <username of service account used by WLS to access other services>> password : <password> loglevel : info key_cache_seconds : 300","title":"Configuration Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/#command-line-options","text":"The Workload Service supports several command-line commands that can be executed only as the Root user: Syntax: workload-service <command>","title":"Command-Line Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/#help","text":"Available Commands: help Show this help message start Start workload-service stop Stop workload-service status Determine if workload-service is running uninstall [--purge] Uninstall workload-service. --purge option needs to be applied to remove configuration and data files setup Setup workload-service for use Setup command usage: workload-service <command> [task...] Available tasks for setup: download_ca_cert - Download CMS root CA certificate - Environment variable CMS_BASE_URL=<url> for CMS API url download_cert TLS - Generates Key pair and CSR, gets it signed from CMS - Environment variable CMS_BASE_URL=<url> for CMS API url - Environment variable BEARER_TOKEN=<token> for authenticating with CMS - Environment variable KEY_PATH=<key_path> to override default specified in config - Environment variable CERT_PATH=<cert_path> to override default specified in config - Environment variable WLS_TLS_CERT_CN=<COMMON NAME> to override default specified in config - Environment variable WLS_CERT_ORG=<CERTIFICATE ORGANIZATION> to override default specified in config - Environment variable WLS_CERT_COUNTRY=<CERTIFICATE COUNTRY> to override default specified in config - Environment variable WLS_CERT_LOCALITY=<CERTIFICATE LOCALITY> to override default specified in config - Environment variable WLS_CERT_PROVINCE=<CERTIFICATE PROVINCE> to override default specified in config server Setup http server on given port -Environment variable WLS_PORT=<port> should be set database Setup workload-service database Required env variables are: - WLS_DB_HOSTNAME : database host name - WLS_DB_PORT : database port number - WLS_DB_USERNAME : database user name - WLS_DB_PASSWORD : database password - WLS_DB : database schema name hvsconnection Setup task for setting up the connection to the Host Verification Service(HVS) Required env variables are: - HVS_URL : HVS URL aasconnection Setup to create workload service user roles in AAS - AAS_API_URL : AAS API URL - BEARER_TOKEN : Bearer Token logs Setup workload-service log level - Environment variable WLS_LOG_LEVEL=<log level> should be set","title":"Help"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/#start","text":"Start workload-service","title":"start"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/#stop","text":"Stop workload-service","title":"stop"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/#status","text":"Determine if workload-service is running","title":"status"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/#uninstall","text":"[--purge] Uninstall workload-service. --purge option needs to be applied to remove configuration and data files","title":"uninstall"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/#setup","text":"Setup command usage : workload-service setup [task] [--force] Available tasks for setup : all Runs all setup tasks Required env variables : - get required env variables from all the setup tasks Optional env variables : - get optional env variables from all the setup tasks download_ca_cert Download CMS root CA certificate - Option [--force] overwrites any existing files, and always downloads new root CA cert Required env variables if WLS_NOSETUP=true or variables not set in config.yml : - AAS_API_URL=<url> : AAS API url - HVS_URL=<url> : HVS API Endpoint URL - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password Required env variables specific to setup task are : - CMS_BASE_URL=<url> : for CMS API url - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance download_cert TLS Generates Key pair and CSR, gets it signed from CMS - Option [--force] overwrites any existing files, and always downloads newly signed WLS TLS cert Required env variables if WLS_NOSETUP=true or variable not set in config.yml : - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance - AAS_API_URL=<url> : AAS API url - HVS_URL=<url> : HVS API Endpoint URL - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password Required env variables specific to setup task are : - CMS_BASE_URL=<url> : for CMS API url - BEARER_TOKEN=<token> : for authenticating with CMS - SAN_LIST=<CSV List> : List of FQDNs to be added to the SAN field in TLS cert to override default specified in config Optional env variables specific to setup task are : - KEY_PATH=<key_path> : Path of file where TLS key needs to be stored - CERT_PATH=<cert_path> : Path of file/directory where TLS certificate needs to be stored - WLS_TLS_CERT_CN=<COMMON NAME> : to override default specified in config database Setup workload-service database - Option [--force] overwrites existing database config Required env variables if WLS_NOSETUP=true or variable not set in config.yml : - CMS_BASE_URL=<url> : for CMS API url - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance - AAS_API_URL=<url> : AAS API url - HVS_URL=<url> : HVS API Endpoint URL - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password Required env variables specific to setup task are : - WLS_DB_HOSTNAME=<db host name> : database host name - WLS_DB_PORT=<db port> : database port number - WLS_DB=<db name> : database schema name - WLS_DB_USERNAME=<db user name> : database user name - WLS_DB_PASSWORD=<db password> : database password Optional env variables specific to setup task are : - WLS_DB_SSLMODE=<db sslmode> : database SSL Connection Mode <disable|allow|prefer|require|verify-ca|verify-full> - WLS_DB_SSLCERT=<ssl certificate path> : database SSL Certificate target path. Only applicable for WLS_DB_SSLMODE=<verify-ca|verify-full>. If left empty, the cert will be copied to /etc/workload-service/wlsdbsslcert.pem - WLS_DB_SSLCERTSRC=<ssl certificate source path> : database SSL Certificate source path. Mandatory if WLS_DB_SSLCERT does not already exist server Setup http server on given port - Option [--force] overwrites existing server config Required env variables if WLS_NOSETUP=true or variable not set in config.yml : - CMS_BASE_URL=<url> : for CMS API url - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance - AAS_API_URL=<url> : AAS API url - HVS_URL=<url> : HVS API Endpoint URL Optional env variables specific to setup task are : - WLS_PORT=<port> : WLS API listener port - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password hvsconnection Setup task for setting up the connection to the Host Verification Service(HVS) - Option [--force] overwrites existing HVS config Required env variables if WLS_NOSETUP=true or variable not set in config.yml : - CMS_BASE_URL=<url> : for CMS API url - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance - AAS_API_URL=<url> : AAS API url - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password Required env variable specific to setup task is : - HVS_URL=<url> : HVS API Endpoint URL download_saml_ca_cert Setup to download SAML CA certificates from HVS - Option [--force] overwrites existing certificate Required env variables if WLS_NOSETUP=true or variable not set in config.yml : - CMS_BASE_URL=<url> : for CMS API url - CMS_TLS_CERT_SHA384=<CMS TLS cert sha384 hash> : to ensure that WLS is talking to the right CMS instance - AAS_API_URL=<url> : AAS API url - WLS_SERVICE_USERNAME=<service username> : WLS service username - WLS_SERVICE_PASSWORD=<service password> : WLS service password Required env variables specific to setup task are : - HVS_URL=<url> : HVS API Endpoint URL - BEARER_TOKEN=<token> for authenticating with HVS","title":"setup"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings6/#directory-layout","text":"The Workload Service installs by default to /opt/wls with the following folders.","title":"Directory Layout"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings7/","text":"Key Broker Service Installation Answer File Options Configuration Options Command-Line Options The Key Broker Service supports several command-line commands that can be executed only as the Root user: Usage : kbs <command> [arguments] Available Commands : help|-h|--help Show this help message version|-v|--version Show the version of current kbs build setup <task> Run setup task start Start kbs status Show the status of kbs stop Stop kbs uninstall [--purge] Uninstall kbs --purge all configuration and data files will be removed if this flag is set Usage of kbs setup : kbs setup <task> [--help] [--force] [-f <answer-file>] --help show help message for setup task --force existing configuration will be overwritten if this flag is set -f|--file <answer-file> the answer file with required arguments Available Tasks for setup : all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls create-default-key-transfer-policy Create default key transfer policy for KBS update-service-config Sets or Updates the Service configuration Directory Layout The Verification Service installs by default with the following folders: /opt/kbs/bin : Contains KBS binaries /etc/kbs/ : Contains KBS configuration files /var/log/kbs/ : Contains KBS logs","title":"Key Broker Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings7/#key-broker-service","text":"","title":"Key Broker Service"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings7/#installation-answer-file-options","text":"","title":"Installation Answer File Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings7/#configuration-options","text":"","title":"Configuration Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings7/#command-line-options","text":"The Key Broker Service supports several command-line commands that can be executed only as the Root user: Usage : kbs <command> [arguments] Available Commands : help|-h|--help Show this help message version|-v|--version Show the version of current kbs build setup <task> Run setup task start Start kbs status Show the status of kbs stop Stop kbs uninstall [--purge] Uninstall kbs --purge all configuration and data files will be removed if this flag is set Usage of kbs setup : kbs setup <task> [--help] [--force] [-f <answer-file>] --help show help message for setup task --force existing configuration will be overwritten if this flag is set -f|--file <answer-file> the answer file with required arguments Available Tasks for setup : all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls create-default-key-transfer-policy Create default key transfer policy for KBS update-service-config Sets or Updates the Service configuration","title":"Command-Line Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings7/#directory-layout","text":"The Verification Service installs by default with the following folders: /opt/kbs/bin : Contains KBS binaries /etc/kbs/ : Contains KBS configuration files /var/log/kbs/ : Contains KBS logs","title":"Directory Layout"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/","text":"Workload Agent Installation Answer File Options Configuration Options Command-Line Options Available Commands: Help wlagent help Show help message setup wlagent setup [task] Run setup task Available Tasks for setup SigningKey Generate a TPM signing key BindingKey Generate a TPM binding key RegisterSigningKey Register a signing key with the host verification service Environment variable BEARER_TOKEN= for authenticating with Verification service RegisterBindingKey Register a binding key with the host verification service Environment variable BEARER_TOKEN= for authenticating with Verification service start Start wlagent stop Stop wlagent status Reports the status of wlagent service uninstall Uninstall wlagent uninstall --purge Uninstalls workload agent and deletes the existing configuration directory version Reports the version of the workload agent Directory Layout The Workload Agent installs by default to /opt/workload-agent with the following folders. Bin Contains scripts and executable binaries.","title":"Workload Agent"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#workload-agent","text":"","title":"Workload Agent"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#installation-answer-file-options","text":"","title":"Installation Answer File Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#configuration-options","text":"","title":"Configuration Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#command-line-options","text":"Available Commands:","title":"Command-Line Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#help","text":"wlagent help Show help message","title":"Help"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#setup","text":"wlagent setup [task] Run setup task","title":"setup"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#available-tasks-for-setup","text":"","title":"Available Tasks for setup"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#signingkey","text":"Generate a TPM signing key","title":"SigningKey"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#bindingkey","text":"Generate a TPM binding key","title":"BindingKey"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#registersigningkey","text":"Register a signing key with the host verification service Environment variable BEARER_TOKEN= for authenticating with Verification service","title":"RegisterSigningKey"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#registerbindingkey","text":"Register a binding key with the host verification service Environment variable BEARER_TOKEN= for authenticating with Verification service","title":"RegisterBindingKey"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#start","text":"Start wlagent","title":"start"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#stop","text":"Stop wlagent","title":"stop"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#status","text":"Reports the status of wlagent service","title":"status"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#uninstall","text":"Uninstall wlagent","title":"uninstall"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#uninstall-purge","text":"Uninstalls workload agent and deletes the existing configuration directory","title":"uninstall --purge"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#version","text":"Reports the version of the workload agent","title":"version"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#directory-layout","text":"The Workload Agent installs by default to /opt/workload-agent with the following folders.","title":"Directory Layout"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings8/#bin","text":"Contains scripts and executable binaries.","title":"Bin"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/","text":"Workload Policy Manager Installation Answer File Options Key Sample Value Description KBS_BASE_URL https://\\<IP address or hostname of the KBS>:9443/v1/ Required. Defines the baseurl for the Key Broker Service. The WPM uses this URL to request new encryption keys when encrypting images. CMS_TLS_CERT_SHA384 Required. SHA384 hash of the CMS TLS certificate CMS_BASE_URL https://\\<IP address or hostname for CMS>:8445/cms/v1/ Required. Defines the base URL for the CMS owned by the image owner. Note that this CMS may be different from the CMS used for other components. AAS_API_URL https://\\<IP address or hostname for AAS>:8444/aas/v1 Required. Defines the baseurl for the AAS owned by the image owner. Note that this AAS may be different from the AAS used for other components. BEARER_TOKEN \\<token> Required; token from CMS with permissions used for installation. WPM_LOG_LEVEL INFO (default), DEBUG Optional; defines the log level for the WPM. Defaults to INFO. WPM_SERVICE_PASSWORD Defines the credentials for the WPM to use to access the KBS WPM_SERVICE_USERNAME Defines the credentials for the WPM to use to access the KBS Configuration Options Command-Line Options The Workload Policy Manager supports several command-line commands that can be executed only as the Root user: Syntax: wpm <command> create-image-flavor Creates a new image flavor and encrypts a source image. Output is the image flavor in JSON format and the encrypted image. usage: wpm create-image-flavor [-l label] [-i in] [-o out] \\[-e encout] [-k key] -l, --label image flavor label -i, --in input image file path -o, --out (optional) output image flavor file path if not specified, will print to the console -e, --encout (optional) output encrypted image file path if not specified, encryption is skipped -k, --key (optional) existing key ID if not specified, a new key is generated create-software-flavor Not currently supported; intended for future functionality. Uninstall Removes the WPM. --help Displays help text --version Displays the WPM version Setup usage : wpm setup [ ] -space separated list of tasks wpm setup wpm setup CreateEnvelopeKey wpm setup RegisterEnvelopeKey wpm setup download_ca_cert [--force] - Download CMS root CA certificate - Option [--force] overwrites any existing files, and always downloads new root CA cert - Environment variable CMS_BASE_URL= for CMS API url wpm setup download_cert Flavor-Signing [--force] - Generates Key pair and CSR, gets it signed from CMS - Option [--force] overwrites any existing files, and always downloads newly signed Flavor Signing cert - Environment variable CMS_BASE_URL=<url> for CMS API url - Environment variable BEARER_TOKEN=<token> for authenticating with CMS - Environment variable KEY_PATH=<key_path> to override default specified in config - Environment variable CERT_PATH=<cert_path> to override default specified in config - Environment variable WPM_FLAVOR_SIGN_CERT_CN=<COMMON NAME> to override default specified in config - Environment variable WPM_CERT_ORG=<CERTIFICATE ORGANIZATION> to override default specified in config - Environment variable WPM_CERT_COUNTRY=<CERTIFICATE COUNTRY> to override default specified in config - Environment variable WPM_CERT_LOCALITY=<CERTIFICATE LOCALITY> to override default specified in config - Environment variable WPM_CERT_PROVINCE=<CERTIFICATE PROVINCE> to override default specified in config","title":"Workload Policy Manager"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#workload-policy-manager","text":"","title":"Workload Policy Manager"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#installation-answer-file-options","text":"Key Sample Value Description KBS_BASE_URL https://\\<IP address or hostname of the KBS>:9443/v1/ Required. Defines the baseurl for the Key Broker Service. The WPM uses this URL to request new encryption keys when encrypting images. CMS_TLS_CERT_SHA384 Required. SHA384 hash of the CMS TLS certificate CMS_BASE_URL https://\\<IP address or hostname for CMS>:8445/cms/v1/ Required. Defines the base URL for the CMS owned by the image owner. Note that this CMS may be different from the CMS used for other components. AAS_API_URL https://\\<IP address or hostname for AAS>:8444/aas/v1 Required. Defines the baseurl for the AAS owned by the image owner. Note that this AAS may be different from the AAS used for other components. BEARER_TOKEN \\<token> Required; token from CMS with permissions used for installation. WPM_LOG_LEVEL INFO (default), DEBUG Optional; defines the log level for the WPM. Defaults to INFO. WPM_SERVICE_PASSWORD Defines the credentials for the WPM to use to access the KBS WPM_SERVICE_USERNAME Defines the credentials for the WPM to use to access the KBS","title":"Installation Answer File Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#configuration-options","text":"","title":"Configuration Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#command-line-options","text":"The Workload Policy Manager supports several command-line commands that can be executed only as the Root user: Syntax: wpm <command>","title":"Command-Line Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#create-image-flavor","text":"Creates a new image flavor and encrypts a source image. Output is the image flavor in JSON format and the encrypted image. usage: wpm create-image-flavor [-l label] [-i in] [-o out] \\[-e encout] [-k key] -l, --label image flavor label -i, --in input image file path -o, --out (optional) output image flavor file path if not specified, will print to the console -e, --encout (optional) output encrypted image file path if not specified, encryption is skipped -k, --key (optional) existing key ID if not specified, a new key is generated","title":"create-image-flavor"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#create-software-flavor","text":"Not currently supported; intended for future functionality.","title":"create-software-flavor"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#uninstall","text":"Removes the WPM.","title":"Uninstall"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#-help","text":"Displays help text","title":"--help"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#-version","text":"Displays the WPM version","title":"--version"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#setup","text":"usage : wpm setup [ ] -space separated list of tasks","title":"Setup"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#wpm-setup","text":"","title":"wpm setup"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#wpm-setup-createenvelopekey","text":"","title":"wpm setup CreateEnvelopeKey"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#wpm-setup-registerenvelopekey","text":"","title":"wpm setup RegisterEnvelopeKey"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#wpm-setup-download_ca_cert-force","text":"- Download CMS root CA certificate - Option [--force] overwrites any existing files, and always downloads new root CA cert - Environment variable CMS_BASE_URL= for CMS API url","title":"wpm setup download_ca_cert [--force]"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/33Intel%20Security%20Libraries%20Configuration%20Settings9/#wpm-setup-download_cert-flavor-signing-force","text":"- Generates Key pair and CSR, gets it signed from CMS - Option [--force] overwrites any existing files, and always downloads newly signed Flavor Signing cert - Environment variable CMS_BASE_URL=<url> for CMS API url - Environment variable BEARER_TOKEN=<token> for authenticating with CMS - Environment variable KEY_PATH=<key_path> to override default specified in config - Environment variable CERT_PATH=<cert_path> to override default specified in config - Environment variable WPM_FLAVOR_SIGN_CERT_CN=<COMMON NAME> to override default specified in config - Environment variable WPM_CERT_ORG=<CERTIFICATE ORGANIZATION> to override default specified in config - Environment variable WPM_CERT_COUNTRY=<CERTIFICATE COUNTRY> to override default specified in config - Environment variable WPM_CERT_LOCALITY=<CERTIFICATE LOCALITY> to override default specified in config - Environment variable WPM_CERT_PROVINCE=<CERTIFICATE PROVINCE> to override default specified in config","title":"wpm setup download_cert Flavor-Signing [--force]"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/34Certificate%20and%20Key%20Management/","text":"Certificate and Key Management Host Verification Service Certificates and Keys The Host Verification Service has several unique certificates not present on other services. SAML The SAML Certificate a is used to sign SAML attestation reports, and is itself signed by the Root Certificate. This certificate is unique to the Verification Service. /opt/hvs/configuration/saml.crt /opt/hvs/configuration/saml.crt.pem /opt/hvs/configuration/SAML.jks The SAML Certificate can be replaced with a user-specified keypair and certificate chain using the following command: hvs replace-saml-key-pair --private-key=new.key.pem --cert-chain=new.cert-chain.pem This will: Replace key pair in /opt/hvs/configuration/SAML.jks , alias samlkey1 Update /opt/hvs/configuration/saml.crt with saml DER public key cert Update /opt/hvs/configuration/saml.crt.pem with saml PEM public key cert Update configuration properties: saml.key.password to null saml.certificate.dn saml.issuer When the SAML certificate is replaced, all hosts will immediately be added to a queue to generate a new attestation report, since the old signing certificate is no longer valid. No service restart is necessary. If the Integration Hub is being used, the new SAML certificate will need to be imported to the Hub. Asset Tag The Asset tag Certificate is used to sign all Asset Tag Certificates. This certificate is unique to the Verification Service. /opt/hvs/configuration/tag-cacerts.pem The Asset Tag Certificate can be replaced with a user-specified keypair and certificate chain using the following command: hvs replace-tag-key-pair --private-key=new.key.pem --cert-chain=new.cert-chain.pem This will: Replace key pair in database table mw_file (cakey is private and public key pem formatted, cacerts is cert chain) Update /opt/hvs/configuration/tag-cacerts.pem with cert chain Update configuration properties: tag.issuer.dn No service restart is needed. However, all existing Asset Tags will be considered invalid, and will need to be recreated. It is recommended to delete any existing Asset Tag certificates and Flavors, and then recreate and deploy new Tags. Privacy CA The Privacy CA certificate is used as part of the certificate chain for creating the Attestation Identity Key (AIK) during Trust Agent provisioning. The Privacy CA must be a self-signed certificate. This certificate is unique to the Verification Service. The Privacy CA certificate is used by Trust Agent nodes during Trust Agent provisioning; if the Privacy CA certificate is changed, all Trust Agent nodes will need to be re-provisioned. /opt/hvs/configuration/PrivacyCA.p12 /opt/hvs/configuration/PrivacyCA.pem The Privacy CA Certificate can be replaced with a user-specified keypair and certificate chain using the following command: hvs replace-pca-key-pair --private-key=new.key.pem --cert-chain=new.cert-chain.pem This will: Replace key pair in /opt/hvs/configuration/PrivacyCA.p12 , alias 1 Update /opt/hvs/configuration/PrivacyCA.pem with cert Update configuration properties: hvs.privacyca.aik.issuer hvs.privacyca.aik.validity.days After the Privacy CA certificate is replaced, all Trust Agent hosts will need to be re-provisioned with a new AIK: tagent setup download-mtwilson-privacy-ca-certificate --force tagent setup request-aik-certificate --force tagent restart Endorsement CA The Endorsement CA is a self-signed certificate used during Trust Agent provisioning. /opt/hvs/configuration/EndorsementCA.p12 /opt/hvs/configuration/EndorsementCA.pem The Endorsement CA Certificate can be replaced with a user-specified keypair and certificate chain using the following command: hvs replace-eca-key-pair --private-key=new.key.pem --cert-chain=new.cert-chain.pem This will: Replace key pair in /opt/hvs/configuration/EndorsementCA.p12 , alias 1 Update /opt/hvs/configuration/EndorsementCA.pem with accepted ECs Update configuration properties: hvs.privacyca.ek.issuer hvs.privacyca.ek.validity.days After the Endorsement CA certificate is replaced, all Trust Agent hosts will need to be re-provisioned with a new Endorsement Certificate: tagent setup request-endorsement-certificate --force tagent restart TLS Certificates TLS certificates for each service are issued by the Certificate Management Service during installation. If the CMS root certificate is changed, or to regenerate the TLS certificate for a given service, use the following commands (note: environment variables will need to be set; typically these are the same variables set in the service installation .env file): <servicename> download_ca_cert Download CMS root CA certificate Environment variable CMS_BASE_URL= for CMS API url <servicename> download_cert TLS Generates Key pair and CSR, gets it signed from CMS Environment variable CMS_BASE_URL= for CMS API url Environment variable BEARER_TOKEN= for authenticating with CMS Environment variable KEY_PATH= to override default specified in config Environment variable CERT_PATH= to override default specified in config","title":"Certificate and Key Management"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/34Certificate%20and%20Key%20Management/#certificate-and-key-management","text":"","title":"Certificate and Key Management"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/34Certificate%20and%20Key%20Management/#host-verification-service-certificates-and-keys","text":"The Host Verification Service has several unique certificates not present on other services.","title":"Host Verification Service Certificates and Keys"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/34Certificate%20and%20Key%20Management/#saml","text":"The SAML Certificate a is used to sign SAML attestation reports, and is itself signed by the Root Certificate. This certificate is unique to the Verification Service. /opt/hvs/configuration/saml.crt /opt/hvs/configuration/saml.crt.pem /opt/hvs/configuration/SAML.jks The SAML Certificate can be replaced with a user-specified keypair and certificate chain using the following command: hvs replace-saml-key-pair --private-key=new.key.pem --cert-chain=new.cert-chain.pem This will: Replace key pair in /opt/hvs/configuration/SAML.jks , alias samlkey1 Update /opt/hvs/configuration/saml.crt with saml DER public key cert Update /opt/hvs/configuration/saml.crt.pem with saml PEM public key cert Update configuration properties: saml.key.password to null saml.certificate.dn saml.issuer When the SAML certificate is replaced, all hosts will immediately be added to a queue to generate a new attestation report, since the old signing certificate is no longer valid. No service restart is necessary. If the Integration Hub is being used, the new SAML certificate will need to be imported to the Hub.","title":"SAML"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/34Certificate%20and%20Key%20Management/#asset-tag","text":"The Asset tag Certificate is used to sign all Asset Tag Certificates. This certificate is unique to the Verification Service. /opt/hvs/configuration/tag-cacerts.pem The Asset Tag Certificate can be replaced with a user-specified keypair and certificate chain using the following command: hvs replace-tag-key-pair --private-key=new.key.pem --cert-chain=new.cert-chain.pem This will: Replace key pair in database table mw_file (cakey is private and public key pem formatted, cacerts is cert chain) Update /opt/hvs/configuration/tag-cacerts.pem with cert chain Update configuration properties: tag.issuer.dn No service restart is needed. However, all existing Asset Tags will be considered invalid, and will need to be recreated. It is recommended to delete any existing Asset Tag certificates and Flavors, and then recreate and deploy new Tags.","title":"Asset Tag"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/34Certificate%20and%20Key%20Management/#privacy-ca","text":"The Privacy CA certificate is used as part of the certificate chain for creating the Attestation Identity Key (AIK) during Trust Agent provisioning. The Privacy CA must be a self-signed certificate. This certificate is unique to the Verification Service. The Privacy CA certificate is used by Trust Agent nodes during Trust Agent provisioning; if the Privacy CA certificate is changed, all Trust Agent nodes will need to be re-provisioned. /opt/hvs/configuration/PrivacyCA.p12 /opt/hvs/configuration/PrivacyCA.pem The Privacy CA Certificate can be replaced with a user-specified keypair and certificate chain using the following command: hvs replace-pca-key-pair --private-key=new.key.pem --cert-chain=new.cert-chain.pem This will: Replace key pair in /opt/hvs/configuration/PrivacyCA.p12 , alias 1 Update /opt/hvs/configuration/PrivacyCA.pem with cert Update configuration properties: hvs.privacyca.aik.issuer hvs.privacyca.aik.validity.days After the Privacy CA certificate is replaced, all Trust Agent hosts will need to be re-provisioned with a new AIK: tagent setup download-mtwilson-privacy-ca-certificate --force tagent setup request-aik-certificate --force tagent restart","title":"Privacy CA"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/34Certificate%20and%20Key%20Management/#endorsement-ca","text":"The Endorsement CA is a self-signed certificate used during Trust Agent provisioning. /opt/hvs/configuration/EndorsementCA.p12 /opt/hvs/configuration/EndorsementCA.pem The Endorsement CA Certificate can be replaced with a user-specified keypair and certificate chain using the following command: hvs replace-eca-key-pair --private-key=new.key.pem --cert-chain=new.cert-chain.pem This will: Replace key pair in /opt/hvs/configuration/EndorsementCA.p12 , alias 1 Update /opt/hvs/configuration/EndorsementCA.pem with accepted ECs Update configuration properties: hvs.privacyca.ek.issuer hvs.privacyca.ek.validity.days After the Endorsement CA certificate is replaced, all Trust Agent hosts will need to be re-provisioned with a new Endorsement Certificate: tagent setup request-endorsement-certificate --force tagent restart","title":"Endorsement CA"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/34Certificate%20and%20Key%20Management/#tls-certificates","text":"TLS certificates for each service are issued by the Certificate Management Service during installation. If the CMS root certificate is changed, or to regenerate the TLS certificate for a given service, use the following commands (note: environment variables will need to be set; typically these are the same variables set in the service installation .env file): <servicename> download_ca_cert Download CMS root CA certificate Environment variable CMS_BASE_URL= for CMS API url <servicename> download_cert TLS Generates Key pair and CSR, gets it signed from CMS Environment variable CMS_BASE_URL= for CMS API url Environment variable BEARER_TOKEN= for authenticating with CMS Environment variable KEY_PATH= to override default specified in config Environment variable CERT_PATH= to override default specified in config","title":"TLS Certificates"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/35Upgrades/","text":"Upgrades Note Before performing any upgrade, Intel strongly recommends backing up the database for the HVS, WLS, and AAS. See Postgres documentation for detailed options for backing up databases. Below is a sample method for backing up an entire database server: Backup to tar file: pg_dump --dbname <database_name> --username=<database username> -F -t > <database_backup_file>.tar Restore from tar file: pg_restore --dbname=<database_name> --username=<database username><database_backup_file>.tar Some upgrades may involve changes to database content, and a backup will ensure that data is not lost in the case of an error during the upgrade process. Backward Compatibility In general Intel SecL services are made to be backward-compatible within a given major release (for example, the 3.6 HVS should be compatible with the 3.5 Trust Agent) in an upgrade priority order (see below). Major version upgrades may require coordinated upgrades across all services. Upgrade Order Upgrades should be performed in the following order to prevent misconfiguration or any service unavailability: 1) CMS, AAS 2) HVS 3) WLS, IHUB 4) KBS, Trust Agents, Workload Agents, WPM Upgrading in this order will make each service unavailable only for the duration of the upgrade for that service. Upgrade Process Binary Installations For services installed directly (not deployed as containers), the upgrade process simply requires executing the new-version installer on the same machine where the old-version is running. The installer will re-use the same configuration elements detected in the existing version's config file. No additional answer file is required unless configuration settings will change.","title":"Upgrades"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/35Upgrades/#upgrades","text":"Note Before performing any upgrade, Intel strongly recommends backing up the database for the HVS, WLS, and AAS. See Postgres documentation for detailed options for backing up databases. Below is a sample method for backing up an entire database server: Backup to tar file: pg_dump --dbname <database_name> --username=<database username> -F -t > <database_backup_file>.tar Restore from tar file: pg_restore --dbname=<database_name> --username=<database username><database_backup_file>.tar Some upgrades may involve changes to database content, and a backup will ensure that data is not lost in the case of an error during the upgrade process.","title":"Upgrades"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/35Upgrades/#backward-compatibility","text":"In general Intel SecL services are made to be backward-compatible within a given major release (for example, the 3.6 HVS should be compatible with the 3.5 Trust Agent) in an upgrade priority order (see below). Major version upgrades may require coordinated upgrades across all services.","title":"Backward Compatibility"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/35Upgrades/#upgrade-order","text":"Upgrades should be performed in the following order to prevent misconfiguration or any service unavailability: 1) CMS, AAS 2) HVS 3) WLS, IHUB 4) KBS, Trust Agents, Workload Agents, WPM Upgrading in this order will make each service unavailable only for the duration of the upgrade for that service.","title":"Upgrade Order"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/35Upgrades/#upgrade-process","text":"","title":"Upgrade Process"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/35Upgrades/#binary-installations","text":"For services installed directly (not deployed as containers), the upgrade process simply requires executing the new-version installer on the same machine where the old-version is running. The installer will re-use the same configuration elements detected in the existing version's config file. No additional answer file is required unless configuration settings will change.","title":"Binary Installations"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/36Appendix/","text":"Appendix PCR Definitions Red Had Enterprise Linux TPM 2.0 PCR Measurement Parameters Description Operating System PCR 0 BIOS ROM and Flash Image Initial Boot Block (Intel\u00ae BootGuard only) This PCR is based solely on the BIOS version, and remains identical across all hosts using the same BIOS. This PCR is used as the PLATFORM Flavor. (Intel\u00ae BootGuard only): Extends measurements based on the Intel\u00ae BootGuard profile configuration and production vs non-production ACM flags; ACM signature; BootGuard key manifest hash; Boot Policy Manifest Signature All PCR 7 Intel\u00ae BootGuard configuration and profiles Describes the success of the IBB measurement event. All (Intel\u00ae BootGuard only) PCR 17 ACM BIOS AC registration information Digest of Processor S-CRTM Digest of Policycontrol Digest of all matching elements used by the policy Digest of STM Digest of Capability field of OsSinitData Digest of MLE For TA hosts, this PCR includes measurements of the OS, InitRD, and UUID. This changes with every install due to InitRD and UUID change. VMware ESXi Red Hat Enterprise Linux PCR 18 MLE [Tboot +VMM] Digest of public key modulus used to verify SINIT signature Digest of Processor S-CRTM Digest of Capability field of OSSinitData table Digest of PolicyControl field of used policy Digest of LCP VMware ESXi Red Hat Enterprise Linux PCR 19 OS Specific. ESX and Trust Agent \u2014 non Kernel modules Citrix Xen \u2014 OS + Init RD + UUID For ESXi and Trust Agent hosts, this PCR contains individual measurements of all of the non-Kernel modules. For Linux hosts, this PCR is a measurement of the OS, InitRD, and UUID. VMware ESXi Red Hat Enterprise Linux VMWare ESXi TPM 1.2 PCR Measurement Parameters Description Operating System PCR 0 BIOS ROM and Flash Image This PCR is based solely on the BIOS version, and remains identical across all hosts using the same BIOS. This PCR is used as the PLATFORM Flavor. All PCR 17 ACM This PCR measures the SINIT ACM, and is hardware platform-specific. This PCR is part of the PLATFORM Flavor. VMware ESXi Red Hat Enterprise Linux PCR 18 MLE [Tboot +VMM] This PCR measures the tboot and hypervisor version. In ESXi hosts, only the tboot version is measured. VMware ESXi Red Hat Enterprise Linux PCR 19 OS Specific. ESX and Trust Agent \u2014 non Kernel modules Citrix Xen \u2014 OS + Init RD + UUID For ESXi and Trust Agent hosts, this PCR contains individual measurements of all of the non-Kernel modules. For Citrix Xen hosts, this PCR is a measurement of the OS, InitRD, and UUID. VMware ESXi Red Hat Enterprise Linux PCR 20 For ESXi only. VM Kernel and VMK Boot This PCR is used only by ESXi hosts and is blank for all other host types. VMware ESXi PCR 22 Asset Tag This PCR contains the measurement of the SHA1 of the Asset Tag Certificate provisioned to the TPM, if any. VMware ESXi TPM 2.0 VMWare supports TPM 2.0 with Intel TXT starting in vSphere 6.7 Update 1. Earlier versions will support TPM 1.2 only. PCR Measurement Parameters Description Operating System PCR 0 BIOS ROM and Flash Image This PCR is based solely on the BIOS version, and remains identical across all hosts using the same BIOS. This PCR is used as part of the PLATFORM flavor. All PCR 17 ACM This PCR measures the SINIT ACM, and is hardware platform-specific. This PCR is part of the PLATFORM Flavor. VMware ESXi Red Hat Enterprise Linux PCR 18 MLE [Tboot +VMM] This PCR measures the tboot and hypervisor version. In ESXi hosts, only the tboot version is measured. This PCR is part of the PLATFORM Flavor. VMware ESXi Red Hat Enterprise Linux PCR 19 OS Specific. ESX and Trust Agent \u2014 non Kernel modules Citrix Xen \u2014 OS + Init RD + UUID For ESXi this PCR contains individual measurements of all of the non-Kernel modules \u2013 this includes all of the VIBs installed on the ESXi host. This is part of the OS flavor. Note that two ESXi hosts with the same version of ESXi installed may require different OS flavors if different VIBs are installed. VMware ESXi Red Hat Enterprise Linux PCR 20 For ESXi only. VM Kernel and VMK Boot This PCR is used only by ESXi hosts for some host-specific measurements, and is part of the host-unique flavor. VMware ESXi PCR 22 Asset Tag Asset Tag is not currently supported for TPM 2.0 with ESXi. VMware ESXi Attestation Rules Platform TPM Flavor Type Rules to be verified Comments RHEL 2.0 HARDWARE PcrMatchesConstant rule for PCR 0 PcrEventLogIncludes rule for PCR 17 (LCP_DETAILS_HASH, BIOSAC_REG_DATA, OSSINITDATA_CAP_HASH, STM_HASH, MLE_HASH, NV_INFO_HASH, tb_policy, CPU_SCRTM_STAT, HASH_START, LCP_CONTROL_HASH) PcrEventLogIntegrity rule for PCR 17 Evaluation of PcrEventLogIncludes would not include initrd and vmlinuz modules. They would be handled in host_specific flavor. Evaluation of PcrEventLogIntegrity rule would also include OS modules (initrd & vmlinuz) OS PcrEventLogIntegrity rule for PCR 17 ASSET_TAG AssetTagMatches rule HOST_SPECIFIC PcrEventLogIncludes rule for PCR 17 (initrd & vmlinuz) VMware ESXi 1.2 PLATFORM PcrMatchesConstant rule for PCR 0 PcrMatchesConstant rule for PCR 17 OS PcrMatchesConstant rule for PCR 18 PcrMatchesConstant rule for PCR 20 PcrEventLogEqualsExcluding rule for PCR 19 (excludes dynamic modules based on component name) PcrEventLogIntegrity rule for PCR 19 ASSET_TAG PcrMatchesConstant rule for PCR 22 VMware ESXi 2.0 NOT SUPPORTED Windows 1.2 PLATFORM PcrMatchesConstant rule for PCR 0 OS PcrMatchesConstant rule for PCR 13 PcrMatchesConstant rule for PCR 14 ASSET_TAG AssetTagMatches rule Windows 2.0 PLATFORM PcrMatchesConstant rule for PCR 0 OS PcrMatchesConstant rule for PCR 13 PcrMatchesConstant rule for PCR 14 ASSET_TAG AssetTagMatches rule AssetTagMatches rule needs to be updated to verify the key-value pairs after verifying the tag certificate.","title":"Appendix"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/36Appendix/#appendix","text":"","title":"Appendix"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/36Appendix/#pcr-definitions","text":"","title":"PCR Definitions"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/36Appendix/#red-had-enterprise-linux","text":"","title":"Red Had Enterprise Linux"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/36Appendix/#tpm-20","text":"PCR Measurement Parameters Description Operating System PCR 0 BIOS ROM and Flash Image Initial Boot Block (Intel\u00ae BootGuard only) This PCR is based solely on the BIOS version, and remains identical across all hosts using the same BIOS. This PCR is used as the PLATFORM Flavor. (Intel\u00ae BootGuard only): Extends measurements based on the Intel\u00ae BootGuard profile configuration and production vs non-production ACM flags; ACM signature; BootGuard key manifest hash; Boot Policy Manifest Signature All PCR 7 Intel\u00ae BootGuard configuration and profiles Describes the success of the IBB measurement event. All (Intel\u00ae BootGuard only) PCR 17 ACM BIOS AC registration information Digest of Processor S-CRTM Digest of Policycontrol Digest of all matching elements used by the policy Digest of STM Digest of Capability field of OsSinitData Digest of MLE For TA hosts, this PCR includes measurements of the OS, InitRD, and UUID. This changes with every install due to InitRD and UUID change. VMware ESXi Red Hat Enterprise Linux PCR 18 MLE [Tboot +VMM] Digest of public key modulus used to verify SINIT signature Digest of Processor S-CRTM Digest of Capability field of OSSinitData table Digest of PolicyControl field of used policy Digest of LCP VMware ESXi Red Hat Enterprise Linux PCR 19 OS Specific. ESX and Trust Agent \u2014 non Kernel modules Citrix Xen \u2014 OS + Init RD + UUID For ESXi and Trust Agent hosts, this PCR contains individual measurements of all of the non-Kernel modules. For Linux hosts, this PCR is a measurement of the OS, InitRD, and UUID. VMware ESXi Red Hat Enterprise Linux","title":"TPM 2.0"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/36Appendix/#vmware-esxi","text":"","title":"VMWare ESXi"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/36Appendix/#tpm-12","text":"PCR Measurement Parameters Description Operating System PCR 0 BIOS ROM and Flash Image This PCR is based solely on the BIOS version, and remains identical across all hosts using the same BIOS. This PCR is used as the PLATFORM Flavor. All PCR 17 ACM This PCR measures the SINIT ACM, and is hardware platform-specific. This PCR is part of the PLATFORM Flavor. VMware ESXi Red Hat Enterprise Linux PCR 18 MLE [Tboot +VMM] This PCR measures the tboot and hypervisor version. In ESXi hosts, only the tboot version is measured. VMware ESXi Red Hat Enterprise Linux PCR 19 OS Specific. ESX and Trust Agent \u2014 non Kernel modules Citrix Xen \u2014 OS + Init RD + UUID For ESXi and Trust Agent hosts, this PCR contains individual measurements of all of the non-Kernel modules. For Citrix Xen hosts, this PCR is a measurement of the OS, InitRD, and UUID. VMware ESXi Red Hat Enterprise Linux PCR 20 For ESXi only. VM Kernel and VMK Boot This PCR is used only by ESXi hosts and is blank for all other host types. VMware ESXi PCR 22 Asset Tag This PCR contains the measurement of the SHA1 of the Asset Tag Certificate provisioned to the TPM, if any. VMware ESXi","title":"TPM 1.2"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/36Appendix/#tpm-20_1","text":"VMWare supports TPM 2.0 with Intel TXT starting in vSphere 6.7 Update 1. Earlier versions will support TPM 1.2 only. PCR Measurement Parameters Description Operating System PCR 0 BIOS ROM and Flash Image This PCR is based solely on the BIOS version, and remains identical across all hosts using the same BIOS. This PCR is used as part of the PLATFORM flavor. All PCR 17 ACM This PCR measures the SINIT ACM, and is hardware platform-specific. This PCR is part of the PLATFORM Flavor. VMware ESXi Red Hat Enterprise Linux PCR 18 MLE [Tboot +VMM] This PCR measures the tboot and hypervisor version. In ESXi hosts, only the tboot version is measured. This PCR is part of the PLATFORM Flavor. VMware ESXi Red Hat Enterprise Linux PCR 19 OS Specific. ESX and Trust Agent \u2014 non Kernel modules Citrix Xen \u2014 OS + Init RD + UUID For ESXi this PCR contains individual measurements of all of the non-Kernel modules \u2013 this includes all of the VIBs installed on the ESXi host. This is part of the OS flavor. Note that two ESXi hosts with the same version of ESXi installed may require different OS flavors if different VIBs are installed. VMware ESXi Red Hat Enterprise Linux PCR 20 For ESXi only. VM Kernel and VMK Boot This PCR is used only by ESXi hosts for some host-specific measurements, and is part of the host-unique flavor. VMware ESXi PCR 22 Asset Tag Asset Tag is not currently supported for TPM 2.0 with ESXi. VMware ESXi","title":"TPM 2.0"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/36Appendix/#attestation-rules","text":"Platform TPM Flavor Type Rules to be verified Comments RHEL 2.0 HARDWARE PcrMatchesConstant rule for PCR 0 PcrEventLogIncludes rule for PCR 17 (LCP_DETAILS_HASH, BIOSAC_REG_DATA, OSSINITDATA_CAP_HASH, STM_HASH, MLE_HASH, NV_INFO_HASH, tb_policy, CPU_SCRTM_STAT, HASH_START, LCP_CONTROL_HASH) PcrEventLogIntegrity rule for PCR 17 Evaluation of PcrEventLogIncludes would not include initrd and vmlinuz modules. They would be handled in host_specific flavor. Evaluation of PcrEventLogIntegrity rule would also include OS modules (initrd & vmlinuz) OS PcrEventLogIntegrity rule for PCR 17 ASSET_TAG AssetTagMatches rule HOST_SPECIFIC PcrEventLogIncludes rule for PCR 17 (initrd & vmlinuz) VMware ESXi 1.2 PLATFORM PcrMatchesConstant rule for PCR 0 PcrMatchesConstant rule for PCR 17 OS PcrMatchesConstant rule for PCR 18 PcrMatchesConstant rule for PCR 20 PcrEventLogEqualsExcluding rule for PCR 19 (excludes dynamic modules based on component name) PcrEventLogIntegrity rule for PCR 19 ASSET_TAG PcrMatchesConstant rule for PCR 22 VMware ESXi 2.0 NOT SUPPORTED Windows 1.2 PLATFORM PcrMatchesConstant rule for PCR 0 OS PcrMatchesConstant rule for PCR 13 PcrMatchesConstant rule for PCR 14 ASSET_TAG AssetTagMatches rule Windows 2.0 PLATFORM PcrMatchesConstant rule for PCR 0 OS PcrMatchesConstant rule for PCR 13 PcrMatchesConstant rule for PCR 14 ASSET_TAG AssetTagMatches rule AssetTagMatches rule needs to be updated to verify the key-value pairs after verifying the tag certificate.","title":"Attestation Rules"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/","text":"Introduction Overview Intel Security Libraries for Datacenter is a collection of software applications and development libraries intended to help turn Intel platform security features into real-world security use cases. Trusted Computing Trusted Computing consists of a set of industry standards defined by the Trusted Computing Group to harden systems and data against attack. These standards include verifying platform integrity, establishing identity, protection of keys and secrets, and more. One of the functions of Intel Security Libraries is to provide a \u201cTrusted Platform,\u201d using Intel security technologies to add visibility, auditability, and control to server platforms. The Chain of Trust In a Trusted Computing environment, a key concept is verification of the integrity of the underlying platform. Verifying platform integrity typically means cryptographic measurement and/or verification of firmware and software components. The process by which this measurement and verification takes place affects the overall strength of the assertion that the measured and verified components have not been altered. Intel refers to this process as the \u201c Chain of Trust ,\u201d whereby at boot time, a sequence of cryptographic measurements and signature verification events happen in a defined order, such that measurement/verification happens before execution, and each entity responsible for performing a measurement or verification is measured by another step earlier in the process. Any break in this chain leads to an opportunity for an attacker to modify code and evade detection. Hardware Root of Trust The Root of Trust, the first link in the chain, can be one of several different options. Anything that happens in the boot process before the Root of Trust must be considered to be within the \u201ctrust boundary,\u201d signifying components whose trustworthiness cannot be assessed. For this reason, it\u2019s best to use a Root of Trust that starts as early in the system boot process as possible, so that the Chain of Trust during the boot process can cover as much as possible. Multiple Root of Trust options exist, ranging from firmware to hardware. In general, a hardware Root of Trust will have a smaller \u201ctrust boundary\u201d than a firmware Root of Trust. A hardware Root of Trust will also have the benefit of immutability \u2013 where firmware can easily be flashed and modified, hardware is much more difficult to tamper with. Intel\u00ae Trusted Execution Technology (Intel\u00ae TXT) Intel\u00ae Trusted Execution Technology is a hardware Root of Trust feature available on Intel\u00ae server platforms starting with the Grantley generation. Intel\u00ae TXT is enabled in the system BIOS (typically under the Processor > Advanced tab), and requires Intel\u00ae VT-d and Intel VT-x features to be enabled as prerequisites (otherwise the option will be grayed out). Intel\u00ae TXT will ship \u201cdisabled\u201d by default. Intel\u00ae BootGuard (Intel\u00ae BtG) Intel\u00ae BootGuard is a hardware Root of Trust feature available on Intel\u00ae server platforms starting with the Purley-Refresh generation. Unlike Intel\u00ae TXT, Intel\u00ae BtG is configured in platform fuses, not in the system BIOS. Intel\u00ae BtG is fused into several \u201cprofiles\u201d that determine the behavior of the feature. Intel\u00ae BtG supports both \u201cverify\u201d and \u201cmeasure\u201d profiles; in \u201cverify\u201d profiles, Intel\u00ae BtG will verify the signature of the platform Initial Boot Block (IBB). In \u201cmeasure\u201dprofiles, Intel\u00ae BtG will hash the IBB and extend that measurement to a TPM PCR. It is recommended that Intel\u00ae BtG be fused into the \u201cmeasure and verify\u201d profile for maximum protection and auditability. Because the Intel\u00ae BtG profile is configured using fuses, the server OEM/ODM will determine the profile used at manufacturing time. Please contact your server vendor to determine what Intel\u00ae BtG profiles are available in their product line. Because Intel\u00ae BtG only measures/verifies the integrity of the IBB, it\u2019s important to have an additional technology handle measurements later in the boot process. Intel\u00ae TXT can provide this function using tboot to invoke SINIT, and UEFI SecureBoot can alternatively provide similar functionality (note that Linux users should properly configure Shim and use a signed kernel for UEFI SecureBoot). Supported Trusted Boot Options Intel\u00ae SecL-DC supports several options for Trusted Computing, depending on the features available on the platform. Remote Attestation Trusted computing consists primarily of two activities \u2013 measurement, and attestation. Measurement is the act of obtaining cryptographic representations for the system state. Attestation is the act of comparing those cryptographic measurements against expected values to determine whether the system booted into an acceptable state. Attestation can be performed either locally, on the same host that is to be attested, or remotely, by an external authority. The trusted boot process can optionally include a local attestation involving the evaluation of a TPM-stored Launch Control Policy (LCP). In this case, the host\u2019s TPM will compare the measurements that have been taken so far to a set of expected PCR values stored in the LCP; if there is a mismatch, the boot process is halted entirely. Intel\u00ae SecL utilizes remote attestation, providing a remote Verification Service that maintains a database of expected measurements (or \u201cflavors\u201d), and compares the actual boot-time measurements from any number of hosts against its database to provide an assertion that the host booted into a \u201ctrusted\u201d or \u201cuntrusted\u201d state. Remote attestation is typically easier to centrally manage (as opposed to creating an LCP for each host and entering the policy into the host\u2019s TPM), does not halt the boot process allowing for easier remediation, and separates the attack surface into separate components that must both be compromised to bypass security controls. Both local and remote attestation can be used concurrently. However, Intel\u00ae SecL, and this document, will focus only on remote attestation. For more information on TPM Launch Control Policies, consult the Intel Trusted Execution Technology (Intel TXT) Software Development Guide . Intel\u00ae Security Libraries for Datacenter Features Platform Integrity Platform Integrity is the use case enabled by the specific implementation of the Chain of Trust and Remote Attestation concepts. This involves the use of a Root of Trust to begin an unbroken chain of platform measurements at server boot time, with measurements extended to the Trusted Platform Module and compared against expected values to verify the integrity of measured components. This use case is foundational for other Intel\u00ae SecL use cases. Data Sovereignty Data Sovereignty builds on the Platform Integrity use case to allow physical TPMs to be written with Asset Tags containing any number of key/value pairs. This use case is typically used to identify the geographic location of the physical server, but can also be used to identify other attributes. For example, the Asset Tags provided by the Data Sovereignty use case could be used to identify hosts that meet specific compliance requirements and can run controlled workloads. Application Integrity Added in the Intel\u00ae SecL-DC 1.5 release, Application Integrity allows any files and folders on a Linux host system to be included in the Chain of Trust integrity measurements. These measurements are attested by the Verification Service along with the other platform measurements, and are included in determining the host\u2019s overall Trust status. The measurements are performed by a measurement agent called tbootXM, which is built into initrd during Trust Agent installation. Because initrd is included in other Trusted Computing measurements, this allows Intel\u00ae SecL-DC to carry the Chain of Trust all the way to the Linux filesystem. Workload Confidentiality for Virtual Machines and Containers Added in the Intel\u00ae SecL-DC 1.6 release, Workload Confidentiality allows virtual machine and container images to be encrypted at rest, with key access tied to platform integrity attestation. Because security attributes contained in the platform integrity attestation report are used to control access to the decryption keys, this feature provides both protection for at-rest data, IP, code, etc in container or virtual machine images, and also enforcement of image-owner-controlled placement policies. When decryption keys are released, they are sealed to the physical TPM of the host that was attested, meaning that only a server that has successfully met the policy requirements for the image can actually gain access. Workload Confidentiality begins with the Workload Policy Manager (WPM) and a qcow2 or container image that needs to be protected. The WPM is a lightweight application that will request a new key from the Key Broker, use that key to encrypt the image, and generate an Image Flavor. The image owner will then upload the encrypted image to their desired image storage service (for example, OpenStack Glance or a local container registry), and the image ID from the image storage will be uploaded along with the Image Flavor to the Intel\u00ae SecL Workload Service. When that image is used to launch a new VM or container, the Workload Agent will intercept the VM or container start and request the decryption key for that image from the Workload Service. The Workload Service will use the image ID and the Image Flavor to find the key transfer URL for the appropriate Key Broker, and will query the Verification Service for the latest Platform Integrity trust attestation report for the host. The Key Broker will use the attestation report to determine whether the host meets the policy requirements for the key transfer, and to verify that the report is signed by a Verification Service known to the Broker. If the report is genuine and meets the policy requirements, the image decryption key is sealed using an asymmetric key from that host\u2019s TPM, and sent back to the Workload Service. The Workload Service then caches the key for 5 minutes (to avoid performance issues for multiple rapid launch requests; note that these keys are still wrapped using a sealing key unique to the hosts TPM, so multiple hosts would require multiple keys even for an identical image) and return the wrapped key to the Workload Agent on the host, which then uses the host TPM to unseal the image decryption key. The key is then used to create a new LUKS volume, and the image is decrypted into this volume. This functionality means that a physical host must pass policy requirements in order to gain access to the image key, and the image will be encrypted at rest both in image storage and on the compute host. Beginning with the Intel\u00ae SecL-DC version 2.1 release, the Key Broker now supports 3 rd -party key managers that are KMIP-compliant. The Key Broker has been updated to use the \u201cgemalto kmip-go\u201d client. Signed Flavors Added in the Intel\u00ae SecL-DC 1.6 release, Flavor signing is an improvement to the existing handling of expected attestation measurements, called \u201cFlavors.\u201d This feature adds the ability to digitally sign Flavors so that the integrity of the expected measurements themselves can be verified when attestations occur. This also means that Flavors can be more securely transferred between different Verification Service instances. Flavor signing is seamlessly added to the existing Flavor creation process (both importing from a sample host and \u201cmanually\u201d creating a Flavor using the POST method to the /v2/flavors resource). When a Flavor is created, the Verification Service will sign it using a signing certificate signed by the Certificate Management Service (this is created during Verification Service setup). Each time that the Verification Service evaluates a Flavor, it will first verify the signature on that Flavor to ensure the integrity of the Flavor contents before it is used to attest the integrity of any host. Trusted Virtual Kubernetes Worker Nodes Added in the Intel\u00ae SecL-DC version 2.1 release, this feature provides a Chain of Trust solution extending to Kubernetes Worker Nodes deployed as Virtual Machines. This feature addresses Kubernetes deployments that use Virtual Machines as Worker Nodes, rather than using bare-metal servers. When libvirt initiates a VM Start, the Intel\u00ae SecL-DC Workload Agent will create a report for the VM that associates the VM\u2019s trust status with the trust status of the host launching the VM. This VM report will be retrievable via the Workload Service, and contains the hardware UUID of the physical server hosting the VM. This UUID can be correlated to the Trust Report of that server at the time of VM launch, creating an audit trail validating that the VM launched on a trusted platform. A new report is created for every VM Start, which includes actions like VM migrations, so that each time a VM is launched or moved a new report is generated ensuring an accurate trust status. By using Platform Integrity and Data Sovereignty-based orchestration (or Workload Confidentiality with encrypted worker VMs) for the Virtual Machines to ensure that the virtual Kubernetes Worker nodes only launch on trusted hardware, these VM trust reports provide an auditing capability to extend the Chain of Trust to the virtual Worker Nodes.","title":"Introduction"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#introduction","text":"","title":"Introduction"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#overview","text":"Intel Security Libraries for Datacenter is a collection of software applications and development libraries intended to help turn Intel platform security features into real-world security use cases.","title":"Overview"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#trusted-computing","text":"Trusted Computing consists of a set of industry standards defined by the Trusted Computing Group to harden systems and data against attack. These standards include verifying platform integrity, establishing identity, protection of keys and secrets, and more. One of the functions of Intel Security Libraries is to provide a \u201cTrusted Platform,\u201d using Intel security technologies to add visibility, auditability, and control to server platforms.","title":"Trusted Computing"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#the-chain-of-trust","text":"In a Trusted Computing environment, a key concept is verification of the integrity of the underlying platform. Verifying platform integrity typically means cryptographic measurement and/or verification of firmware and software components. The process by which this measurement and verification takes place affects the overall strength of the assertion that the measured and verified components have not been altered. Intel refers to this process as the \u201c Chain of Trust ,\u201d whereby at boot time, a sequence of cryptographic measurements and signature verification events happen in a defined order, such that measurement/verification happens before execution, and each entity responsible for performing a measurement or verification is measured by another step earlier in the process. Any break in this chain leads to an opportunity for an attacker to modify code and evade detection.","title":"The Chain of Trust"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#hardware-root-of-trust","text":"The Root of Trust, the first link in the chain, can be one of several different options. Anything that happens in the boot process before the Root of Trust must be considered to be within the \u201ctrust boundary,\u201d signifying components whose trustworthiness cannot be assessed. For this reason, it\u2019s best to use a Root of Trust that starts as early in the system boot process as possible, so that the Chain of Trust during the boot process can cover as much as possible. Multiple Root of Trust options exist, ranging from firmware to hardware. In general, a hardware Root of Trust will have a smaller \u201ctrust boundary\u201d than a firmware Root of Trust. A hardware Root of Trust will also have the benefit of immutability \u2013 where firmware can easily be flashed and modified, hardware is much more difficult to tamper with.","title":"Hardware Root of Trust"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#intel-trusted-execution-technology-intel-txt","text":"Intel\u00ae Trusted Execution Technology is a hardware Root of Trust feature available on Intel\u00ae server platforms starting with the Grantley generation. Intel\u00ae TXT is enabled in the system BIOS (typically under the Processor > Advanced tab), and requires Intel\u00ae VT-d and Intel VT-x features to be enabled as prerequisites (otherwise the option will be grayed out). Intel\u00ae TXT will ship \u201cdisabled\u201d by default.","title":"Intel\u00ae Trusted Execution Technology (Intel\u00ae TXT)"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#intel-bootguard-intel-btg","text":"Intel\u00ae BootGuard is a hardware Root of Trust feature available on Intel\u00ae server platforms starting with the Purley-Refresh generation. Unlike Intel\u00ae TXT, Intel\u00ae BtG is configured in platform fuses, not in the system BIOS. Intel\u00ae BtG is fused into several \u201cprofiles\u201d that determine the behavior of the feature. Intel\u00ae BtG supports both \u201cverify\u201d and \u201cmeasure\u201d profiles; in \u201cverify\u201d profiles, Intel\u00ae BtG will verify the signature of the platform Initial Boot Block (IBB). In \u201cmeasure\u201dprofiles, Intel\u00ae BtG will hash the IBB and extend that measurement to a TPM PCR. It is recommended that Intel\u00ae BtG be fused into the \u201cmeasure and verify\u201d profile for maximum protection and auditability. Because the Intel\u00ae BtG profile is configured using fuses, the server OEM/ODM will determine the profile used at manufacturing time. Please contact your server vendor to determine what Intel\u00ae BtG profiles are available in their product line. Because Intel\u00ae BtG only measures/verifies the integrity of the IBB, it\u2019s important to have an additional technology handle measurements later in the boot process. Intel\u00ae TXT can provide this function using tboot to invoke SINIT, and UEFI SecureBoot can alternatively provide similar functionality (note that Linux users should properly configure Shim and use a signed kernel for UEFI SecureBoot).","title":"Intel\u00ae BootGuard (Intel\u00ae BtG)"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#supported-trusted-boot-options","text":"Intel\u00ae SecL-DC supports several options for Trusted Computing, depending on the features available on the platform.","title":"Supported Trusted Boot Options"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#remote-attestation","text":"Trusted computing consists primarily of two activities \u2013 measurement, and attestation. Measurement is the act of obtaining cryptographic representations for the system state. Attestation is the act of comparing those cryptographic measurements against expected values to determine whether the system booted into an acceptable state. Attestation can be performed either locally, on the same host that is to be attested, or remotely, by an external authority. The trusted boot process can optionally include a local attestation involving the evaluation of a TPM-stored Launch Control Policy (LCP). In this case, the host\u2019s TPM will compare the measurements that have been taken so far to a set of expected PCR values stored in the LCP; if there is a mismatch, the boot process is halted entirely. Intel\u00ae SecL utilizes remote attestation, providing a remote Verification Service that maintains a database of expected measurements (or \u201cflavors\u201d), and compares the actual boot-time measurements from any number of hosts against its database to provide an assertion that the host booted into a \u201ctrusted\u201d or \u201cuntrusted\u201d state. Remote attestation is typically easier to centrally manage (as opposed to creating an LCP for each host and entering the policy into the host\u2019s TPM), does not halt the boot process allowing for easier remediation, and separates the attack surface into separate components that must both be compromised to bypass security controls. Both local and remote attestation can be used concurrently. However, Intel\u00ae SecL, and this document, will focus only on remote attestation. For more information on TPM Launch Control Policies, consult the Intel Trusted Execution Technology (Intel TXT) Software Development Guide .","title":"Remote Attestation"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#intel-security-libraries-for-datacenter-features","text":"","title":"Intel\u00ae Security Libraries for Datacenter Features"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#platform-integrity","text":"Platform Integrity is the use case enabled by the specific implementation of the Chain of Trust and Remote Attestation concepts. This involves the use of a Root of Trust to begin an unbroken chain of platform measurements at server boot time, with measurements extended to the Trusted Platform Module and compared against expected values to verify the integrity of measured components. This use case is foundational for other Intel\u00ae SecL use cases.","title":"Platform Integrity"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#data-sovereignty","text":"Data Sovereignty builds on the Platform Integrity use case to allow physical TPMs to be written with Asset Tags containing any number of key/value pairs. This use case is typically used to identify the geographic location of the physical server, but can also be used to identify other attributes. For example, the Asset Tags provided by the Data Sovereignty use case could be used to identify hosts that meet specific compliance requirements and can run controlled workloads.","title":"Data Sovereignty"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#application-integrity","text":"Added in the Intel\u00ae SecL-DC 1.5 release, Application Integrity allows any files and folders on a Linux host system to be included in the Chain of Trust integrity measurements. These measurements are attested by the Verification Service along with the other platform measurements, and are included in determining the host\u2019s overall Trust status. The measurements are performed by a measurement agent called tbootXM, which is built into initrd during Trust Agent installation. Because initrd is included in other Trusted Computing measurements, this allows Intel\u00ae SecL-DC to carry the Chain of Trust all the way to the Linux filesystem.","title":"Application Integrity"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#workload-confidentiality-for-virtual-machines-and-containers","text":"Added in the Intel\u00ae SecL-DC 1.6 release, Workload Confidentiality allows virtual machine and container images to be encrypted at rest, with key access tied to platform integrity attestation. Because security attributes contained in the platform integrity attestation report are used to control access to the decryption keys, this feature provides both protection for at-rest data, IP, code, etc in container or virtual machine images, and also enforcement of image-owner-controlled placement policies. When decryption keys are released, they are sealed to the physical TPM of the host that was attested, meaning that only a server that has successfully met the policy requirements for the image can actually gain access. Workload Confidentiality begins with the Workload Policy Manager (WPM) and a qcow2 or container image that needs to be protected. The WPM is a lightweight application that will request a new key from the Key Broker, use that key to encrypt the image, and generate an Image Flavor. The image owner will then upload the encrypted image to their desired image storage service (for example, OpenStack Glance or a local container registry), and the image ID from the image storage will be uploaded along with the Image Flavor to the Intel\u00ae SecL Workload Service. When that image is used to launch a new VM or container, the Workload Agent will intercept the VM or container start and request the decryption key for that image from the Workload Service. The Workload Service will use the image ID and the Image Flavor to find the key transfer URL for the appropriate Key Broker, and will query the Verification Service for the latest Platform Integrity trust attestation report for the host. The Key Broker will use the attestation report to determine whether the host meets the policy requirements for the key transfer, and to verify that the report is signed by a Verification Service known to the Broker. If the report is genuine and meets the policy requirements, the image decryption key is sealed using an asymmetric key from that host\u2019s TPM, and sent back to the Workload Service. The Workload Service then caches the key for 5 minutes (to avoid performance issues for multiple rapid launch requests; note that these keys are still wrapped using a sealing key unique to the hosts TPM, so multiple hosts would require multiple keys even for an identical image) and return the wrapped key to the Workload Agent on the host, which then uses the host TPM to unseal the image decryption key. The key is then used to create a new LUKS volume, and the image is decrypted into this volume. This functionality means that a physical host must pass policy requirements in order to gain access to the image key, and the image will be encrypted at rest both in image storage and on the compute host. Beginning with the Intel\u00ae SecL-DC version 2.1 release, the Key Broker now supports 3 rd -party key managers that are KMIP-compliant. The Key Broker has been updated to use the \u201cgemalto kmip-go\u201d client.","title":"Workload Confidentiality for Virtual Machines and Containers"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#signed-flavors","text":"Added in the Intel\u00ae SecL-DC 1.6 release, Flavor signing is an improvement to the existing handling of expected attestation measurements, called \u201cFlavors.\u201d This feature adds the ability to digitally sign Flavors so that the integrity of the expected measurements themselves can be verified when attestations occur. This also means that Flavors can be more securely transferred between different Verification Service instances. Flavor signing is seamlessly added to the existing Flavor creation process (both importing from a sample host and \u201cmanually\u201d creating a Flavor using the POST method to the /v2/flavors resource). When a Flavor is created, the Verification Service will sign it using a signing certificate signed by the Certificate Management Service (this is created during Verification Service setup). Each time that the Verification Service evaluates a Flavor, it will first verify the signature on that Flavor to ensure the integrity of the Flavor contents before it is used to attest the integrity of any host.","title":"Signed Flavors"},{"location":"product-guides/Foundational%20%26%20Workload%20Security/Introduction/#trusted-virtual-kubernetes-worker-nodes","text":"Added in the Intel\u00ae SecL-DC version 2.1 release, this feature provides a Chain of Trust solution extending to Kubernetes Worker Nodes deployed as Virtual Machines. This feature addresses Kubernetes deployments that use Virtual Machines as Worker Nodes, rather than using bare-metal servers. When libvirt initiates a VM Start, the Intel\u00ae SecL-DC Workload Agent will create a report for the VM that associates the VM\u2019s trust status with the trust status of the host launching the VM. This VM report will be retrievable via the Workload Service, and contains the hardware UUID of the physical server hosting the VM. This UUID can be correlated to the Trust Report of that server at the time of VM launch, creating an audit trail validating that the VM launched on a trusted platform. A new report is created for every VM Start, which includes actions like VM migrations, so that each time a VM is launched or moved a new report is generated ensuring an accurate trust status. By using Platform Integrity and Data Sovereignty-based orchestration (or Workload Confidentiality with encrypted worker VMs) for the Virtual Machines to ensure that the virtual Kubernetes Worker nodes only launch on trusted hardware, these VM trust reports provide an auditing capability to extend the Chain of Trust to the virtual Worker Nodes.","title":"Trusted Virtual Kubernetes Worker Nodes"},{"location":"product-guides/SGX%20Infrastructure/01/","text":"Product Guide July 2021 Revision 4.0 Notice: This document contains information on products in the design phase of development. The information here is subject to change without notice. Do not finalize a design with this information. Intel technologies\u2019 features and benefits depend on system configuration and may require enabled hardware, software, or service activation. Learn more at intel.com, or from the OEM or retailer. No computer system can be absolutely secure. Intel does not assume any liability for lost or stolen data or systems or any damages resulting from such losses. You may not use or facilitate the use of this document in connection with any infringement or other legal analysis concerning Intel products described herein. You agree to grant Intel a non-exclusive, royalty-free license to any patent claim thereafter drafted which includes subject matter disclosed herein. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document. The products described may contain design defects or errors known as errata which may cause the product to deviate from published specifications. Current characterized errata are available on request. This document contains information on products, services and/or processes in development. All information provided here is subject to change without notice. Contact your Intel representative to obtain the latest Intel product specifications and roadmaps. Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade. Warning Altering PC clock or memory frequency and/or voltage may (i) reduce system stability and use life of the system, memory and processor; (ii) cause the processor and other system components to fail; (iii) cause reductions in system performance; (iv) cause additional heat or other damage; and (v) affect system data integrity. Intel assumes no responsibility that the memory, included if used with altered clock frequencies and/or voltages, will be fit for any particular purpose. Check with memory manufacturer for warranty and additional details. Tests document performance of components on a particular test, in specific systems. Differences in hardware, software, or configuration will affect actual performance. Consult other sources of information to evaluate performance as you consider your purchase. For more complete information about performance and benchmark results, visit http://www.intel.com/performance . Cost reduction scenarios described are intended as examples of how a given Intel- based product, in the specified circumstances and configurations, may affect future costs and provide cost savings. Circumstances will vary. Intel does not guarantee any costs or cost reduction. Results have been estimated or simulated using internal Intel analysis or architecture simulation or modeling, and provided to you for informational purposes. Any differences in your system hardware, software or configuration may affect your actual performance. Intel does not control or audit third-party benchmark data or the web sites referenced in this document. You should visit the referenced web site and confirm whether referenced data are accurate. Intel is a sponsor and member of the Benchmark XPRT Development Community, and was the major developer of the XPRT family of benchmarks. Principled Technologies is the publisher of the XPRT family of benchmarks. You should consult other information and performance tests to assist you in fully evaluating your contemplated purchases. Copies of documents which have an order number and are referenced in this document may be obtained by calling 1-800-548-4725 or by visiting w ww.intel.com/design/literature.htm. Intel, the Intel logo, Intel TXT, and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries. *Other names and brands may be claimed as the property of others. Copyright \u00a9 2021, Intel Corporation. All Rights Reserved.","title":"Product Guide"},{"location":"product-guides/SGX%20Infrastructure/01/#product-guide","text":"","title":"Product Guide"},{"location":"product-guides/SGX%20Infrastructure/01/#july-2021","text":"","title":"July 2021"},{"location":"product-guides/SGX%20Infrastructure/01/#revision-40","text":"Notice: This document contains information on products in the design phase of development. The information here is subject to change without notice. Do not finalize a design with this information. Intel technologies\u2019 features and benefits depend on system configuration and may require enabled hardware, software, or service activation. Learn more at intel.com, or from the OEM or retailer. No computer system can be absolutely secure. Intel does not assume any liability for lost or stolen data or systems or any damages resulting from such losses. You may not use or facilitate the use of this document in connection with any infringement or other legal analysis concerning Intel products described herein. You agree to grant Intel a non-exclusive, royalty-free license to any patent claim thereafter drafted which includes subject matter disclosed herein. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document. The products described may contain design defects or errors known as errata which may cause the product to deviate from published specifications. Current characterized errata are available on request. This document contains information on products, services and/or processes in development. All information provided here is subject to change without notice. Contact your Intel representative to obtain the latest Intel product specifications and roadmaps. Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade. Warning Altering PC clock or memory frequency and/or voltage may (i) reduce system stability and use life of the system, memory and processor; (ii) cause the processor and other system components to fail; (iii) cause reductions in system performance; (iv) cause additional heat or other damage; and (v) affect system data integrity. Intel assumes no responsibility that the memory, included if used with altered clock frequencies and/or voltages, will be fit for any particular purpose. Check with memory manufacturer for warranty and additional details. Tests document performance of components on a particular test, in specific systems. Differences in hardware, software, or configuration will affect actual performance. Consult other sources of information to evaluate performance as you consider your purchase. For more complete information about performance and benchmark results, visit http://www.intel.com/performance . Cost reduction scenarios described are intended as examples of how a given Intel- based product, in the specified circumstances and configurations, may affect future costs and provide cost savings. Circumstances will vary. Intel does not guarantee any costs or cost reduction. Results have been estimated or simulated using internal Intel analysis or architecture simulation or modeling, and provided to you for informational purposes. Any differences in your system hardware, software or configuration may affect your actual performance. Intel does not control or audit third-party benchmark data or the web sites referenced in this document. You should visit the referenced web site and confirm whether referenced data are accurate. Intel is a sponsor and member of the Benchmark XPRT Development Community, and was the major developer of the XPRT family of benchmarks. Principled Technologies is the publisher of the XPRT family of benchmarks. You should consult other information and performance tests to assist you in fully evaluating your contemplated purchases. Copies of documents which have an order number and are referenced in this document may be obtained by calling 1-800-548-4725 or by visiting w ww.intel.com/design/literature.htm. Intel, the Intel logo, Intel TXT, and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries. *Other names and brands may be claimed as the property of others. Copyright \u00a9 2021, Intel Corporation. All Rights Reserved.","title":"Revision 4.0"},{"location":"product-guides/SGX%20Infrastructure/10Binary%20Upgrades/","text":"Binary Upgrades Note Before performing any upgrade, Intel strongly recommends backing up the database for the SHVS, SCS, and AAS. See Postgres documentation for detailed options for backing up databases. Below is a sample method for backing up an entire database server: Backup to tar file: pg_dump --dbname <database_name> --username=<database username> -F t > <database_backup_file>.tar Restore from tar file: pg_restore --dbname=<database_name> --username=<database username><database_backup_file>.tar Some upgrades may involve changes to database content, and a backup will ensure that data is not lost in the case of an error during the upgrade process. Backward Compatibility In general Intel SecL services are made to be backward-compatible within a given major release (for example, the 3.6 SHVS should be compatible with the 3.5 SGX Agent) in an upgrade priority order (see below). Major version upgrades may require coordinated upgrades across all services. Upgrade Order Upgrades should be performed in the following order to prevent misconfiguration or any service unavailability: 1) CMS, AAS 2) SCS, SHVS 3) SQVS, KBS, SGX Agent Upgrading in this order will make each service unavailable only for the duration of the upgrade for that service. Upgrade Process Binary Installations For services installed directly (not deployed as containers), the upgrade process simply requires executing the new-version installer on the same machine where the old-version is running. The installer will re-use the same configuration elements detected in the existing version's config file. No additional answer file is required.","title":"Binary Upgrades"},{"location":"product-guides/SGX%20Infrastructure/10Binary%20Upgrades/#binary-upgrades","text":"Note Before performing any upgrade, Intel strongly recommends backing up the database for the SHVS, SCS, and AAS. See Postgres documentation for detailed options for backing up databases. Below is a sample method for backing up an entire database server: Backup to tar file: pg_dump --dbname <database_name> --username=<database username> -F t > <database_backup_file>.tar Restore from tar file: pg_restore --dbname=<database_name> --username=<database username><database_backup_file>.tar Some upgrades may involve changes to database content, and a backup will ensure that data is not lost in the case of an error during the upgrade process.","title":"Binary Upgrades"},{"location":"product-guides/SGX%20Infrastructure/10Binary%20Upgrades/#backward-compatibility","text":"In general Intel SecL services are made to be backward-compatible within a given major release (for example, the 3.6 SHVS should be compatible with the 3.5 SGX Agent) in an upgrade priority order (see below). Major version upgrades may require coordinated upgrades across all services.","title":"Backward Compatibility"},{"location":"product-guides/SGX%20Infrastructure/10Binary%20Upgrades/#upgrade-order","text":"Upgrades should be performed in the following order to prevent misconfiguration or any service unavailability: 1) CMS, AAS 2) SCS, SHVS 3) SQVS, KBS, SGX Agent Upgrading in this order will make each service unavailable only for the duration of the upgrade for that service.","title":"Upgrade Order"},{"location":"product-guides/SGX%20Infrastructure/10Binary%20Upgrades/#upgrade-process","text":"","title":"Upgrade Process"},{"location":"product-guides/SGX%20Infrastructure/10Binary%20Upgrades/#binary-installations","text":"For services installed directly (not deployed as containers), the upgrade process simply requires executing the new-version installer on the same machine where the old-version is running. The installer will re-use the same configuration elements detected in the existing version's config file. No additional answer file is required.","title":"Binary Installations"},{"location":"product-guides/SGX%20Infrastructure/11Container%20Upgrades/","text":"Container Upgrades Container upgrades will be supported only on multi node deployments and will be based on recreate strategy from v3.6 to v4.0. All services except KBS can be upgraded just by updating the image name and tag to newer version in respective deployment.yml files. Backup and roll back applicable to all services Take back up of data in NFS mount for all services from NFS server system /isecl If upgrade is not successful, then update deployment.yml files with older images(v3.6) for 4.0 upgrade path and restore the backed up NFS data at same mount path. Bring up individual components with ./isecl-bootstrap.sh up . Note If in case service fails to start or gets crashed, then copy all the backed up data as per folder structure like how was it earlier. Bring up DB instance pointing to backed up data and bring up component deployment by pointing to older version of container image. Backward Compatibility In general Intel SecL services are made to be backward-compatible within a given major release (for example, the SHVS should be compatible with the 3.5 SGX Agent) in an upgrade priority order (see below). Major version upgrades may require coordinated upgrades across all services. Upgrade Order Upgrades should be performed in the following order to prevent misconfiguration or any service unavailability: CMS, AAS SCS, SHVS SQVS, KBS, SGX Agent ISECL-CONTROLLER, IHUB, ISECL-SCHEDULER Upgrading in this order will make each service unavailable only for the duration of the upgrade for that service. Upgrade Process Container Installations Assuming all services for any use-case are up and running. Push all the newer version of container images to registry. All oci images will be in k8s/container-images. e.g skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/<image-name>:<image-tag> --dest-tls-verify=false Individual services upgrade except KBS Update the image name in existing deployment.yml of respective service. Redeploy by running the below command, By doing \"kubectl apply -f\" on deployment.yml with change in image name will terminate service with older image version and bring up service with new image version. cd /k8s/manifests/<service-manifests-folder> && kubectl kustomize . | kubectl apply -f - e.g cd /k8s/manifests/cms && kubectl kustomize . | kubectl apply -f - KBS Upgrade Update the image name with new image name/image tag in existing deployment.yml. Copy upgrade-job.yml from builds k8s/manifests/kbs/upgrade-job.yml into control plane node k8s/manifests/kbs/ scp <build-vm>:<build-dir>/k8s/manifests/kbs/upgrade-job.yml <control-plane-vm>:<existing dir>/k8s/manifests/kbs/upgrade-job.yml Add the following variables in kbs/configMap.yml KMIP_HOSTNAME: <kmip fqdn or ip> Run the command kubectl apply -f kbs/configMap.yml --namespace=isecl (Optional) Add the following variables in kbs/secrets.yml if required KMIP_USERNAME and KMIP_PASSWORD Run the following commands kubectl delete secret -n isecl kbs-secret kubectl create secret generic kbs-secret --from-file=kbs/secrets.txt --namespace=isecl Update \"image-name\" and \"image-tag\" and existing version of deployed kbs in \"current deployed version\" in k8s/manifests/kbs/upgrade-job.yml containers: - name: kbs image: <image-name>:<image-tag> imagePullPolicy: Always securityContext: runAsUser: 1001 runAsGroup: 1001 env: - name: CONFIG_DIR value: \"/etc/kbs\" - name: COMPONENT_VERSION value: <current deployed version> Run the upgrade job, cd k8s/manifests kubectl delete deployment -n isecl kbs-deployment kubectl apply -f kbs/upgrade-job.yml Check the status of kbs-upgrade job for completion. kubectl get jobs -n isecl kubectl logs -n isecl kbs-upgrade-<pod id> Update the image name in kbs/deployment.yml to newer version and deploy the latest kbs kubectl apply -f kbs/deployment.yml or cd kbs && kubectl kustomize . | kubectl apply -f -","title":"Container Upgrades"},{"location":"product-guides/SGX%20Infrastructure/11Container%20Upgrades/#container-upgrades","text":"Container upgrades will be supported only on multi node deployments and will be based on recreate strategy from v3.6 to v4.0. All services except KBS can be upgraded just by updating the image name and tag to newer version in respective deployment.yml files.","title":"Container Upgrades"},{"location":"product-guides/SGX%20Infrastructure/11Container%20Upgrades/#backup-and-roll-back-applicable-to-all-services","text":"Take back up of data in NFS mount for all services from NFS server system /isecl If upgrade is not successful, then update deployment.yml files with older images(v3.6) for 4.0 upgrade path and restore the backed up NFS data at same mount path. Bring up individual components with ./isecl-bootstrap.sh up . Note If in case service fails to start or gets crashed, then copy all the backed up data as per folder structure like how was it earlier. Bring up DB instance pointing to backed up data and bring up component deployment by pointing to older version of container image.","title":"Backup and roll back applicable to all services"},{"location":"product-guides/SGX%20Infrastructure/11Container%20Upgrades/#backward-compatibility","text":"In general Intel SecL services are made to be backward-compatible within a given major release (for example, the SHVS should be compatible with the 3.5 SGX Agent) in an upgrade priority order (see below). Major version upgrades may require coordinated upgrades across all services.","title":"Backward Compatibility"},{"location":"product-guides/SGX%20Infrastructure/11Container%20Upgrades/#upgrade-order","text":"Upgrades should be performed in the following order to prevent misconfiguration or any service unavailability: CMS, AAS SCS, SHVS SQVS, KBS, SGX Agent ISECL-CONTROLLER, IHUB, ISECL-SCHEDULER Upgrading in this order will make each service unavailable only for the duration of the upgrade for that service.","title":"Upgrade Order"},{"location":"product-guides/SGX%20Infrastructure/11Container%20Upgrades/#upgrade-process","text":"","title":"Upgrade Process"},{"location":"product-guides/SGX%20Infrastructure/11Container%20Upgrades/#container-installations","text":"Assuming all services for any use-case are up and running. Push all the newer version of container images to registry. All oci images will be in k8s/container-images. e.g skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/<image-name>:<image-tag> --dest-tls-verify=false","title":"Container Installations"},{"location":"product-guides/SGX%20Infrastructure/11Container%20Upgrades/#individual-services-upgrade-except-kbs","text":"Update the image name in existing deployment.yml of respective service. Redeploy by running the below command, By doing \"kubectl apply -f\" on deployment.yml with change in image name will terminate service with older image version and bring up service with new image version. cd /k8s/manifests/<service-manifests-folder> && kubectl kustomize . | kubectl apply -f - e.g cd /k8s/manifests/cms && kubectl kustomize . | kubectl apply -f -","title":"Individual services upgrade except KBS"},{"location":"product-guides/SGX%20Infrastructure/11Container%20Upgrades/#kbs-upgrade","text":"Update the image name with new image name/image tag in existing deployment.yml. Copy upgrade-job.yml from builds k8s/manifests/kbs/upgrade-job.yml into control plane node k8s/manifests/kbs/ scp <build-vm>:<build-dir>/k8s/manifests/kbs/upgrade-job.yml <control-plane-vm>:<existing dir>/k8s/manifests/kbs/upgrade-job.yml Add the following variables in kbs/configMap.yml KMIP_HOSTNAME: <kmip fqdn or ip> Run the command kubectl apply -f kbs/configMap.yml --namespace=isecl (Optional) Add the following variables in kbs/secrets.yml if required KMIP_USERNAME and KMIP_PASSWORD Run the following commands kubectl delete secret -n isecl kbs-secret kubectl create secret generic kbs-secret --from-file=kbs/secrets.txt --namespace=isecl Update \"image-name\" and \"image-tag\" and existing version of deployed kbs in \"current deployed version\" in k8s/manifests/kbs/upgrade-job.yml containers: - name: kbs image: <image-name>:<image-tag> imagePullPolicy: Always securityContext: runAsUser: 1001 runAsGroup: 1001 env: - name: CONFIG_DIR value: \"/etc/kbs\" - name: COMPONENT_VERSION value: <current deployed version> Run the upgrade job, cd k8s/manifests kubectl delete deployment -n isecl kbs-deployment kubectl apply -f kbs/upgrade-job.yml Check the status of kbs-upgrade job for completion. kubectl get jobs -n isecl kubectl logs -n isecl kbs-upgrade-<pod id> Update the image name in kbs/deployment.yml to newer version and deploy the latest kbs kubectl apply -f kbs/deployment.yml or cd kbs && kubectl kustomize . | kubectl apply -f -","title":"KBS Upgrade"},{"location":"product-guides/SGX%20Infrastructure/12Appendix/","text":"Appendix SGX Attestation flow To Deploy SampleApp: Copy sample_apps.tar, sample_apps.sha2 and sampleapps_untar.sh from binaries directory to a directory in SGX compute node and untar it using './sample_apps_untar.sh' Install Intel\u00ae SGX SDK for Linux*OS into /opt/intel/sgxsdk using './install_sgxsdk.sh' Install SGX dependencies using './deploy_sgx_dependencies.sh' Note: Make sure to deploy SQVS with includetoken configuration as false. To Verify the SampleApp flow: Update sample_apps.conf with the following - IP address for SQVS services deployed on Enterprise system - IP address for SCS services deployed on CSP system - ENTERPRISE_CMS_IP should point to the IP of CMS service deployed on Enterprise system - Network Port numbers for SCS services deployed on CSP system - Network Port numbers for SQVS and CMS services deployed on Enterprise system - Set RUN_ATTESTING_APP to yes if user wants to run both apps in same machine Run SampleApp using './run_sample_apps.sh' Check the output of attestedApp and attestingApp under out/attested_app_console_out.log and out/attesting_app_console_out.log files Creating RSA Keys in Key Broker Service Steps to run KMIP Server Note: Below mentioned steps are provided as script (install_pykmip.sh and pykmip.service) as part of kbs_script folder which will install KMIP Server as daemon. Refer to \u2018Install KMIP Server as daemon\u2019 section. 1. Install python3 and vim-common For RHEL 8.2 # dnf -y install python3-pip vim-common For Ubuntu 18.04 # apt -y install python3-pip vim-common ln -s /usr/bin/python3 /usr/bin/python > /dev/null 2>&1 ln -s /usr/bin/pip3 /usr/bin/pip > /dev/null 2>&1 2. Install pykmip # pip3 install pykmip==0.9.1 3. In the /etc/ directory create pykmip and policies folders mkdir -p /etc/pykmip/policies 4. Configure pykmip server using server.conf Update hostname in the server.conf 5. Copy the following to /etc/pykmip/ from kbs_script folder available under binaries directory create_certificates.py, run_server.py, server.conf 6. Create certificates > cd /etc/pykmip > python3 create_certificates.py <KMIP Host IP/KMIP Host FQDN> 7. Kill running KMIP Server processes and wait for 10 seconds until all the KMIP Server processes are killed. > ps -ef | grep run_server.py | grep -v grep | awk '{print $2}' | xargs kill 8. Run pykmip server using run_server.py script > python3 run_server.py & Install KMIP Server as daemon 1. cd into /root/binaries/kbs_script folder 2. Configure pykmip server using server.conf Update hostname in the server.conf 3. Run the install_pykmip.sh script and KMIP server will be installed as daemon process ./install_pykmip.sh Create RSA key in PyKMIP and generate certificate NOTE: This step is required only when PyKMIP script is used as a backend KMIP server. 1. Update Host IP in /root/binaries/kbs_script rsa_create.py script 2. In the kbs_script folder, Run rsa_create.py script > cd /root/binaries/kbs_script > python3 rsa_create.py This script will generate \u201cPrivate Key ID\u201d and \u201cServer certificate\u201d, which should be provided in the kbs.conf file for \u201cKMIP_KEY_ID\u201d and \u201cSERVER_CERT\u201d. Configuration Update to create Keys in KBS cd into /root/binaries/kbs_script folder **To register keys with KBS KMIP** Update the following variables in kbs.conf: KMIP_KEY_ID (Private key ID registered in KMIP server) SERVER_CERT (Server certificate for created private key) Enterprise system IP address where CMS, AAS and KBS services are deployed Port of CMS, AAS and KBS services deployed on enterprise system AAS admin and Enterprise admin credentials NOTE: If KMIP_KEY_ID is not provided then RSA key register will be done with keystring. Update sgx_enclave_measurement_anyof value in transfer_policy_request.json with enclave measurement value obtained using sgx_sign utility. Refer to \"Extracting SGX Enclave values for Key Transfer Policy\" section. Create RSA Key Execute the command ./run.sh reg Copy the generated cert file to SGX Compute node where skc_library is deployed. Also make a note of the key id generated. Configuration for NGINX testing on RHEL 8.2 Note Below mentioned OpenSSL and NGINX configuration updates are provided as patches (nginx.patch and openssl.patch) as part of skc_library deployment script. Patch can be applied with default nginx and openssl file. In case nginx/openssl contains any external changes then refer manual step. Apply Patch Execute the command with nginx version - nginx 1.14.1 (Rhel) and openssl version- Openssl 1.1.1g (Rhel) patch -b /etc/nginx/nginx.conf < nginx.patch patch -b /etc/ssl/openssl.cnf < openssl.patch OpenSSL Configuration Update openssl configuration file /etc/pki/tls/openssl.cnf with below changes: [openssl_def] engines = engine_section [engine_section] pkcs11 = pkcs11_section [pkcs11_section] engine_id = pkcs11 dynamic_path =/usr/lib64/engines-1.1/pkcs11.so MODULE_PATH =/opt/skc/lib/libpkcs11-api.so init = 0 Nginx Configuration Update nginx configuration file /etc/nginx/nginx.conf with below changes: ssl_engine pkcs11; Update the location of certificate with the location where it was copied into the SGX compute node. ssl_certificate \"add absolute path of crt file\"; Update the fields(token, object and pin-value) with the values given in keys.txt for the KeyID corresponding to the certificate. ssl_certificate_key \"engine:pkcs11:pkcs11:token=KMS;object=RSAKEY;pin-value=1234\"; SKC Configuration Create keys.txt in /root folder. This provides key preloading functionality in skc_library. Any number of keys can be added in keys.txt. Each PKCS11 URL should contain different Key IDs which need to be transferred from KBS along with respective object tag for each key id specified Token, object and pin-value given in PKCS11 url entry in keys.txt should match with the one in nginx.conf. The keyID should match the keyID of RSA key created in KBS. File location should match on pkcs11-apimodule.ini; pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234; Sample /opt/skc/etc/pkcs11-apimodule.ini file [core] preload_keys=/root/keys.txt keyagent_conf=/opt/skc/etc/key-agent.ini mode=SGX debug=true [SGX] module=/opt/intel/cryptoapitoolkit/lib/libp11sgx.so Configuration for NGINX testing for Ubuntu 18.04 Note Below mentioned OpenSSL and NGINX configuration updates are provided as patches (nginx.patch and openssl.patch) as part of skc_library deployment script. Patch can be applied with default nginx and openssl file. In case nginx/openssl contains any external changes then refer manual step. Apply Patch Execute the command with nginx version - nginx 1.14.0 (Ubuntu) and openssl version- Openssl 1.1.1 (Ubuntu) patch -b /etc/nginx/nginx.conf < nginx_ubuntu.patch patch -b /etc/ssl/openssl.cnf < openssl_ubuntu.patch OpenSSL In the /etc/ssl/openssl.cnf file, look for the below line: [ new_oids ] Just before the line [ new_oids ], add the below section: openssl_conf = openssl_def [openssl_def] engines = engine_section oid_section = new_oids [engine_section] pkcs11 = pkcs11_section [pkcs11_section] engine_id = pkcs11 dynamic_path =/usr/lib/x86_64-linux-gnu/engines-1.1/pkcs11.so MODULE_PATH =/opt/skc/lib/libpkcs11-api.so init = 0 Nginx Update nginx configuration file /etc/nginx/nginx.conf with below changes: ssl_engine pkcs11; Update the location of certificate with the loaction where it was copied into the skc_library machine. ssl_certificate \"add absolute path of crt file\"; Update the fields(token, object and pin-value) with the values given in keys.txt for the KeyID corresponding to the certificate. ssl_certificate_key \"engine:pkcs11:pkcs11:token=KMS;object=RSAKEY;pin-value=1234\"; SKC Configuration Create keys.txt in /root folder. This provides key preloading functionality in skc_library. Any number of keys can be added in keys.txt. Each PKCS11 URL should contain different Key ID which need to be transferred from KBS along with respective object tag for each key id specified Sample PKCS11 url is as below pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234; Token, object and pin-value given in PKCS11 url entry in keys.txt should match with the one in nginx.conf. The keyID should match the keyID of RSA key created in KBS. File location should match with preload_keys directive in pkcs11-apimodule.ini; Sample /opt/skc/etc/pkcs11-apimodule.ini file [core] preload_keys=/root/keys.txt keyagent_conf=/opt/skc/etc/key-agent.ini mode=SGX debug=true [SGX] module=/opt/intel/cryptoapitoolkit/lib/libp11sgx.so","title":"Appendix"},{"location":"product-guides/SGX%20Infrastructure/12Appendix/#appendix","text":"","title":"Appendix"},{"location":"product-guides/SGX%20Infrastructure/12Appendix/#sgx-attestation-flow","text":"To Deploy SampleApp: Copy sample_apps.tar, sample_apps.sha2 and sampleapps_untar.sh from binaries directory to a directory in SGX compute node and untar it using './sample_apps_untar.sh' Install Intel\u00ae SGX SDK for Linux*OS into /opt/intel/sgxsdk using './install_sgxsdk.sh' Install SGX dependencies using './deploy_sgx_dependencies.sh' Note: Make sure to deploy SQVS with includetoken configuration as false. To Verify the SampleApp flow: Update sample_apps.conf with the following - IP address for SQVS services deployed on Enterprise system - IP address for SCS services deployed on CSP system - ENTERPRISE_CMS_IP should point to the IP of CMS service deployed on Enterprise system - Network Port numbers for SCS services deployed on CSP system - Network Port numbers for SQVS and CMS services deployed on Enterprise system - Set RUN_ATTESTING_APP to yes if user wants to run both apps in same machine Run SampleApp using './run_sample_apps.sh' Check the output of attestedApp and attestingApp under out/attested_app_console_out.log and out/attesting_app_console_out.log files","title":"SGX Attestation flow"},{"location":"product-guides/SGX%20Infrastructure/12Appendix/#creating-rsa-keys-in-key-broker-service","text":"Steps to run KMIP Server Note: Below mentioned steps are provided as script (install_pykmip.sh and pykmip.service) as part of kbs_script folder which will install KMIP Server as daemon. Refer to \u2018Install KMIP Server as daemon\u2019 section. 1. Install python3 and vim-common For RHEL 8.2 # dnf -y install python3-pip vim-common For Ubuntu 18.04 # apt -y install python3-pip vim-common ln -s /usr/bin/python3 /usr/bin/python > /dev/null 2>&1 ln -s /usr/bin/pip3 /usr/bin/pip > /dev/null 2>&1 2. Install pykmip # pip3 install pykmip==0.9.1 3. In the /etc/ directory create pykmip and policies folders mkdir -p /etc/pykmip/policies 4. Configure pykmip server using server.conf Update hostname in the server.conf 5. Copy the following to /etc/pykmip/ from kbs_script folder available under binaries directory create_certificates.py, run_server.py, server.conf 6. Create certificates > cd /etc/pykmip > python3 create_certificates.py <KMIP Host IP/KMIP Host FQDN> 7. Kill running KMIP Server processes and wait for 10 seconds until all the KMIP Server processes are killed. > ps -ef | grep run_server.py | grep -v grep | awk '{print $2}' | xargs kill 8. Run pykmip server using run_server.py script > python3 run_server.py & Install KMIP Server as daemon 1. cd into /root/binaries/kbs_script folder 2. Configure pykmip server using server.conf Update hostname in the server.conf 3. Run the install_pykmip.sh script and KMIP server will be installed as daemon process ./install_pykmip.sh Create RSA key in PyKMIP and generate certificate NOTE: This step is required only when PyKMIP script is used as a backend KMIP server. 1. Update Host IP in /root/binaries/kbs_script rsa_create.py script 2. In the kbs_script folder, Run rsa_create.py script > cd /root/binaries/kbs_script > python3 rsa_create.py This script will generate \u201cPrivate Key ID\u201d and \u201cServer certificate\u201d, which should be provided in the kbs.conf file for \u201cKMIP_KEY_ID\u201d and \u201cSERVER_CERT\u201d. Configuration Update to create Keys in KBS cd into /root/binaries/kbs_script folder **To register keys with KBS KMIP** Update the following variables in kbs.conf: KMIP_KEY_ID (Private key ID registered in KMIP server) SERVER_CERT (Server certificate for created private key) Enterprise system IP address where CMS, AAS and KBS services are deployed Port of CMS, AAS and KBS services deployed on enterprise system AAS admin and Enterprise admin credentials NOTE: If KMIP_KEY_ID is not provided then RSA key register will be done with keystring. Update sgx_enclave_measurement_anyof value in transfer_policy_request.json with enclave measurement value obtained using sgx_sign utility. Refer to \"Extracting SGX Enclave values for Key Transfer Policy\" section. Create RSA Key Execute the command ./run.sh reg Copy the generated cert file to SGX Compute node where skc_library is deployed. Also make a note of the key id generated.","title":"Creating RSA Keys in Key Broker Service"},{"location":"product-guides/SGX%20Infrastructure/12Appendix/#configuration-for-nginx-testing-on-rhel-82","text":"Note Below mentioned OpenSSL and NGINX configuration updates are provided as patches (nginx.patch and openssl.patch) as part of skc_library deployment script. Patch can be applied with default nginx and openssl file. In case nginx/openssl contains any external changes then refer manual step. Apply Patch Execute the command with nginx version - nginx 1.14.1 (Rhel) and openssl version- Openssl 1.1.1g (Rhel) patch -b /etc/nginx/nginx.conf < nginx.patch patch -b /etc/ssl/openssl.cnf < openssl.patch OpenSSL Configuration Update openssl configuration file /etc/pki/tls/openssl.cnf with below changes: [openssl_def] engines = engine_section [engine_section] pkcs11 = pkcs11_section [pkcs11_section] engine_id = pkcs11 dynamic_path =/usr/lib64/engines-1.1/pkcs11.so MODULE_PATH =/opt/skc/lib/libpkcs11-api.so init = 0 Nginx Configuration Update nginx configuration file /etc/nginx/nginx.conf with below changes: ssl_engine pkcs11; Update the location of certificate with the location where it was copied into the SGX compute node. ssl_certificate \"add absolute path of crt file\"; Update the fields(token, object and pin-value) with the values given in keys.txt for the KeyID corresponding to the certificate. ssl_certificate_key \"engine:pkcs11:pkcs11:token=KMS;object=RSAKEY;pin-value=1234\"; SKC Configuration Create keys.txt in /root folder. This provides key preloading functionality in skc_library. Any number of keys can be added in keys.txt. Each PKCS11 URL should contain different Key IDs which need to be transferred from KBS along with respective object tag for each key id specified Token, object and pin-value given in PKCS11 url entry in keys.txt should match with the one in nginx.conf. The keyID should match the keyID of RSA key created in KBS. File location should match on pkcs11-apimodule.ini; pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234; Sample /opt/skc/etc/pkcs11-apimodule.ini file [core] preload_keys=/root/keys.txt keyagent_conf=/opt/skc/etc/key-agent.ini mode=SGX debug=true [SGX] module=/opt/intel/cryptoapitoolkit/lib/libp11sgx.so","title":"Configuration for NGINX testing on RHEL 8.2"},{"location":"product-guides/SGX%20Infrastructure/12Appendix/#configuration-for-nginx-testing-for-ubuntu-1804","text":"Note Below mentioned OpenSSL and NGINX configuration updates are provided as patches (nginx.patch and openssl.patch) as part of skc_library deployment script. Patch can be applied with default nginx and openssl file. In case nginx/openssl contains any external changes then refer manual step. Apply Patch Execute the command with nginx version - nginx 1.14.0 (Ubuntu) and openssl version- Openssl 1.1.1 (Ubuntu) patch -b /etc/nginx/nginx.conf < nginx_ubuntu.patch patch -b /etc/ssl/openssl.cnf < openssl_ubuntu.patch OpenSSL In the /etc/ssl/openssl.cnf file, look for the below line: [ new_oids ] Just before the line [ new_oids ], add the below section: openssl_conf = openssl_def [openssl_def] engines = engine_section oid_section = new_oids [engine_section] pkcs11 = pkcs11_section [pkcs11_section] engine_id = pkcs11 dynamic_path =/usr/lib/x86_64-linux-gnu/engines-1.1/pkcs11.so MODULE_PATH =/opt/skc/lib/libpkcs11-api.so init = 0 Nginx Update nginx configuration file /etc/nginx/nginx.conf with below changes: ssl_engine pkcs11; Update the location of certificate with the loaction where it was copied into the skc_library machine. ssl_certificate \"add absolute path of crt file\"; Update the fields(token, object and pin-value) with the values given in keys.txt for the KeyID corresponding to the certificate. ssl_certificate_key \"engine:pkcs11:pkcs11:token=KMS;object=RSAKEY;pin-value=1234\"; SKC Configuration Create keys.txt in /root folder. This provides key preloading functionality in skc_library. Any number of keys can be added in keys.txt. Each PKCS11 URL should contain different Key ID which need to be transferred from KBS along with respective object tag for each key id specified Sample PKCS11 url is as below pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234; Token, object and pin-value given in PKCS11 url entry in keys.txt should match with the one in nginx.conf. The keyID should match the keyID of RSA key created in KBS. File location should match with preload_keys directive in pkcs11-apimodule.ini; Sample /opt/skc/etc/pkcs11-apimodule.ini file [core] preload_keys=/root/keys.txt keyagent_conf=/opt/skc/etc/key-agent.ini mode=SGX debug=true [SGX] module=/opt/intel/cryptoapitoolkit/lib/libp11sgx.so","title":"Configuration for NGINX testing for Ubuntu 18.04"},{"location":"product-guides/SGX%20Infrastructure/13KBS%20key-transfer%20flow%20validation/","text":"KBS key-transfer flow validation On SGX compute node, Execute below commands for KBS key-transfer: Note: Before initiating key transfer make sure, PYKMIP server is running. pkill nginx Remove any existing pkcs11 token rm -rf /opt/intel/cryptoapitoolkit/tokens/* Initiate Key tranfer from KBS systemctl restart nginx Changing group ownership and permissions of pkcs11 token chown -R root:intel /opt/intel/cryptoapitoolkit/tokens/ chmod -R 770 /opt/intel/cryptoapitoolkit/tokens/ Establish tls session with the nginx using the key transferred inside the enclave wget https://localhost:2443 --no-check-certificate","title":"KBS key-transfer flow validation"},{"location":"product-guides/SGX%20Infrastructure/13KBS%20key-transfer%20flow%20validation/#kbs-key-transfer-flow-validation","text":"On SGX compute node, Execute below commands for KBS key-transfer: Note: Before initiating key transfer make sure, PYKMIP server is running. pkill nginx Remove any existing pkcs11 token rm -rf /opt/intel/cryptoapitoolkit/tokens/* Initiate Key tranfer from KBS systemctl restart nginx Changing group ownership and permissions of pkcs11 token chown -R root:intel /opt/intel/cryptoapitoolkit/tokens/ chmod -R 770 /opt/intel/cryptoapitoolkit/tokens/ Establish tls session with the nginx using the key transferred inside the enclave wget https://localhost:2443 --no-check-certificate","title":"KBS key-transfer flow validation"},{"location":"product-guides/SGX%20Infrastructure/14Note%20on%20Key%20Transfer%20Policy/","text":"Note on Key Transfer Policy Key transfer policy is used to enforce a set of policies which need to be compiled with before the secret can be securely provisioned onto a sgx enclave A typical Key Transfer Policy would look as below \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"], \"sgx_enclave_issuer_product_id_anyof\":[0], \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"], \"tls_client_certificate_issuer_cn_anyof\":[\"CMSCA\", \"CMS TLS Client CA\"], \"client_permissions_allof\":[\"nginx\",\"USA\"], \"sgx_enforce_tcb_up_to_date\":false sgx_enclave_issuer_anyof establishes the signing identity provided by an authority who has signed the sgx enclave. in other words the owner of the enclave sgx_enclave_measurement_anyof represents the cryptographic hash of the enclave log (enclave code, data) sgx_enforce_tcb_up_to_date - If set to true, Key Broker service will provision the key only of the platform generating the quote conforms to the latest Trusted Computing Base client_permissions_allof - Special permission embedded into the skc_library client TLS certificate which can enforce additional restrictons on who can get access to the key, In above example: the key is provisioned only to the nginx workload and platform which is tagged with value for ex: USA","title":"Note on Key Transfer Policy"},{"location":"product-guides/SGX%20Infrastructure/14Note%20on%20Key%20Transfer%20Policy/#note-on-key-transfer-policy","text":"Key transfer policy is used to enforce a set of policies which need to be compiled with before the secret can be securely provisioned onto a sgx enclave A typical Key Transfer Policy would look as below \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"], \"sgx_enclave_issuer_product_id_anyof\":[0], \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"], \"tls_client_certificate_issuer_cn_anyof\":[\"CMSCA\", \"CMS TLS Client CA\"], \"client_permissions_allof\":[\"nginx\",\"USA\"], \"sgx_enforce_tcb_up_to_date\":false sgx_enclave_issuer_anyof establishes the signing identity provided by an authority who has signed the sgx enclave. in other words the owner of the enclave sgx_enclave_measurement_anyof represents the cryptographic hash of the enclave log (enclave code, data) sgx_enforce_tcb_up_to_date - If set to true, Key Broker service will provision the key only of the platform generating the quote conforms to the latest Trusted Computing Base client_permissions_allof - Special permission embedded into the skc_library client TLS certificate which can enforce additional restrictons on who can get access to the key, In above example: the key is provisioned only to the nginx workload and platform which is tagged with value for ex: USA","title":"Note on Key Transfer Policy"},{"location":"product-guides/SGX%20Infrastructure/15Note%20on%20SKC%20Library%20Deployment/","text":"Note on SKC Library Deployment SKC Library Deployment (Binary as well as container) needs to performed with root privilege For binary deployment of SKC client Library, only one instance of Workload can use SKC Client Library. The config information for SKC client library is bound to the workload. In future, Multiple workloads might be supported For container deployment, since configmaps are used, each container instance of workload gets its own private SKC Client Library config information The SKC Client Library TLS client certificate private key is stored in the configuration directories and can be read only with elevated root privileges keys.txt (set of PKCS11 URIs for the keys to be securely provisioned into an SGX enclave) can only be modified with elevated privileges Extracting SGX Enclave values for Key Transfer Policy Values that are specific to the enclave such as sgx_enclave_issuer_anyof, sgx_enclave_measurement_anyof and sgx_enclave_issuer_product_id_anyof can be retrived using sgx_sign utility that is available as part of Intel SGX SDK. Run sgx_sign utility on the signed enclave (This command should be run on the build system). /opt/intel/sgxsdk/bin/x64/sgx_sign dump -enclave <path to the signed enclave> -dumpfile info.txt For sgx_enclave_issuer_anyof , in info.txt, search for \"mrsigner->value\" . E.g mrsigner->value : mrsigner->value: \"0x83 0xd7 0x19 0xe7 0x7d 0xea 0xca 0x14 0x70 0xf6 0xba 0xf6 0x2a 0x4d 0x77 0x43 0x03 0xc8 0x99 0xdb 0x69 0x02 0x0f 0x9c 0x70 0xee 0x1d 0xfc 0x08 0xc7 0xce 0x9e\" Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"] For sgx_enclave_measurement_anyof , in info.txt, search for metadata->enclave_css.body.enclave_hash.m . E.g metadata->enclave_css.body.enclave_hash.m : metadata->enclave_css.body.enclave_hash.m: 0xad 0x46 0x74 0x9e 0xd4 0x1e 0xba 0xa2 0x32 0x72 0x52 0x04 0x1e 0xe7 0x46 0xd3 0x79 0x1a 0x9f 0x24 0x31 0x83 0x0f 0xee 0x08 0x83 0xf7 0x99 0x3c 0xaf 0x31 0x6a Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"] Please note that the SGX Enclave measurement value will depend on the toolchain used to build and link the SGX enclave. Hence the SGX Enclave measurement value would differ across OS flavours. For more details please refer https://github.com/intel/linux-sgx/tree/master/linux/reproducibility","title":"Note on SKC Library Deployment"},{"location":"product-guides/SGX%20Infrastructure/15Note%20on%20SKC%20Library%20Deployment/#note-on-skc-library-deployment","text":"SKC Library Deployment (Binary as well as container) needs to performed with root privilege For binary deployment of SKC client Library, only one instance of Workload can use SKC Client Library. The config information for SKC client library is bound to the workload. In future, Multiple workloads might be supported For container deployment, since configmaps are used, each container instance of workload gets its own private SKC Client Library config information The SKC Client Library TLS client certificate private key is stored in the configuration directories and can be read only with elevated root privileges keys.txt (set of PKCS11 URIs for the keys to be securely provisioned into an SGX enclave) can only be modified with elevated privileges","title":"Note on SKC Library Deployment"},{"location":"product-guides/SGX%20Infrastructure/15Note%20on%20SKC%20Library%20Deployment/#extracting-sgx-enclave-values-for-key-transfer-policy","text":"Values that are specific to the enclave such as sgx_enclave_issuer_anyof, sgx_enclave_measurement_anyof and sgx_enclave_issuer_product_id_anyof can be retrived using sgx_sign utility that is available as part of Intel SGX SDK. Run sgx_sign utility on the signed enclave (This command should be run on the build system). /opt/intel/sgxsdk/bin/x64/sgx_sign dump -enclave <path to the signed enclave> -dumpfile info.txt For sgx_enclave_issuer_anyof , in info.txt, search for \"mrsigner->value\" . E.g mrsigner->value : mrsigner->value: \"0x83 0xd7 0x19 0xe7 0x7d 0xea 0xca 0x14 0x70 0xf6 0xba 0xf6 0x2a 0x4d 0x77 0x43 0x03 0xc8 0x99 0xdb 0x69 0x02 0x0f 0x9c 0x70 0xee 0x1d 0xfc 0x08 0xc7 0xce 0x9e\" Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"] For sgx_enclave_measurement_anyof , in info.txt, search for metadata->enclave_css.body.enclave_hash.m . E.g metadata->enclave_css.body.enclave_hash.m : metadata->enclave_css.body.enclave_hash.m: 0xad 0x46 0x74 0x9e 0xd4 0x1e 0xba 0xa2 0x32 0x72 0x52 0x04 0x1e 0xe7 0x46 0xd3 0x79 0x1a 0x9f 0x24 0x31 0x83 0x0f 0xee 0x08 0x83 0xf7 0x99 0x3c 0xaf 0x31 0x6a Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"] Please note that the SGX Enclave measurement value will depend on the toolchain used to build and link the SGX enclave. Hence the SGX Enclave measurement value would differ across OS flavours. For more details please refer https://github.com/intel/linux-sgx/tree/master/linux/reproducibility","title":"Extracting SGX Enclave values for Key Transfer Policy"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/","text":"Introduction Overview The SGX Attestation infrastructure and Secure Key Caching (SKC) are part of the Intel Security Libraries for datacenter (ISecL-DC). Intel Security Libraries for Datacenter is a collection of software applications and development libraries intended to help turn Intel platform security features into real-world security use cases. The SGX Attestation infrastructure provides an end to end support for registering SGX hosts and provisioning them with SGX material (PCK certificates) and SGX collateral (security patches information - TCB Information - and Certificate Revocation Lists - CRLs). The SGX Attestation infrastructure also provides support for generating SGX quotes for SGX enclaves hosted by workloads and verifying them by a remote attesting application. The remote attesting application can also use the SGX Attestation infrastructure to enforce enclave policies (like requiring a specific enclave signer). Optionally, the SGX Attestation Infrastructure allows to integrate with Cloud Orchestrators like Openstack and Kubernetes. SKC leverages the SGX Attestation Infrastructure to support the Secure Key Caching (SKC) use case.SKC provides the key protection at rest and in-use use case using the Intel Software Guard Extensions technology (SGX). SGX implements the Trusted Execution Environment (TEE) paradigm. Using the SKC Client -- a set of libraries -- applications can retrieve keys from the ISecL-DC Key Broker Service (KBS) and load them to an SGX-protected memory (called SGX enclave) in the application memory space. KBS performs the SGX enclave attestation to ensure that the application will store the keys in a genuine SGX enclave. Application keys are wrapped with an enclave public key by KBS prior to transferring to the application enclave. Consequently, application keys are protected from infrastructure admins, malicious applications and compromised HW/BIOS/OS/VMM. SKC does not require the refactoring of the application because it supports a standard PKCS#11 interface. Trusted Execution Environment A Trusted Execution Environment (TEE) provides a computer secure area where code and data can be loaded with the assurance that their confidentiality and integrity are protected. TEEs have various applications in areas where the confidentiality of the data and the integrity of the code are of the highest importance. One examples of a TEE usage is the protection of sensitive secrets like cryptographic keys and authentication strings. These secrets can be preserved within a TEE, ensuring that they never get exposed to software that is outside of the TEE. Operations involving these secrets can be performed within the TEE, removing the need to ever extract the secrets outside of the TEE. Another example is the processing of data with privacy concerns. This is often the case in financial, healthcare and artificial intelligence (AI) applications. Putting sensitive operations inside a TEE allows organizations to support business processes without compromising the privacy expectations of their customers. Intel Software Guard Extensions Intel Software Guard Extensions (SGX) is an Intel platform security feature that implements the TEE paradigm. A portion of RAM called EPC (Enclave Page Cache) is used by applications to load secure isolated areas called SGX enclaves. Code and data inside SGX enclaves are encrypted and only decrypted inside the Intel CPU. From the host application perspective, an SGX enclave looks like a dynamic library. Any part of the application that is not contained in an SGX enclave is considered untrusted while the SGX enclave is considered trusted. Communications between the untrusted part and the trusted part (the SGX enclave) of an application uses a special calls called ECALLS and call from the enclave to the untrusted part of the application use OCALLS. A signed claim called SGX quote can be generated for an enclave. The SGX quote may contain a measurement of the code and the data of the enclave. An SGX quote allows to prove to a remote verifier (relying party) that an application includes the expected SGX enclave. SGX ECDSA Attestation SGX ECDSA attestation is the process that allows an application (relying party) to verify that a remote piece of code and data that it's interacting with is contained in a genuine Intel SGX enclave. The remote enclave can generate a signed claim called an SGX quote. A valid SGX quote signature generated on an SGX enabled platform can be chained up to a trusted Intel signing key. The SGX quote contains the measurement of the enclave (MREnclave), the enclave developer's signature (MRSigner), the security patch level of the platform (Trusted Computing Base or TCB) and any user data that the enclave wants to include in the quote. Typically, the user data in an SGX quote contains the hash of the public key part of a public/private key pair generated inside the enclave. The public key is transferred along with the SGX quote to the relying party. The latter generates a Symmetric Wrapping Key (SWK) and wraps it with the public key of the enclave. The wrapped SWK is provisioned into the SGX enclave, which can unwrap it since it has the corresponding private key. The relying party can then provision secrets into the SGX enclave after wrapping them with the SWK. For an enclave to generate an SGX quote, a PCK certificate for the host platform must be obtained from Intel SGX Provisioning Certification Service (PCS). PCK Certificates Provisioning To generate an SGX quote for an enclave, a PCK certificate must be obtained from SGX Provisioning Certification Service (PCS). Requiring a workload to retrieve the PCK certificate from Intel PCS at the time of the SGX quote generation can be detrimental to the workload. Network connectivity issues can prevent the connection to Intel PCS. To remove the dependency on network connectivity, the PCK certificates of the data center platforms are fetched before running any workload. This is achieved by extracting SGX related information from the platform using the PCK ID Retrieval tool and pushing it to a Caching Service running in the same data center. The Caching Service then retrieves the PCK certificates of all the platforms that pushed SGX information to it from Intel PCS. Network connectivity issues are not a problem since the Caching Service can retry if needed. When an SGX workload needs its PCK certificate, it can just get it from the Caching Service. Key Protection Cryptographic keys are high value assets that must be protected against disclosure and corruption. Key disclosure or corruption expose the key owner to data confidentiality breaches, impersonation and denial of service. The industry has good solutions to protect keys at rest. A popular solution is to store keys in a central secure Key Management System (KMS), and applications retrieve them at runtime. However, this solution does not protect keys once they are in RAM and used to perform cryptographic operations. Keys in RAM can be disclosed because of software vulnerabilities like Heartbleed or because of memory snapshots. Therefore, keys are not protected in use. This concern can be addressed by having the application send the payload that needs cryptographic processing to the KMS where the processing happens instead. By doing this, the key is never exposed in RAM. However, this solution incurs an overhead caused by the network round trip to the KMS. Another solution is to store keys in Hardware Security Modules (HSMs) HSM A Hardware Security Module or HSM is a separate hardware part that can be attached to a server. HSMs provide APIs to create and load keys. HSMs also support APIs to perform cryptographic operations using keys stored inside them. The typical flow for using an HSM is to create or load a key in the HSM in a secure environment then take the HSM to the server where the workload runs and attach it to this server. The application then performs cryptographic operations using the key inside the HSM. This ensures that the key is never exposed in RAM. Therefore, HSMs protect keys both at rest and in-use. The drawback with HSMs is that they can be a costly hardware add-on to the server, and they require physical access to the server to get attached to it (via the USB port for example). Most HSMs support the PKCS#11 cryptographic programming interface. PKCS#11 PKCS#11 is the standard cryptographic programming interface supported by HSMs. The PKCS#11 interface is defined using a C-style definition, but many languages support bindings exist. Although applications can directly use the PKCS#11 programming interface, most applications use other cryptographic interfaces like openssl. Fortunately, openssl supports a PKCS#11 engine mechanism that converts openssl calls to PKCS#11calls. This allows applications written against the openssl cryptographic interface to use an HSM supporting the PKCS#11 interface without code change. Popular applications that use openssl but can still use an HSM to protect the key include Nginx and Apache. Features SGX Attestation Infrastructure The SGX Attestation Infrastructure allows to fetch PCK certificates and SGX collateral from Intel SGX Provisioning Certification Service (PCS). It makes the PCK certificates available to workloads that use the SKC Client, which allows them to generate SGX quotes. The SGX Attestation Infrastructure also includes components that perform the verification of SGX quotes. SGX Support in Orchestrators The SGX Attestation Infrastructure can optionally push the SGX information on compute nodes to cloud orchestrators so that SGX workloads (like SKC) can be scheduled on compute nodes that support SGX. Currently, the Kubernetes and Openstack orchestrators are supported. Key Protection SKC leverages the SGX Attestation Infrastructure to protect keys in an SGX enclave at rest and in use. Applications use the SKC Client -- a set of libraries -- to retrieves keys at runtime from KBS. KBS performs an SGX enclave attestation. If the attestation is successful, KBS generates a Symmetric Wrapping Key (SWK), wraps it with the enclave public key and provisions it into the enclave, which can unwrap it since it has the corresponding private key. Application can then be provisioned into the SGX enclave after being wrapped with the SWK. Application keys are therefore never exposed to any software outside of the enclave.","title":"Introduction"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#introduction","text":"","title":"Introduction"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#overview","text":"The SGX Attestation infrastructure and Secure Key Caching (SKC) are part of the Intel Security Libraries for datacenter (ISecL-DC). Intel Security Libraries for Datacenter is a collection of software applications and development libraries intended to help turn Intel platform security features into real-world security use cases. The SGX Attestation infrastructure provides an end to end support for registering SGX hosts and provisioning them with SGX material (PCK certificates) and SGX collateral (security patches information - TCB Information - and Certificate Revocation Lists - CRLs). The SGX Attestation infrastructure also provides support for generating SGX quotes for SGX enclaves hosted by workloads and verifying them by a remote attesting application. The remote attesting application can also use the SGX Attestation infrastructure to enforce enclave policies (like requiring a specific enclave signer). Optionally, the SGX Attestation Infrastructure allows to integrate with Cloud Orchestrators like Openstack and Kubernetes. SKC leverages the SGX Attestation Infrastructure to support the Secure Key Caching (SKC) use case.SKC provides the key protection at rest and in-use use case using the Intel Software Guard Extensions technology (SGX). SGX implements the Trusted Execution Environment (TEE) paradigm. Using the SKC Client -- a set of libraries -- applications can retrieve keys from the ISecL-DC Key Broker Service (KBS) and load them to an SGX-protected memory (called SGX enclave) in the application memory space. KBS performs the SGX enclave attestation to ensure that the application will store the keys in a genuine SGX enclave. Application keys are wrapped with an enclave public key by KBS prior to transferring to the application enclave. Consequently, application keys are protected from infrastructure admins, malicious applications and compromised HW/BIOS/OS/VMM. SKC does not require the refactoring of the application because it supports a standard PKCS#11 interface.","title":"Overview"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#trusted-execution-environment","text":"A Trusted Execution Environment (TEE) provides a computer secure area where code and data can be loaded with the assurance that their confidentiality and integrity are protected. TEEs have various applications in areas where the confidentiality of the data and the integrity of the code are of the highest importance. One examples of a TEE usage is the protection of sensitive secrets like cryptographic keys and authentication strings. These secrets can be preserved within a TEE, ensuring that they never get exposed to software that is outside of the TEE. Operations involving these secrets can be performed within the TEE, removing the need to ever extract the secrets outside of the TEE. Another example is the processing of data with privacy concerns. This is often the case in financial, healthcare and artificial intelligence (AI) applications. Putting sensitive operations inside a TEE allows organizations to support business processes without compromising the privacy expectations of their customers.","title":"Trusted Execution Environment"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#intel-software-guard-extensions","text":"Intel Software Guard Extensions (SGX) is an Intel platform security feature that implements the TEE paradigm. A portion of RAM called EPC (Enclave Page Cache) is used by applications to load secure isolated areas called SGX enclaves. Code and data inside SGX enclaves are encrypted and only decrypted inside the Intel CPU. From the host application perspective, an SGX enclave looks like a dynamic library. Any part of the application that is not contained in an SGX enclave is considered untrusted while the SGX enclave is considered trusted. Communications between the untrusted part and the trusted part (the SGX enclave) of an application uses a special calls called ECALLS and call from the enclave to the untrusted part of the application use OCALLS. A signed claim called SGX quote can be generated for an enclave. The SGX quote may contain a measurement of the code and the data of the enclave. An SGX quote allows to prove to a remote verifier (relying party) that an application includes the expected SGX enclave.","title":"Intel Software Guard Extensions"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#sgx-ecdsa-attestation","text":"SGX ECDSA attestation is the process that allows an application (relying party) to verify that a remote piece of code and data that it's interacting with is contained in a genuine Intel SGX enclave. The remote enclave can generate a signed claim called an SGX quote. A valid SGX quote signature generated on an SGX enabled platform can be chained up to a trusted Intel signing key. The SGX quote contains the measurement of the enclave (MREnclave), the enclave developer's signature (MRSigner), the security patch level of the platform (Trusted Computing Base or TCB) and any user data that the enclave wants to include in the quote. Typically, the user data in an SGX quote contains the hash of the public key part of a public/private key pair generated inside the enclave. The public key is transferred along with the SGX quote to the relying party. The latter generates a Symmetric Wrapping Key (SWK) and wraps it with the public key of the enclave. The wrapped SWK is provisioned into the SGX enclave, which can unwrap it since it has the corresponding private key. The relying party can then provision secrets into the SGX enclave after wrapping them with the SWK. For an enclave to generate an SGX quote, a PCK certificate for the host platform must be obtained from Intel SGX Provisioning Certification Service (PCS).","title":"SGX ECDSA Attestation"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#pck-certificates-provisioning","text":"To generate an SGX quote for an enclave, a PCK certificate must be obtained from SGX Provisioning Certification Service (PCS). Requiring a workload to retrieve the PCK certificate from Intel PCS at the time of the SGX quote generation can be detrimental to the workload. Network connectivity issues can prevent the connection to Intel PCS. To remove the dependency on network connectivity, the PCK certificates of the data center platforms are fetched before running any workload. This is achieved by extracting SGX related information from the platform using the PCK ID Retrieval tool and pushing it to a Caching Service running in the same data center. The Caching Service then retrieves the PCK certificates of all the platforms that pushed SGX information to it from Intel PCS. Network connectivity issues are not a problem since the Caching Service can retry if needed. When an SGX workload needs its PCK certificate, it can just get it from the Caching Service.","title":"PCK Certificates Provisioning"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#key-protection","text":"Cryptographic keys are high value assets that must be protected against disclosure and corruption. Key disclosure or corruption expose the key owner to data confidentiality breaches, impersonation and denial of service. The industry has good solutions to protect keys at rest. A popular solution is to store keys in a central secure Key Management System (KMS), and applications retrieve them at runtime. However, this solution does not protect keys once they are in RAM and used to perform cryptographic operations. Keys in RAM can be disclosed because of software vulnerabilities like Heartbleed or because of memory snapshots. Therefore, keys are not protected in use. This concern can be addressed by having the application send the payload that needs cryptographic processing to the KMS where the processing happens instead. By doing this, the key is never exposed in RAM. However, this solution incurs an overhead caused by the network round trip to the KMS. Another solution is to store keys in Hardware Security Modules (HSMs)","title":"Key Protection"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#hsm","text":"A Hardware Security Module or HSM is a separate hardware part that can be attached to a server. HSMs provide APIs to create and load keys. HSMs also support APIs to perform cryptographic operations using keys stored inside them. The typical flow for using an HSM is to create or load a key in the HSM in a secure environment then take the HSM to the server where the workload runs and attach it to this server. The application then performs cryptographic operations using the key inside the HSM. This ensures that the key is never exposed in RAM. Therefore, HSMs protect keys both at rest and in-use. The drawback with HSMs is that they can be a costly hardware add-on to the server, and they require physical access to the server to get attached to it (via the USB port for example). Most HSMs support the PKCS#11 cryptographic programming interface.","title":"HSM"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#pkcs11","text":"PKCS#11 is the standard cryptographic programming interface supported by HSMs. The PKCS#11 interface is defined using a C-style definition, but many languages support bindings exist. Although applications can directly use the PKCS#11 programming interface, most applications use other cryptographic interfaces like openssl. Fortunately, openssl supports a PKCS#11 engine mechanism that converts openssl calls to PKCS#11calls. This allows applications written against the openssl cryptographic interface to use an HSM supporting the PKCS#11 interface without code change. Popular applications that use openssl but can still use an HSM to protect the key include Nginx and Apache.","title":"PKCS#11"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#features","text":"","title":"Features"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#sgx-attestation-infrastructure","text":"The SGX Attestation Infrastructure allows to fetch PCK certificates and SGX collateral from Intel SGX Provisioning Certification Service (PCS). It makes the PCK certificates available to workloads that use the SKC Client, which allows them to generate SGX quotes. The SGX Attestation Infrastructure also includes components that perform the verification of SGX quotes.","title":"SGX Attestation Infrastructure"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#sgx-support-in-orchestrators","text":"The SGX Attestation Infrastructure can optionally push the SGX information on compute nodes to cloud orchestrators so that SGX workloads (like SKC) can be scheduled on compute nodes that support SGX. Currently, the Kubernetes and Openstack orchestrators are supported.","title":"SGX Support in Orchestrators"},{"location":"product-guides/SGX%20Infrastructure/1Introduction/#key-protection_1","text":"SKC leverages the SGX Attestation Infrastructure to protect keys in an SGX enclave at rest and in use. Applications use the SKC Client -- a set of libraries -- to retrieves keys at runtime from KBS. KBS performs an SGX enclave attestation. If the attestation is successful, KBS generates a Symmetric Wrapping Key (SWK), wraps it with the enclave public key and provisions it into the enclave, which can unwrap it since it has the corresponding private key. Application can then be provisioned into the SGX enclave after being wrapped with the SWK. Application keys are therefore never exposed to any software outside of the enclave.","title":"Key Protection"},{"location":"product-guides/SGX%20Infrastructure/2SGX%20Attestation%20Infrastructure%20and%20SKC%20Components/","text":"SGX Attestation Infrastructure and SKC Components The components documented in this section are used by the SGX Attestation Infrastructure and therefore by SKC, which leverages the SGX Attestation Infrastructure. Components that are exclusively used by SKC have (SKC Only) in the corresponding sub-section title. Certificate Management Service All the certificates used by SKC services and by the SGX Agent are issued by the Certificate Management Service (CMS). CMS has a root CA certificate and all the SKC services and the SGX Agent certificates chain up to the CMS root CA. CMS is an infrastructure service and is shared with other Intel\u00ae SecL-DC components. Authentication and Authorization Service The authentication and authorization for all SKC services and the SGX Agent are centrally managed by the Authentication and Authorization Service (AAS). AAS is an infrastructure service and is shared with other Intel\u00ae SecL-DC components. SGX Caching Service The SGX Caching Service (SCS) allows to retrieve the PCK certificates of the data center server platforms from Intel SGX Provisioning Certification Service (PCS). SCS retrieves also platform models collateral. The collateral consists of the security patches (TCBInfo) that have been issued for Intel platform models. Finally, SCS retrieves the Certificate Revocation Lists (CRLs). Since the Caching Service stores all the TCBInfo of all the platform models in the datacenter, the SGX Quote Verification Service (SQVS) uses it to determine the TCB status of the platforms in the data center. The SKC Client retrieves its PCK certificate from the Caching Service when it generates an SGX quote. SCS can be deployed in both Cloud Service Provider (CSP) and tenant environments. In the CSP environment, SCS is used to fetch PCK certificates for compute nodes in the data center. In the tenant environment, it's used to cache SGX collateral information used in verifying SGX quotes. SGX Host Verification Service If SGX Host Verification Service API URL is specified in SGX Agent env file, then SGX Agent will push the platform enablement info and TCB status to SHVS at regular interval, else, Agent pushes the platform enablement info and TCB status to SHVS periodically. The SGX enablement information consists of SGX discovery information (SGX supported, SGX enabled, FLC enabled and EPC memory size). SGX Agent The SGX Agent resides on physical servers and pushes SGX platform specific values to SGX Caching Service (SCS). If SGX Host Verification Service (SHVS) URL is specified in SGX Agent env file, SGX Agent would fetch the TCB status from SCS and updates SHVS with platform enablement info and TCB status periodically. Integration Hub The Integration Hub (IHUB) allows to support SGX in Kubernetes and Open stack. IHUB pulls the list of hosts details from Kubernetes and then using the host information it pulls the SGX Data from SGX Host Verification Service and pushes it to Kubernetes. IHUB performs these steps on a regular basis so that the most recent SGX information about nodes is reflected in Kubernetes and Openstack. This integration allows Kubernetes and Openstack to schedule VMs and containers that need to run SGX workloads on compute nodes that support SGX. The SGX data that IHUB pushes to Kubernetes consists of SGX enabled/disabled, SGX supported/not supported, FLC enabled/not enabled, EPC memory size, TCB status up to date/not up to date and platform-data expiry time. Key Broker Service (SKC Only) The Key Broker Service (KBS) is typically deployed in the tenant environment, not the Cloud Service Provider (CSP) environment. KBS is effectively a policy compliance engine. Its job is to manage key transfer requests from SKC Clients, releasing keys only to those that meet policy requirements. A user admin can create and register keys in KBS. He can also create key policies and assign them to keys. A key policy specifies the conditions that the SKC Client must fulfill for keys that have the policy assigned to them to be released. Most of the information about an SKC Client is contained in the SGX quote that it sends to KBS. The SGX quote also contains a hash of the enclave's public key. KBS gets the public key along the quote so the hash in the quote allows to verify that the public key is genuine. If the SGX quote verification (attestation) is successful, KBS generates a Symmetric Wrapping Key (SWK), wraps it with the enclave public key and provisions it into the enclave, which can unwrap it since it has the corresponding private key. Application can then be provisioned into the SGX enclave after being wrapped with the SWK. Application keys are therefore never exposed to any software outside of the enclave. KBS is shared with other Intel\u00ae SecL-DC components. SGX Quote Verification Service The SGX Quote Verification Service (SQVS) is typically deployed in the tenant environment, not the Cloud Service Provider (CSP) environment. SQVS performs the verification of SGX quotes on behalf of KBS. SQVS determines if the SGX quote signature is valid. It also determines if the SGX quote has been generated on a platform that is up to date on security patches (TCB). For the latter, SQVS uses the SGX Caching Service, which caches the SGX collateral information about Intel platform models. SQVS also parses the SGX quote and extracts the entities and returns them to KBS, which can then make additional policy decisions based on the values of the theses entities. The Workload SGX Dependencies This is a set of dependencies needed by SGX workloads. The SKC Client (Secure Key Caching Use case Only) The SKC Client refers to a suite of libraries that applications that require key protection must link with. It's comprised of the SKC Library, which is an Intel\u00ae SecL-DC component and the Intel Crypto Toolkit. the SKC Client uses the workload SGX dependencies component. The SKC Library supports the PKCS#11 interface and is therefore considered as a PKCS#11 module from the host application perspective. The SKC Library uses Intel Crypto Toolkit to protect keys in an SGX enclave. When a key is requested by the host application, the SKC Library sends a request to the Key Broker Service (KBS) along with an SGX quote that is generated by the Crypto Toolkit. KBS releases the key after verifying the quote and evaluating the attributes contained in the quote. The key policy can also specify conditions that can't be verified with the SGX quote alone. The SKC Client is typically deployed inside a tenant VM or container. It can also be used on bare metal. In all these deployments, the underlying platform is typically owned by a Cloud Service Provider (CSP) and is considered untrusted. Definitions, Acronyms, and Abbreviation SKC -- Secure Key Caching SGX -- Software Guard Extension TEE -- Trusted Execution Environment HSM -- Hardware Security Module KBS -- Key Broker Service CSP -- Cloud Service Provider PCS -- Provisioning Certification Service CRLs -- Certificate Revocation Lists AAS -- Authentication and Authorization Service SWK -- Symmetric Wrapping Key CRDs -- Custom Resource Definitions Architecture Overview As indicated in the Features section, SKC provides 3 features essentially: SGX Attestation Support: this is the feature that CSPs provide to tenants who need to run SGX workloads that require attestation. SGX Support in Orchestrators: this feature allows to discover SGX support in physical servers and related information: SGX supported. SGX enabled. Size of RAM reserved for SGX. It's called Enclave Page Cache (EPC). Flexible Launch Control enabled. Key Protection: this is the feature used by tenants using a CSP to run workloads with key protection requirements. The high-level architectures of these features are presented in the next sub-sections. SGX Attestation Support and SGX Support in Orchestrators The diagram below shows the infrastructure that CSPs need to deploy to support SGX attestation and optionally, integration with orchestrators (Kubernetes and OpenStack). THE SGX Agent pushes platform information to SGX Caching Service (SCS), which uses it to get the PCK Certificate and other SGX collateral from the Intel SGX Provisioning Certification Service (PCS) and caches them locally. When a workload on the platform needs to generate an SGX Quote, it retrieves the PCK Certificate of the platform from SCS. If SGX Host Verification Service (SHVS) URL is configured, the SGX Agent fetches the TCB Status from SCS and updates SHVS with SGX platform enablement information and TCB status periodically. The platform information is made available to Kubernetes and Openstack via the SGX Hub (IHUB), which pulls it from SHVS. The SGX Quote Verification Service (SQVS) allows attesting applications to verify SGX quotes and extract the SGX quote attributes to verify compliance with a user-defined SGX enclave policy. SQVS uses the SGX Caching Service to retrieve SGX collateral needed to verify SGX quotes from the Intel SGX Provisioning Certification Service (PCS). SQVS typically runs in the attesting application owner network environment. Typically, a separate instance of the SGX Caching Service is setup in the attesting application owner network environment. The SGX Agent and the SGX services integrate with the Authentication and Authorization Service (AAS) and the Certificate Management Service (CMS). AAS and CMS are not represented on the diagram for clarity. Key Protection Key Protection leverages the SGX Attestation support and optionally, the SGX support in orchestrators. Key Protection is implemented by the SKC Client -- a set of libraries - which must be linked with a tenant workload, like Nginx, deployed in a CSP environment and the Key Broker Service (KBS) deployed in the tenant's enterprise environment. The SKC Client retrieves the keys needed by the workload from KBS after proving that the key can be protected in an SGX enclave as shown in the diagram below. Step 6 is optional (keys can be stored in KBS). Keys policies in step 2 are called Key Transfer Policies and are created by an Admin and assigned to Application keys. SKC Virtualization (Supported only on RHEL 8.2, not supported on Ubuntu 18.04) Virtualization enabled on SGX Host machines, uses SGX key features. With Virtualization being enabled on SGX host, SKC Library which uses Intel crypto tool kit to protect keys in SGX Enclave can be configured on Virtual Machines which are created on SGX Hosts. This enhancement further provides the privilege for a workload on a VM allowing successful Secure Key transfer flow which meets the policy requirements. Hence virtualization on SGX Hosts supports key transfer flow for Workload on bare metal, Workload inside a VM, Workload in a container and Workload in a container inside a VM enabled on SGX Host.","title":"SGX Attestation Infrastructure and SKC Components"},{"location":"product-guides/SGX%20Infrastructure/2SGX%20Attestation%20Infrastructure%20and%20SKC%20Components/#sgx-attestation-infrastructure-and-skc-components","text":"The components documented in this section are used by the SGX Attestation Infrastructure and therefore by SKC, which leverages the SGX Attestation Infrastructure. Components that are exclusively used by SKC have (SKC Only) in the corresponding sub-section title. Certificate Management Service All the certificates used by SKC services and by the SGX Agent are issued by the Certificate Management Service (CMS). CMS has a root CA certificate and all the SKC services and the SGX Agent certificates chain up to the CMS root CA. CMS is an infrastructure service and is shared with other Intel\u00ae SecL-DC components. Authentication and Authorization Service The authentication and authorization for all SKC services and the SGX Agent are centrally managed by the Authentication and Authorization Service (AAS). AAS is an infrastructure service and is shared with other Intel\u00ae SecL-DC components. SGX Caching Service The SGX Caching Service (SCS) allows to retrieve the PCK certificates of the data center server platforms from Intel SGX Provisioning Certification Service (PCS). SCS retrieves also platform models collateral. The collateral consists of the security patches (TCBInfo) that have been issued for Intel platform models. Finally, SCS retrieves the Certificate Revocation Lists (CRLs). Since the Caching Service stores all the TCBInfo of all the platform models in the datacenter, the SGX Quote Verification Service (SQVS) uses it to determine the TCB status of the platforms in the data center. The SKC Client retrieves its PCK certificate from the Caching Service when it generates an SGX quote. SCS can be deployed in both Cloud Service Provider (CSP) and tenant environments. In the CSP environment, SCS is used to fetch PCK certificates for compute nodes in the data center. In the tenant environment, it's used to cache SGX collateral information used in verifying SGX quotes. SGX Host Verification Service If SGX Host Verification Service API URL is specified in SGX Agent env file, then SGX Agent will push the platform enablement info and TCB status to SHVS at regular interval, else, Agent pushes the platform enablement info and TCB status to SHVS periodically. The SGX enablement information consists of SGX discovery information (SGX supported, SGX enabled, FLC enabled and EPC memory size). SGX Agent The SGX Agent resides on physical servers and pushes SGX platform specific values to SGX Caching Service (SCS). If SGX Host Verification Service (SHVS) URL is specified in SGX Agent env file, SGX Agent would fetch the TCB status from SCS and updates SHVS with platform enablement info and TCB status periodically. Integration Hub The Integration Hub (IHUB) allows to support SGX in Kubernetes and Open stack. IHUB pulls the list of hosts details from Kubernetes and then using the host information it pulls the SGX Data from SGX Host Verification Service and pushes it to Kubernetes. IHUB performs these steps on a regular basis so that the most recent SGX information about nodes is reflected in Kubernetes and Openstack. This integration allows Kubernetes and Openstack to schedule VMs and containers that need to run SGX workloads on compute nodes that support SGX. The SGX data that IHUB pushes to Kubernetes consists of SGX enabled/disabled, SGX supported/not supported, FLC enabled/not enabled, EPC memory size, TCB status up to date/not up to date and platform-data expiry time. Key Broker Service (SKC Only) The Key Broker Service (KBS) is typically deployed in the tenant environment, not the Cloud Service Provider (CSP) environment. KBS is effectively a policy compliance engine. Its job is to manage key transfer requests from SKC Clients, releasing keys only to those that meet policy requirements. A user admin can create and register keys in KBS. He can also create key policies and assign them to keys. A key policy specifies the conditions that the SKC Client must fulfill for keys that have the policy assigned to them to be released. Most of the information about an SKC Client is contained in the SGX quote that it sends to KBS. The SGX quote also contains a hash of the enclave's public key. KBS gets the public key along the quote so the hash in the quote allows to verify that the public key is genuine. If the SGX quote verification (attestation) is successful, KBS generates a Symmetric Wrapping Key (SWK), wraps it with the enclave public key and provisions it into the enclave, which can unwrap it since it has the corresponding private key. Application can then be provisioned into the SGX enclave after being wrapped with the SWK. Application keys are therefore never exposed to any software outside of the enclave. KBS is shared with other Intel\u00ae SecL-DC components. SGX Quote Verification Service The SGX Quote Verification Service (SQVS) is typically deployed in the tenant environment, not the Cloud Service Provider (CSP) environment. SQVS performs the verification of SGX quotes on behalf of KBS. SQVS determines if the SGX quote signature is valid. It also determines if the SGX quote has been generated on a platform that is up to date on security patches (TCB). For the latter, SQVS uses the SGX Caching Service, which caches the SGX collateral information about Intel platform models. SQVS also parses the SGX quote and extracts the entities and returns them to KBS, which can then make additional policy decisions based on the values of the theses entities. The Workload SGX Dependencies This is a set of dependencies needed by SGX workloads. The SKC Client (Secure Key Caching Use case Only) The SKC Client refers to a suite of libraries that applications that require key protection must link with. It's comprised of the SKC Library, which is an Intel\u00ae SecL-DC component and the Intel Crypto Toolkit. the SKC Client uses the workload SGX dependencies component. The SKC Library supports the PKCS#11 interface and is therefore considered as a PKCS#11 module from the host application perspective. The SKC Library uses Intel Crypto Toolkit to protect keys in an SGX enclave. When a key is requested by the host application, the SKC Library sends a request to the Key Broker Service (KBS) along with an SGX quote that is generated by the Crypto Toolkit. KBS releases the key after verifying the quote and evaluating the attributes contained in the quote. The key policy can also specify conditions that can't be verified with the SGX quote alone. The SKC Client is typically deployed inside a tenant VM or container. It can also be used on bare metal. In all these deployments, the underlying platform is typically owned by a Cloud Service Provider (CSP) and is considered untrusted.","title":"SGX Attestation Infrastructure and SKC Components"},{"location":"product-guides/SGX%20Infrastructure/2SGX%20Attestation%20Infrastructure%20and%20SKC%20Components/#definitions-acronyms-and-abbreviation","text":"SKC -- Secure Key Caching SGX -- Software Guard Extension TEE -- Trusted Execution Environment HSM -- Hardware Security Module KBS -- Key Broker Service CSP -- Cloud Service Provider PCS -- Provisioning Certification Service CRLs -- Certificate Revocation Lists AAS -- Authentication and Authorization Service SWK -- Symmetric Wrapping Key CRDs -- Custom Resource Definitions","title":"Definitions, Acronyms, and Abbreviation"},{"location":"product-guides/SGX%20Infrastructure/2SGX%20Attestation%20Infrastructure%20and%20SKC%20Components/#architecture-overview","text":"As indicated in the Features section, SKC provides 3 features essentially: SGX Attestation Support: this is the feature that CSPs provide to tenants who need to run SGX workloads that require attestation. SGX Support in Orchestrators: this feature allows to discover SGX support in physical servers and related information: SGX supported. SGX enabled. Size of RAM reserved for SGX. It's called Enclave Page Cache (EPC). Flexible Launch Control enabled. Key Protection: this is the feature used by tenants using a CSP to run workloads with key protection requirements. The high-level architectures of these features are presented in the next sub-sections.","title":"Architecture Overview"},{"location":"product-guides/SGX%20Infrastructure/2SGX%20Attestation%20Infrastructure%20and%20SKC%20Components/#sgx-attestation-support-and-sgx-support-in-orchestrators","text":"The diagram below shows the infrastructure that CSPs need to deploy to support SGX attestation and optionally, integration with orchestrators (Kubernetes and OpenStack). THE SGX Agent pushes platform information to SGX Caching Service (SCS), which uses it to get the PCK Certificate and other SGX collateral from the Intel SGX Provisioning Certification Service (PCS) and caches them locally. When a workload on the platform needs to generate an SGX Quote, it retrieves the PCK Certificate of the platform from SCS. If SGX Host Verification Service (SHVS) URL is configured, the SGX Agent fetches the TCB Status from SCS and updates SHVS with SGX platform enablement information and TCB status periodically. The platform information is made available to Kubernetes and Openstack via the SGX Hub (IHUB), which pulls it from SHVS. The SGX Quote Verification Service (SQVS) allows attesting applications to verify SGX quotes and extract the SGX quote attributes to verify compliance with a user-defined SGX enclave policy. SQVS uses the SGX Caching Service to retrieve SGX collateral needed to verify SGX quotes from the Intel SGX Provisioning Certification Service (PCS). SQVS typically runs in the attesting application owner network environment. Typically, a separate instance of the SGX Caching Service is setup in the attesting application owner network environment. The SGX Agent and the SGX services integrate with the Authentication and Authorization Service (AAS) and the Certificate Management Service (CMS). AAS and CMS are not represented on the diagram for clarity.","title":"SGX Attestation Support and SGX Support in Orchestrators"},{"location":"product-guides/SGX%20Infrastructure/2SGX%20Attestation%20Infrastructure%20and%20SKC%20Components/#key-protection","text":"Key Protection leverages the SGX Attestation support and optionally, the SGX support in orchestrators. Key Protection is implemented by the SKC Client -- a set of libraries - which must be linked with a tenant workload, like Nginx, deployed in a CSP environment and the Key Broker Service (KBS) deployed in the tenant's enterprise environment. The SKC Client retrieves the keys needed by the workload from KBS after proving that the key can be protected in an SGX enclave as shown in the diagram below. Step 6 is optional (keys can be stored in KBS). Keys policies in step 2 are called Key Transfer Policies and are created by an Admin and assigned to Application keys.","title":"Key Protection"},{"location":"product-guides/SGX%20Infrastructure/2SGX%20Attestation%20Infrastructure%20and%20SKC%20Components/#skc-virtualization-supported-only-on-rhel-82-not-supported-on-ubuntu-1804","text":"Virtualization enabled on SGX Host machines, uses SGX key features. With Virtualization being enabled on SGX host, SKC Library which uses Intel crypto tool kit to protect keys in SGX Enclave can be configured on Virtual Machines which are created on SGX Hosts. This enhancement further provides the privilege for a workload on a VM allowing successful Secure Key transfer flow which meets the policy requirements. Hence virtualization on SGX Hosts supports key transfer flow for Workload on bare metal, Workload inside a VM, Workload in a container and Workload in a container inside a VM enabled on SGX Host.","title":"SKC Virtualization (Supported only on RHEL 8.2, not supported on Ubuntu 18.04)"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/","text":"Intel\u00ae Security Libraries Installation Building from Source Intel\u00ae Security Libraries is distributed as open source code and must be compiled into installation binaries before installation. Instructions and sample scripts for building the Intel\u00ae SecL-DC components can be found here . (Section 1 to 6) After the components have been built, the installation binaries and database scripts can be found in the binaries directory created by the build scripts. Generated component binaries/installers are: CMS: cms-v4.0.0.bin AAS: authservice-v4.0.0.bin SCS: scs-v4.0.0.bin SHVS: shvs-v4.0.0.bin IHUB: ihub-v4.0.0.bin SQVS: sqvs-v4.0.0.bin KBS: kbs-v4.0.0.bin K8S-Extensions: isecl-k8s-extensions-v4.0.0.tar.gz SGX-Agent: agent_untar.sh, sgx_agent.sha2 and sgx_agent.tar SKC-Library: skclib_untar.sh, skc_library.sha2 and skc_library.tar DB scripts: Postgres installation script: install_pgdb.sh AAS, SCS and SHVS DB creation script: create_db.sh Building from Source - OCI images & K8s Manifests Intel\u00ae Security Libraries is distributed as open source code and must be compiled into OCI images before installation. Instructions and sample scripts for building the Intel\u00ae SecL-DC components as containerized images for Kubernetes deployments can be found in Quick Start guide . After the components have been built, the OCI images and pre-req scripts can be found in the K8s directory created by the build scripts. Generated components OCI images under <working directory>/k8s/container-images : Authentication Authorization Service: aas-v4.0.0.tar Certificate Management Service: cms-v4.0.0.tar Integration Hub: ihub-v4.0.0.tar Key Broker Service: kbs-v4.0.0.tar K8s Extensions Custom Controller: isecl-k8s-controller-v4.0.0.tar K8s Extensions Extended Scheduler: isecl-k8s-scheduler-v4.0.0.tar SGX Caching Service: scs-v4.0.0.tar SGX Quote Verification Service: sqvs-v4.0.0.tar SGX Host Verification Service: shvs-v4.0.0.tar SGX Agent: sgx-agent-v4.0.0.tar SKC Library: skc-lib-4.0.0.tar Generated Components K8s Manifests directories under <working directory>/k8s/manifests : Authentication Authorization Service Database: aas-db SGX Caching Service Database: scs-db SGX Host Verification Service Database: shvs-db Certificate Management Service: cms Authentication Authorization Service: aas Integration Hub: ihub Key Broker Service: kbs SGX Caching Service: scs SGX Host Verification Service: shvs SGX Quote Verification Service: sqvs K8s Extensions Custom Controller: k8s-extensions-controller K8s Extensions Extended Scheduler: k8s-extensions-scheduler SGX Agent: sgx_agent SKC Library: skc_library Bootstrap scripts and answer file under <working directory>/k8s/manifests : Pre-req: pre-requisites.sh Bootstrap DB: skc-bootstrap-db-services.sh Answer file: isecl-skc-k8s.env Bootstrap: skc-bootstrap.sh Hardware Considerations Supported Hardware Intel\u00ae Xeon\u00ae SP products those support SGX BIOS Requirements Intel\u00ae SGX-TEM BIOS requirements are outlined in the latest Intel\u00ae SGX Platforms BIOS Writer's Guide Intel\u00ae SGX should be enabled in BIOS menu (Intel\u00ae SGX is Disabled by default on Ice Lake). Intel\u00ae SGX BIOS requirements include exposing Flexible Launch Control menu. OS Requirements (Intel\u00ae SGX does not supported on 32-bit OS): Linux: RHEL 8.2 Linux: Ubuntu 18.04 Requirements for Containerized Deployment with K8s Operating System: RHEL 8.2 Ubuntu 18.04 Kubernetes Single-node: microk8s (1.17.17) Multi-node: kubeadm (1.17.17) Container Runtime Docker 19.03.13 on RHEL 8.2 Docker 19.03.5 on Ubuntu 18.04 Storage: hostPath for Single Node microk8s for all services and agents NFS for Multi Node kubeadm for all services, hostPath for sgx_agent and skc_library Recommended Service Layout The Intel\u00ae SecL-DC services can be installed in a variety of layouts, partially depending on the use cases desired and the OS of the server(s) to be protected. In general, the Intel\u00ae SecL-DC applications can be divided into management services that are deployed on the network on the management plane, and host or node components that must be installed on each protected server. Management services can typically be deployed anywhere with network access to all the protected servers. This could be a set of individual VMs per service; containers; or all installed on a single physical or virtual machine. Node components must be installed on each protected physical server. Typically, this is needed for Linux deployments. For a POC environment, all the management services can be installed on a single machine or VM. This Includes: Certificate Management Service (CMS) Authentication and Authorization Service (AAS) SGX Caching Service (SCS) SGX Host Verification Service (SHVS) Integration HUB (IHUB) Key Broker Service (KBS) with backend key management SGX Quote Verification Service (SQVS) SGX Agent & SKC Library needs to be installed on SGX Enabled Machine. Isecl-K8s-extensions must be installed on separate VM. Recommended Service Layout & Architecture - Containerized Deployment with K8s The containerized deployment makes use of Kubernetes orchestrator for single node and multi node deployments. The supported deployment models are as below: Single Node: Multi Node: Services Deployments & Agent DaemonSets: Every service including databases will be deployed as separate K8s deployment with 1 replica, i.e(1 pod per deployment). Each deployment will be further exposed through k8s service and also will be having corresponding Persistent Volume Claims(PV) for configuration and log directories and mounted on persistent storage. In case of daemonsets/agents, the configuration and log directories will be mounted on respective Baremetal worker nodes. For stateful services which requires database like shvs, aas, scs, A separate database deployment will be created for each of such services. The data present on the database deployment will also made to persist on a NFS, through K8s persistent storage mechanism Networking within the Cluster: Networking Outside the Cluster: SKC Virtualization: Follow the Installation of Containerized Services and Agent in K8s Cluster for installation instructions once deployment model is chosen. Using the provided Database Installation Script Install a sample Postgresql 11 database using the install_pgdb.sh script provided in binaries directory. This script will automatically install the Postgresql database and client packages required. Create the iseclpgdb.env answer file: ISECL_PGDB_IP_INTERFACES = localhost ISECL_PGDB_PORT = 5432 ISECL_PGDB_SAVE_DB_INSTALL_LOG = true ISECL_PGDB_CERT_DNS = localhost ISECL_PGDB_CERT_IP = 127 .0.0.1 Note that the values above assume that the database will be accessed locally. If the database server will be external to the Intel\u00ae SecL services, change these values to the hostname or FQDN and IP address where the client will access the database server. Provisioning the Database Each Intel\u00ae SecL service that uses a database (the Authentication and Authorization Service, the SGX host Verification Service, the SGX caching Service,) requires its own schema and access. The database must be created and initialized. Execute the install_pgdb.sh script to install the database. If a single shared database server will be used for each Intel\u00ae SecL service (for example, if all management plane services will be installed on a single VM), run install_pgdb.sh script only once and create_db.sh script for each component that uses a database. If separate database servers will be used (for example, if the management plane services will reside on separate systems and will use their own local database servers), execute the install_pgdb.sh script on each server hosting a database and create_db.sh script for each component that uses a DB. Command to install postgres DB: ./install_pgdb.sh Command to create DB for AAS/SCS/SHVS: ./create_db.sh <DB Name> <DB Username> <DB Password> Note that the db name, db username and db user password should match with the respective component environment files. Database Server TLS Certificate The database client for Intel\u00ae SecL services requires the database TLS certificate to authenticate communication with the database server. If the database server for a service is located on the same server that the service will run on, only the path to this certificate is needed. If the provided Postgres scripts are used, the certificate will be in /usr/local/pgsql/data/server.crt. If the database server will be run separately from the Intel\u00ae SecL service(s), the certificate will need to be copied from the database server to the service machine before installing the Intel\u00ae SecL services. The database client for Intel\u00ae SecL services will validate that the Subject Alternative Names in the database server's TLS certificate contain the hostname(s)/IP address(es) that the clients will use to access the database server. If configuring a database without using the provided scripts, ensure that these attributes are present in the database TLS certificate. Installation of Containerized Services and Agent in K8s Cluster The containerized deployment utilizes K8s orchestrator to deploy SGX components. The deployments are fairly automated once the pre-reqs are in place for K8s cluster deployments. Note The K8s manifests are declarative in nature and the same can be modified as required for SGX services deployments for single node and multi node deployments. Modifications would require specific steps to ensure services and agents get updated as per the required configuration. More details for the same present in Setup Task Flows for K8s Deployments, Configuration Update Flows for K8s Deployments and Intel Security Libraries Configuration Settings . Pre-requisites Ensure based on the deployment model , microk8s or kubeadm in installed. Supported versions in Requirements for Containerized Deployment with K8s . Docker runtime is configured for each of these deployments. Supported versions in Requirements for Containerized Deployment with K8s . The build would generate a script for platform dependencies under <working directory>/k8s/platform dependencies . Follow the deployment pre-reqs as given in the Quick Start guide based on the chosen deployment model. Deploy Steps The deploy steps are detailed in the Quick Start guide based on the deployment model. Follow the instructions for the deployment using the scripts. Additional Details Default Service and Agent Mount Paths - Single Node Default Service and Agent Mount Paths - Multi Node Default Service Ports NFS Setup Pre-reqs - Multi Node Installing the Certificate Management Service Required For The CMS is REQUIRED for all use cases. Supported Operating System The Intel\u00ae Security Libraries Certificate Management Service supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver). Recommended Hardware 1 vCPUs RAM: 2 GB 10 GB One network interface with network access to all Intel\u00ae SecL-DC services Installation To install the Intel\u00ae SecL-DC Certificate Management Service: Copy the Certificate Management Service installation binary to the /root/ directory. Create the cms.env installation answer file for an unattended installation: AAS_TLS_SAN = \\< Comma-Separated list of IPs and hostnames for the AAS \\> AAS_API_URL = https:// \\< Authentication and Authorization Service IP or Hostname \\> :8444/aas/v1 SAN_LIST = < Comma-Separated list of IP addresses and hostnames for the CMS> The SAN list will be used to authenticate the Certificate Signing Request from the AAS to the CMS. Only a CSR originating from a host matching the SAN list will be honored. Later, in the AAS authservice.env installation answer file, this same SAN list will be provided for the AAS installation. These lists must match and must be valid for IPs and/or hostnames used by the AAS system. The SAN list variables also accept the wildcards \"?\" (for single-character wildcards) and \"*\" (for multiple-character wildcards) to allow address ranges or multiple FQDNs. The AAS_API_URL represents the URL for the AAS that will exist after the AAS is installed. For all configuration options and their descriptions, refer to the Intel\u00ae SecL Configuration section on the Certificate Management Service. Execute the installer binary. ./cms-v4.0.0.bin When the installation completes, the Certificate Management Service is available. The services can be verified by running cms status from the command line. cms status After installation is complete, the CMS will output a bearer token to the console. This token will be used with the AAS during installation to authenticate certificate requests to the CMS. If this token expires or otherwise needs to be recreated, use the following command: cms setup cms-auth-token \\--force In addition, the SHA384 digest of the CMS TLS certificate will be needed for installation of the remaining Intel\u00ae SecL services. The digest can be obtained using the following command: cms tlscertsha384 Installing the Authentication and Authorization Service Required For The AAS is REQUIRED for all use cases. Prerequisites The following must be completed before installing the Authentication and Authorization Service: The Certificate Management Service must be installed and available The Authentication and Authorization Service database must be available Package Dependencies The Intel\u00ae SecL-DC Authentication and Authorization Service (AAS) requires a Postgresql 11 database. script (install_pgdb.sh) is provided with the AAS that will install Postgresql repositories. Supported Operating Systems The Intel\u00ae Security Libraries Authentication and Authorization Service supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver). Recommended Hardware 1 vCPUs RAM: 2 GB 10 GB One network interface with network access to all Intel\u00ae SecL-DC services Installation Before AAS is installed, Database needs to be created. Use the following commands to install postgres and create AAS DB copy install_pgdb.sh and create_db.sh to /root/ directory ./install_pgdb.sh ./create_db.sh <db name> <db_user> <db_password> To install the AAS, a bearer token from the CMS is required. This bearer token is output at the end of the CMS installation. However, if a new token is needed, simply use the following command from the CMS command line: cms setup cms-auth-token --force Create the authservice.env installation answer file in /root/ directory as below: CMS_BASE_URL=https://< CMS IP or hostname>:8445/cms/v1/ CMS_TLS_CERT_SHA384=<CMS TLS certificate sha384> AAS_DB_SSLMODE=verify-full AAS_DB_HOSTNAME=<IP or hostname of database server> AAS_DB_PORT=<database port number; default is 5432> AAS_DB_NAME=<database name> AAS_DB_USERNAME=<database username> AAS_DB_PASSWORD=<database password> AAS_DB_SSLCERTSRC=<path to database TLS certificate; the default location is typically /usr/local/pgsql/data/server.crt> AAS_ADMIN_USERNAME=<username for AAS administrative user> AAS_ADMIN_PASSWORD=<password for AAS administrative user> AAS_JWT_TOKEN_DURATION_MINS=2880 SAN_LIST=<comma-separated list of IPs and hostnames for the AAS; this should match the value for the AAS_TLS_SAN in the cms.env file from the CMS installation> BEARER_TOKEN=<bearer token from CMS installation> Execute the AAS installer: ./authservice-v4.0.0.bin Note The AAS_ADMIN credentials specified in this answer file will have administrator rights for the AAS and can be used to create other users, create new roles, and assign roles to users. Creating Users Before deployment is initiated, user account and roles must be generated for each component. Most of these accounts will be service users, used by the various Intel\u00ae SecL SKC services to work together. Creating these required users and roles is facilitated by the populate-user script. Creating Users and Roles During installation of each services, number of user accounts and roles specific to services must be generated. Most of these accounts will be service users, which is used by the various services to function together. Another set of users will be used for installation permissions, and administrative user will be created to provide the initial authentication interface for the actual user based on the organizational requirements. Creating these required users and roles is facilitated by a script that will accept credentials and configuration settings from an answer file and automate the process. Create the populate-users.env file using the following values: # SKC Components include AAS,SCS,SHVS,SQVS,SIH and SKBS. ISECL_INSTALL_COMPONENTS = AAS,SCS,SHVS,SQVS,SIH,SKBS AAS_API_URL = https://<AAS IP address or hostname>:8444/aas/v1 AAS_ADMIN_USERNAME = <AAS username> AAS_ADMIN_PASSWORD = <AAS password> IH_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Integration Hub> KBS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Key Broker Service> SCS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the SGX Caching Service> SQVS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the SGX Quote Verification Service> SHVS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the SGX Host Verification Service> IHUB_SERVICE_USERNAME = <Username for the Hub service user> IHUB_SERVICE_PASSWORD = <Password for the Hub service user> SCS_SERVICE_USERNAME = <Username for the SCS service user> SCS_SERVICE_PASSWORD = <Password for the SCS service user> SHVS_SERVICE_USERNAME = <Username for the SHVS service user> SHVS_SERVICE_PASSWORD = <Password for the SHVS service user> KBS_SERVICE_USERNAME = <Username for the KBS service user> KBS_SERVICE_PASSWORD = <Password for the KBS service user> INSTALL_ADMIN_USERNAME = <Username for the Admin user> INSTALL_ADMIN_PASSWORD = <Password for the Admin user> CCC_ADMIN_USERNAME = <Username for the Custom Claims Creator Admin user> CCC_ADMIN_PASSWORD = <Password for the Custom Claims Creator Admin user> Note The ISECL_INSTALL_COMPONENTS variable is a comma-separated list of all the components that will be used in your environment. Not all services are required for every use case. Include only services which are required specific to the use case. Note The SAN list variables each support wildcards( \"*\" and \"?\"). Using wildcards, domain names and entire IP ranges can be included in the SAN list, which will allow any host matching those ranges to install the relevant service. The SAN list specified here must exactly match the SAN list for the applicable service in that service\u2019s env installation file. Execute the populate-users script: ./populate-users The script will automatically generate the following users: Authentication and Authorization Service (AAS) SGX Caching Service (SCS) SGX Host Verificatin Service (SHVS) Integration HUB (IHUB) Key Broker Service (KBS) with backend key management Installation Admin User CCC Admin User These user accounts will be used during installation of each components of SGX Attestation or SKC. In general, whenever credentials are required by an installation answer file, the variable name should match the name of the corresponding variable used in the populate-users.env file. The populate-users script will also output an installation token. This token has all privileges needed for installation of the services, and uses the credentials provided with the INSTALL_ADMIN_USERNAME and INSTALL_ADMIN_PASSWORD . The remaining Intel \u00ae SecL-DC services require this token (set as the BEARER_TOKEN variable in the installation env files) to grant the appropriate privileges for installation. By default this token will be valid for two hours; the populate-users script can be rerun with the same populate-users.env file to regenerate the token if more time is required, or the INSTALL_ADMIN_USERNAME and INSTALL_ADMIN_PASSWORD can be used to generate an authentication token. Installing the Caching Service Required For The SCS is REQUIRED for the following use cases. Prerequisites (CSP & Enterprise) The following must be completed before installing the SGX Caching Service The Certificate Management Service must be installed and available The Authentication and Authorization Service must be installed and available User needs to subscribe to Intel\\'s Provisioning Certificate Service to obtain an API Key The SGX Caching Service database must be available Package Dependencies The Intel\u00ae SecL-DC SGX Caching Service (SCS) requires a Postgresql 11 database. A set of scripts (install_pgdb.sh and create_db.sh) is provided with the SCS that will automatically add the Postgresql repositories and install/configure a sample database. If this script will not be used, a Postgresql 11 database must be installed by the user before executing the SCS installation. Supported Operating System The Intel\u00ae Security Libraries SGX Caching Service supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver). Recommended Hardware 4 vCPUs RAM: 8 GB 100 GB One network interface with network access to all managed servers. Internet connection is needed for connecting to Intel PCS Server. Installation Before SCS is installed, Database needs to be created. Use the following commands to install postgres and create SCS DB copy install_pgdb.sh and create_db.sh to /root/ directory ./install_pgdb.sh (if services reside on separate VM) ./create_db.sh <db name> <db_user> <db_password> Copy the SCS installation binary to the /root/ directory. Create the scs.env installation answer file in /root/ directory as below: SCS_DB_USERNAME = <database username> SCS_DB_PASSWORD = <database password> SCS_DB_HOSTNAME = <IP or hostname of database server> SCS_DB_PORT = <Database port ; 5432 by default> SCS_DB_NAME = <name of the SCS database ; pgscsdb by default> SCS_DB_SSLCERTSRC = <path to database TLS certificate ; the default location is typically /usr/local/pgsql/data/server.crt> INTEL_PROVISIONING_SERVER = <hostname of INTEL PCS Server> INTEL_PROVISIONING_SERVER_API_KEY = <subscription key> SCS_REFRESH_HOURS = < time in hours to refresh SGX collaterals ; 1 hour by default> SCS_ADMIN_USERNAME = <username for SCS service account> SCS_ADMIN_PASSWORD = <password for SCS service account> CMS_BASE_URL = https://<IP or hostname to CMS>:8445/cms/v1/ CMS_TLS_CERT_SHA384 = <sha384 of CMS TLS certificate> AAS_API_URL = https://<IP or hostname to AAS>:8444/aas/v1 RETRY_COUNT = 3 WAIT_TIME = 1 SAN_LIST = <comma-separated list of IPs and hostnames for the SCS BEARER_TOKEN = <Installation token> Update the BEARER_TOKEN with the TOKEN obtained after running populate-users.sh script. Execute the SCS installer binary: ./scs-v4.0.0.bin Installing the SGX Host Verification Service Required For If SGX Host Verification Service API URL is specified in SGX Agent env file, then SGX Agent will push the platform enablement information and TCB status to SHVS at regular interval. Prerequisites The following must be completed before installing the SGX Host Verification Service: The Certificate Management Service must be installed and available The Authentication and Authorization Service must be installed and available The SGX Host Verification Service database must be available Package Dependencies The Intel\u00ae Security Libraries SGX Host Verification Service requires the following packages and their dependencies: Postgres* client and server 11.6 (server component optional if an external Postgres database is used) Golang packages If they are not already installed, the SGX Host Verification Service installer attempts to install these automatically using the package manager. Automatic installation requires access to package repositories (the RHEL/Ubuntu subscription repositories, the EPEL repository, or a suitable mirror), which may require an Internet connection. If the packages are to be installed from the package repository, be sure to update the repository package lists before installation. Supported Operating Systems The Intel\u00ae Security Libraries SGX Host Verification Service supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver). Recommended Hardware 4 vCPUs RAM: 8 GB 100 GB One network interface with network access to all managed servers Installation Before SHVS is installed, Database needs to be created. Use the following commands to install postgres and create SHVS DB copy install_pgdb.sh and create_db.sh to /root/ directory ./install_pgdb.sh (if services reside on separate VM) ./create_db.sh <db name> <db_user> <db_password> To install the SGX Host Verification Service, follow these steps: Copy the SGX Host Verification Service installation binary to the /root/ directory. Create the shvs.env installation answer file in /root/ directory as below. A sample minimal shvs.env file is provided below. For all configuration options and their descriptions, refer to the Intel\u00ae SecL Configuration section on the SGX Host Verification Service. SHVS_DB_HOSTNAME=<hostname or IP address to database server> SHVS_DB_USERNAME=<Database administrative username> SHVS_DB_PORT=<Database port, default is 5432> SHVS_DB_PASSWORD=<Database password> SHVS_DB_NAME=<Database schema> SHVS_ADMIN_USERNAME=<SGX Host Verification Service username> SHVS_ADMIN_PASSWORD=<SGX HostVerification Service password> CMS_TLS_CERT_SHA384=<Certificate Management Service TLS digest> SHVS_DB_SSLCERTSRC=/usr/local/pgsql/data/server.crt SHVS_SCHEDULER_TIMER=10 #Maximum allowed time before a platform enablement record in SHVS database is considered as stale SHVS_HOST_PLATFORM_EXPIRY_TIME=240 SHVS_AUTO_REFRESH_TIMER=120 BEARER_TOKEN=<Installation token> AAS_API_URL=https://<Authentication and Authorization Service IP or Hostname>:8444/aas/v1 CMS_BASE_URL=https://<Certificate Management Service IP or Hostname>:8445/cms/v1/ SCS_BASE_URL=https://<SGX Caching Service IP or Hostname>:9000/scs/sgx/ SAN_LIST=<Comma-separated list of IP addresses and hostnames for the SHVS> Update the BEARER_TOKEN with the TOKEN obtained after running populate-users.sh script Execute the installer binary. ./shvs-v4.0.0.bin When the installation completes, the SGX Host Verification Service is available. The service can be verified by running shvs status from the SGX Host Verification Service command line. \\# shvs status Installing the SGX Agent Required for The SGX Agent is REQUIRED for all use cases. The SGX Agent pushes SGX platform data to SGX Caching Service (SCS). SGX Agent gets current TCB Status for the platform from SCS. If SGX Host Verification Service (SHVS) URL is configured, the SGX Agent pushes platform enablement information and TCB Status to SHVS. Prerequisites The following must be completed before installing the SGX Agent: Certificate Management Service, Authentication and Authorization Service,SGX Caching Service and SGX Host Verification Service must be installed and available. Make sure system date and time of SGX machine and CSP machine both are in sync. Also, if the system is configured to read the RTC time in the local time zone, then use RTC in UTC by running timedatectl set-local-rtc 0 command on both the machine. Otherwise SGX Agent deployment will fail with certificate expiry error. Package Dependencies The Intel\u00ae Security Libraries SGX Agent Service requires the following packages and their dependencies: Golang packages Supported Operating Systems Red Hat Enterprise Linux 8.2 Ubuntu 18.04.5 LTS(Bionic Beaver). Recommended Hardware Intel\u00ae Xeon\u00ae SP (Ice Lake-SP) Installation Copy sgx_agent.tar, sgx_agent.sha2 and agent_untar.sh from binaries directoy to a directory in SGX compute node ./agent_untar.sh Edit agent.conf with the following CSP system IP address where CMS, AAS, SHVS and SCS services deployed Network Port numbers for CMS, AAS, SCS and SHVS CSP Admin credentials (same which are provided in service configuration file. for ex: csp_skc.conf, orchestrator.conf or skc.conf) Token validity period in days CMS TLS SHA Value (Run \"cms tlscertsha384\" on CSP system) Save and Close Note In case orchestration support is not needed, please comment/delete SHVS_IP in agent.conf available in same folder ./deploy_sgx_agent.sh Installing the SQVS Required for SGX ECDSA Attestation / SGX Quote Verification by KBS Prerequisites The following must be completed before installing the SQVS: Certificate Management Service, Authentication and Authorization Service and SGX Caching Service must be installed and available. Package Dependencies The Intel\u00ae Security Libraries Quote Verification Service requires the following packages and their dependencies: Golang packages Supported Operating Systems Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver). Recommended Hardware 4 vCPUs RAM: 8 GB 100 GB One network interface with network access to all managed servers Installation To install the SQVS Service, follow these steps: Copy the SQVS installation binary to the ~/root directory Copy the Root CA Certificate of the Intel PCS Service (Refer to below Note) from sgx-verification-service/dist/linux/ directory as trusted_rootca.pem to the /tmp directory Note Retrieve appropriate Trusted RootCA certificate files for SGX platform (trusted_rootca.pem for pre-production systems using IceLake Sandbox PCS, trusted_rootca_icx_prod.pem for production systems using IceLake Live PCS and trusted_rootca_clx_prod.pem for CascadeLake production systems using Live PCS Server) from dist/linux directory in SQVS repository Create the sqvs.env installation answer file in /root/ directory as below A sample minimal sqvs.env file is provided below. For all configuration options and their descriptions, refer to the Configuration section on the SGX Quote Verification Service. SGX_TRUSTED_ROOT_CA_PATH=< Path where trusted root ca cert for PCS is stored, by default /tmp/trusted_rootca.pem > SCS_BASE_URL=https://< SCS IP or Hostname >:9000/scs/sgx/certification/v1 CMS_TLS_CERT_SHA384=< Certificate Management Service TLS digest > BEARER_TOKEN=< Installation token > AAS_API_URL=https://< Authentication and Authorization Service IP or Hostname >:8444/aas/v1 CMS_BASE_URL=https://< Certificate Management Service IP or Hostname >:8445/cms/v1/ SAN_LIST=< *Comma-separated list of IP addresses and hostnames for the SQVS* > SQVS_NOSETUP=false SQVS_PORT=12000 SQVS_LOGLEVEL=info SQVS_INCLUDE_TOKEN=true Update the BEARER_TOKEN with the TOKEN obtained after running populate-users.sh script Execute the sqvs installer binary. ./sqvs-v4.0.0.bin When the installation completes, the SGX Quote Verification Service is available. The service can be verified by sqvs status from the sqvs command line. \\# sqvs status Setup K8S Cluster and Deploy Isecl-k8s-extensions Setup master and worker node for k8s. Worker node should be setup on SGX enabled host machine. Master node can be any system. To setup k8 cluster on RHEL 8.2, follow https://phoenixnap.com/kb/how-to-install-kubernetes-on-centos To setup k8 cluster on Ubuntu 18.04, follow the \"Install, Enable and start the Docker daemon\" section in Ubuntu Quick Start Guide - https://github.com/intel-secl/docs/blob/v4.0/develop/quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu.md Once done, go to https://vitux.com/install-and-deploy-kubernetes-on-ubuntu/ and follow from step 3 onwards. Once the master/worker setup is done, follow below steps on Master Node: Untar packages and push OCI images to registry Copy tar output isecl-k8s-extensions-*.tar.gz from build system's binaries folder to /opt/ directory on the Master Node and extract the contents. cd /opt/ tar -xvzf isecl-k8s-extensions-*.tar.gz cd isecl-k8s-extensions/ Configure private registry Push images to private registry using skopeo command, (this can be done from build vm also) skopeo copy oci-archive:isecl-k8s-controller-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-controller:v4.0.0 skopeo copy oci-archive:isecl-k8s-scheduler-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-scheduler:v4.0.0 Add the image names in isecl-controller.yml and isecl-scheduler.yml in /opt/isecl-k8s-extensions/yamls with full image name including registry IP/hostname (e.g : /isecl-k8s-scheduler:v4.0.0). It will automatically pull the images from registry. Deploy isecl-controller Create hostattributes.crd.isecl.intel.com crd kubectl apply -f yamls/crd-1.17.yaml Check whether the crd is created kubectl get crds Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterrolebinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole=system:node --user=system:serviceaccount:isecl:isecl Fetch token required for ihub installation and follow below steps to update ihub.env, kubectl get secrets -n isecl kubectl describe secret default-token-<name> -n isecl For IHUB installation, make sure to update below configuration in /root/binaries/env/ihub.env before installing ihub on CSP VM: Copy /etc/kubernetes/pki/apiserver.crt from master node to /root on CSP VM. Update KUBERNETES_CERT_FILE. Get k8s token in master, using above commands and update KUBERNETES_TOKEN Update the value of CRD name KUBERNETES_CRD=custom-isecl-sgx Deploy isecl-scheduler The isecl-scheduler default configuration is provided for common cluster support in /opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml. Variables HVS_IHUB_PUBLIC_KEY_PATH and SGX_IHUB_PUBLIC_KEY_PATH are by default set to default paths. Please use and set only required variables based on the use case. For example, if only sgx based attestation is required then remove/comment HVS_IHUB_PUBLIC_KEY_PATH variables. Install cfssl and cfssljson on Kubernetes Control Plane #Download cfssl to /usr/local/bin/ wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl #Download cfssljson to /usr/local/bin wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create tls key pair for isecl-scheduler service, which is signed by k8s apiserver.crt cd /opt/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \"<K8_MASTER_IP>\",\"<K8_MASTER_HOST>\" -c /etc/kubernetes/pki/ca.crt -k /etc/kubernetes/pki/ca.key After iHub deployment, copy /etc/ihub/ihub_public_key.pem from ihub to /opt/isecl-k8s-extensions/ directory on k8 master vm. Also, copy tls key pair generated in previous step to secrets directory. mkdir secrets cp /opt/isecl-k8s-extensions/server.key secrets/ cp /opt/isecl-k8s-extensions/server.crt secrets/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem cp /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem secrets/ Note Prefix the attestation type for ihub_public_key.pem before copying to secrets folder. Create kubernetes secrets scheduler-secret for isecl-scheduler kubectl create secret generic scheduler-certs --namespace isecl --from-file=secrets Deploy isecl-scheduler kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl Configure kube-scheduler to establish communication with isecl-scheduler Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec: containers: - command: - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml. Restart Kubelet which restart all the k8s services including kube base schedular systemctl restart kubelet Check if CRD Data is populated kubectl get -o json hostattributes.crd.isecl.intel.com Installing the Integration Hub Note: The Integration Hub is only required to integrate Intel\u00ae SecL with third-party scheduler services, such as Kubernetes. The Integration Hub is not required for usage models that do not require Intel\u00ae SecL security attributes to be pushed to an integration endpoint. Required For The Integration Hub is REQUIRED the for enabling support for orchestration support. Prerequisites The Intel\u00ae Security Libraries Integration Hub can be run on a VM or on a bare-metal server. The Integration Hub may be installed on the same server (physical or VM) as the SGX Host Verification Service. SGX Caching Service must be installed and available. The SGX Host Verification Service must be installed and available The Authentication and Authorization Service must be installed and available The Certificate Management Service must be installed and available Package Dependencies The Intel\u00ae SecL Integration Hub requires a number of packages and their dependencies: Golang packages If these are not already installed, the Integration Hub installer attempts to install these packages automatically using the package manager. Automatic installation requires access to package repositories (the RHEL/Ubuntu subscription repositories, the EPEL repository, or a suitable mirror), which may require an Internet connection. If the packages are to be installed from the package repository, be sure to update your repository package lists before installation. Supported Operating Systems Red Hat Enterprise Linux 8.2 Ubuntu 18.04.5 LTS(Bionic Beaver). Recommended Hardware 1 vCPUs RAM: 2 GB 1 GB free space to install the Integration Hub (database and log space requirements are dependent on the number of managed servers). One network interface with network access to the SGX Host Verification Service. One network interface with network access to any integration endpoints (for example, OpenStack Nova). Installing the Integration Hub To install the SGX Integration Hub, follow these steps: Copy the Integration Hub installation binary to the /root/ directory. Create the ihub.env installation answer file in /root/ directory as below IHUB_SERVICE_USERNAME=< IHUB service user username > IHUB_SERVICE_PASSWORD=< IHUB service user password > SHVS_BASE_URL=< https://< SHVS IP or Hostname >:13000/sgx-hvs/v2 CMS_TLS_CERT_SHA384=< CMS TLS digest > BEARER_TOKEN=<Installation token> AAS_API_URL=https://< AAS IP or Hostname >:8444/aas/v1 CMS_BASE_URL=https://< CMS IP or Hostname >:8445/cms/v1 POLL_INTERVAL_MINUTES=2 TLS_SAN_LIST=< comma separated list of IPs and hostnames for the IHUB > TENANT=< tenant-type e.g. KUBERNETES or OPENSTACK > # Kubernetes Integration Credentials - required for Kubernetes integration only KUBERNETES_URL=< https://< Kubernetes IP >:6443/> KUBERNETES_CRD=custom-isecl-sgx KUBERNETES_TOKEN=< K8S token > KUBERNETES_CERT_FILE =< Path of Kubernetes master node certificate > # OpenStack Integration Credentials - required for OpenStack integration only OPENSTACK_AUTH_URL=<OpenStack Keystone URL; typically http://openstack-ip:5000/> OPENSTACK_PLACEMENT_URL=<OpenStack Nova Placement API URL; typically http://openstack-ip:8778/> OPENSTACK_USERNAME=< OpenStack username > OPENSTACK_PASSWORD=< OpenStack password > Create Integrated Hub Service user account and Roles. A sample script is provided in the appendix section for reference. Update the BEARER_TOKEN with the TOKEN obtained after running populate-users.sh script Execute the installer binary. ./ihub-v4.0.0.bin In case installation fails, its recommended to run the following command to clear failed service instance systemctl reset-failed Copy IHUB public key to the master node and restart kubelet. scp -r /etc/ihub/ihub_public_key.pem <master-node IP>:/opt/isecl-k8s-extensions/ systemctl restart kubelet Run this command to validate if the data has been pushed to CRD: kubectl get -o json hostattributes.crd.isecl.intel.com Run this command to validate that the labels have been populated: kubectl get nodes --show-labels. Sample labels: EPC-Memory=2.0GB,FLC-Enabled=true,SGX-Enabled=true,SGX-Supported=true,SgxTrustExpiry=2020-11-09T08.07.43Z,TCBUpToDate=true Create sample yml file for nginx workload and add SGX labels to it such as: apiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: SGX-Enabled operator: In values: - \"true\" - key: EPC-Memory operator: In values: - \"2.0GB\" containers: - name: nginx image: nginx ports: - containerPort: 80 Validate if pod can be launched on the node. Run following commands: kubectl apply -f pod.yml kubectl get pods kubectl describe pods nginx Pod should be in running state and launched on the host as per values in pod.yml. Validate running below commands on sgx host: docker ps Integration with OpenStack (Supported only on RHEL 8.2, not supported on Ubuntu 18.04) OpenStack can now use \u201cTraits\u201d to provide qualitative data about Nova Compute hosts to establish Trait requirements. The Integration Hub continually push SGX data to the OpenStack Traits resources. This means OpenStack scheduler natively supports workload scheduling incorporating SGX Host information, including SGX enabled/disabled, SGX supported/not supported, FLC enabled/not enabled, EPC memory size, TCB status upto date/not. The OpenStack Placement Service will automatically attempt to place images with Trait requirements on compute nodes that have those Traits. Note This control only applies to instances launched using the OpenStack scheduler, and the Traits functions will not affect manually-launched instances where a specific Compute Node is defined (since this does not use the scheduler at all). Intel SecL-DC uses existing OpenStack interfaces and does not modify OpenStack code. The datacenter owner or OpenStack administrator is responsible for the security of the OpenStack workload scheduling process in general, and Intel recommends following published OpenStack security best practices. Setting Image Traits Image Traits define the policy for which Traits are required for that instance to be launched on a Nova Compute node.By setting these Traits to \u201crequired\u201d the OpenStack scheduler will require the same Traits to be present on a Nova Compute node in order to launch instances. To set the Image Traits for Intel SecL-DC,a specific naming convention is used. This naming convention will match the Traits that the Integration Hub will automatically push to OpenStack. Two types of Traits are currently supported \u2013 one Trait is used to require that the Compute Node must be SGX supported and the other Trait is used to require specific SGXkey/value pairs. Required Image trait for SGX Enabled Host: CUSTOM_ISECL_SGX_ENABLED_TRUE=required These Traits can be set using CLI commands for OpenStack Glance: openstack image set --property trait:CUSTOM_ISECL_SGX_ENABLED_TRUE=required <image name> To veiw the Traits that has been set: openstack image show List the set of resources mapped to the Openstack openstack resource provider list To view the traits enabled for the SGX Host: openstack resource provider trait list <uuid of the host which the openstack resoruce provider lists> Create the instances openstack server create --flavor tiny --image <image name> --net vmnet <vm instance name> Instances should be created and the status should be \"Active\". Instance should be launched successfully. openstack server list To remove a Trait that is not required for an Image: openstack image unset --property trait:CUSTOM_ISECL_SGX_ENABLED_TRUE openstack image unset --property trait:CUSTOM_ISECL_SGX_ENABLED_FALSE Scheduling Instances Once Trait requirements are set for Images and the Integration Hub is configured to push attributes to OpenStack, instances can be launched in OpenStack as normal. As long as the OpenStack Nova scheduler is used to schedule the workloads, only compliant Compute Nodes will be scheduled to run instances of controlled Images. Note This control only applies to instances launched using the OpenStack scheduler and the Traits functions will not affect manually-launched instances where a specific Compute Node is defined (since this does not use the scheduler at all). Intel SecL-DC uses existing OpenStack interfaces and does not modify OpenStack code. The datacenter owner or OpenStack administrator is responsible for the security of the OpenStack workload scheduling process in general and Intel recommends following published OpenStack security best practices. Installing the Key Broker Service Required for The KBS is REQUIRED for - Storing Application Keys and Verifying the SGX Quote Prerequisites The following must be completed before installing the Key Broker: The Authentication and Authorization Service must be installed and available The Certificate Management Service must be installed and available If a 3 rd -party Key Management server will be used following must be completed before installing the Key Broker: A KMIP 3 rd -party Key management Server must be available. The Key Broker will require the KMIP server\u2019s client certificate, client key and root ca certificate. This key and certificate will be available in KMIP server. Note The Key Broker has been validated using the pykmip 0.9.1 KMIP server as a 3 rd -party Key Management Server. While any general KMIP 2.0-compliant Key Management Server should work, implementation differences among KMIP providers may prevent functionality with specific providers. Package Dependencies N/A Supported Operating Systems supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver). Recommended Hardware NA Installation Copy the Key Broker installation binary to the /root/ directory. Create the installation answer file kbs.env /root/ directory as below: KBS_SERVICE_USERNAME=< KBS service user username > KBS_SERVICE_PASSWORD=< KBS service user password > SERVER_PORT=9443 AAS_API_URL=https://<AAS IP or hostname>:8444/aas/v1 CMS_BASE_URL=https://<CMS IP or hostname>:8445/cms/v1/ SQVS_URL=https://<SQVS IP or hostname>:12000/svs/v1 ### KEY_MANAGER is set to KMIP KEY_MANAGER=KMIP ENDPOINT_URL=https://<KBS Hostname>:9443/v1 TLS_COMMON_NAME=\"KBS TLS Certificate\" SKC_CHALLENGE_TYPE=\"SGX\" CMS_TLS_CERT_SHA384=<SHA384 hash of CMS TLS certificate> TLS_SAN_LIST=<KBS Hostname/IP> BEARER_TOKEN=<Installation token from AAS> ## Session Expiry Time Between KBS and SKC Library in Minutes SESSION_EXPIRY_TIME=60 KMIP_SERVER_IP=<IP address of KMIP server> KMIP_SERVER_PORT=<Port number of KMIP server> ## KMIP_VERSION variable can be used to mention KMIP protocol version. ## This is an OPTIONAL field, default value is set to '2.0'. KBS supports KMIP version '1.4' and '2.0'. KMIP_VERSION=<kmip version> ## KMIP_HOSTNAME can be used to configure TLS config with ServerName. ## KMIP server certificate should contain SAN(IP/DNS) or valid COMMON NAME and this value can be provided in KMIP_HOSTNAME. Only FQDN names are allowed. ## This is an OPTIONAL field; if KMIP_HOSTNAME is not provided then KMIP_SERVER_IP will be considered as ServerName in TLS configuration. KMIP_HOSTNAME=<Hostname of KMIP server> ## KMIP supports authentication mechanism to authenticate requestor. This is an OPTIONAL field. ## This feature can be added to KBS by updating kbs.env with KMIP_USERNAME and KMIP_PASSWORD. ## These are OPTIONAL variables. PyKMIP doesn't supports this feature. This feature is validated in Thales cipher trust manager. KMIP_USERNAME=<Username of KMIP server> KMIP_PASSWORD=<Password of KMIP server> ### Retrieve the following certificates and keys from the KMIP server KMIP_CLIENT_KEY_PATH=<path>/client_key.pem KMIP_ROOT_CERT_PATH=<path>/root_certificate.pem KMIP_CLIENT_CERT_PATH=<path>/client_certificate.pem Update the BEARER_TOKEN with the TOKEN obtained after running populate-users.sh script Execute the KBS installer. ./kbs-4.0.0.bin Note When a 3 rd -party Key Management Server is used, KBS supports only association of RSA key. User needs to create RSA key in backend kmip server and note the Private key Id. User also needs to create the tls certificate for the private key (it will be used during key transfer). rsa-create.py available in kbs_scripts can be used to create the private key and generate the certificate by providing the kmip server ip and certificates path in the script. Installing the SKC Library Required For The SKC_Library enables secure transfer of application keys from KBS after performing SGX attestation. It stores the keys in the SGX enclave and performs crypto operations ensuring the keys are never exposed in use, at rest and in transit outside of enclave. Package Dependencies The Intel\u00ae Security Libraries SKC Library requires the following packages and their dependencies: Openssl Curl Supported Operation System The Intel\u00ae Security Libraries SKC Library supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver). Recommended Hardware Icelake Server with SGX enabled in BIOS RAM: 8 GB 100 GB One network interface with network access to the Key Broker Installation Copy skc_library.tar, skc_library.sha2 and skclib_untar.sh from binaries directoy to a directory in SGX compute node ./skclib_untar.sh Update create_roles.conf with the following - IP address of AAS deployed on Enterprise system - Admin account credentials of AAS deployed on Enterprise system. These credentials should match with the AAS admin credentials provided in authservice.env on enterprise side. - Permission string to be embedded into skc_libraty client TLS Certificate - For Each SKC Library installation on a SGX compute node, please change SKC_USER and SKC_USER_PASSWORD Save and Close ./skc_library_create_roles.sh Copy the token printed on console. Update skc_library.conf with the following - IP address for CMS and KBS services deployed on Enterprise system - CSP_CMS_IP should point to the IP of CMS service deployed on CSP system - CSP_SCS_IP should point to the IP of SCS service deployed on CSP system - Hostname of the Enterprise system where KBS is deployed - Network Port numbers for CMS and SCS services deployed on CSP system - Network Port numbers for CMS and KBS services deployed on Enterprise system - For Each SKC Library installation on a SGX compute node, please change SKC_USER (should be same as SKC_USER provided in create_roles.conf) - SKC_TOKEN with the token copied from previous step Save and Close ./deploy_skc_library.sh Deploying SKC Library as a Container (Supported only on RHEL 8.2, not supported on Ubuntu 18.04) Use the following steps to configure SKC library running in a container and to validate key transfer in container on bare metal and inside a VM on SGX enabled hosts. Note: All the configuration files required for SKC Library container are modified in the resources directory only 1. Docker should be installed, enabled and services should be active 2. In the build System, SKC Library tar file \"<skc-lib*>.tar\" required to load is located in the \"/root/workspace/skc_library\" directory. 3. Copy \"resources\" folder from \"workspace/skc_library/container/resources\" to the \"/root/\" directory of SGX host. Inside the resources folder all the key transfer flow related files will be available. 4. Update sgx_default_qcnl.conf file inside resources folder with SCS IP and SCS port and update hosts file present in same folder with KBS IP and hostname. 5. Generate the RSA key in the kbs host and copy it to SGX host. 6. Refer to openssl and nginx sub sections of QSG in the \"Configuration for NGINX testing\" to configure nginx.conf and openssl.conf present resource in the directory. 7. Update keyID in the keys.txt and nginx.conf. 8. Under [core] section of pkcs11-apimodule.ini in the \"/root/resources/\" directory add preload_keys=/root/keys.txt. 9. Update skc_library.conf with IP addresses where SKC services are deployed. 10. On the SGX Compute node, load the skc library docker image provided in the tar file. docker load < <SKC_Library>.tar 11. Provide valid paramenets in the docker run command and execute the docker run command. Update the genertaed RSA Key ID and <keys>.crt in the resources directory. docker run -p 8080:2443 -p 80:8080 --mount type=bind,source=/root/<KBS_cert>.crt,target=/root/<KBS_cert>.crt --mount type=bind,source=/root/resources/sgx_default_qcnl.conf,target=/etc/sgx_default_qcnl.conf --mount type=bind,source=/root/resources/nginx.conf,target=/etc/nginx/nginx.conf --mount type=bind,source=/root/resources/keys.txt,target=/root/keys.txt,readonly --mount type=bind,source=/root/resources/pkcs11-apimodule.ini,target=/opt/skc/etc/pkcs11-apimodule.ini,readonly --mount type=bind,source=/root/resources/openssl.cnf,target=/etc/pki/tls/openssl.cnf --mount type=bind,source=/root/resources/skc_library.conf,target=/skc_library.conf --add-host=<SHC_HOSTNAME>:<SGX_HOST_IP> --add-host=<KBS_Hostname>:<KBS host IP> --mount type=bind,source=/dev/sgx,target=/dev/sgx --cap-add=SYS_MODULE --privileged=true <SKC_LIBRARY_IMAGE_NAME> Note: In the above docker run command, source refers to the actual path of the files located on the host and the target always refers to the files which would be mounted inside the container 12. Restore index.html for the transferred key inside the container Get the container id using \"docker ps\" command docker exec -it <container_id> /bin/sh Download index.html wget https://localhost:2443 --no-check-certificate","title":"Intel\u00ae Security Libraries Installation"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#intel-security-libraries-installation","text":"","title":"Intel\u00ae Security Libraries Installation"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#building-from-source","text":"Intel\u00ae Security Libraries is distributed as open source code and must be compiled into installation binaries before installation. Instructions and sample scripts for building the Intel\u00ae SecL-DC components can be found here . (Section 1 to 6) After the components have been built, the installation binaries and database scripts can be found in the binaries directory created by the build scripts. Generated component binaries/installers are: CMS: cms-v4.0.0.bin AAS: authservice-v4.0.0.bin SCS: scs-v4.0.0.bin SHVS: shvs-v4.0.0.bin IHUB: ihub-v4.0.0.bin SQVS: sqvs-v4.0.0.bin KBS: kbs-v4.0.0.bin K8S-Extensions: isecl-k8s-extensions-v4.0.0.tar.gz SGX-Agent: agent_untar.sh, sgx_agent.sha2 and sgx_agent.tar SKC-Library: skclib_untar.sh, skc_library.sha2 and skc_library.tar DB scripts: Postgres installation script: install_pgdb.sh AAS, SCS and SHVS DB creation script: create_db.sh","title":"Building from Source"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#building-from-source-oci-images-k8s-manifests","text":"Intel\u00ae Security Libraries is distributed as open source code and must be compiled into OCI images before installation. Instructions and sample scripts for building the Intel\u00ae SecL-DC components as containerized images for Kubernetes deployments can be found in Quick Start guide . After the components have been built, the OCI images and pre-req scripts can be found in the K8s directory created by the build scripts. Generated components OCI images under <working directory>/k8s/container-images : Authentication Authorization Service: aas-v4.0.0.tar Certificate Management Service: cms-v4.0.0.tar Integration Hub: ihub-v4.0.0.tar Key Broker Service: kbs-v4.0.0.tar K8s Extensions Custom Controller: isecl-k8s-controller-v4.0.0.tar K8s Extensions Extended Scheduler: isecl-k8s-scheduler-v4.0.0.tar SGX Caching Service: scs-v4.0.0.tar SGX Quote Verification Service: sqvs-v4.0.0.tar SGX Host Verification Service: shvs-v4.0.0.tar SGX Agent: sgx-agent-v4.0.0.tar SKC Library: skc-lib-4.0.0.tar Generated Components K8s Manifests directories under <working directory>/k8s/manifests : Authentication Authorization Service Database: aas-db SGX Caching Service Database: scs-db SGX Host Verification Service Database: shvs-db Certificate Management Service: cms Authentication Authorization Service: aas Integration Hub: ihub Key Broker Service: kbs SGX Caching Service: scs SGX Host Verification Service: shvs SGX Quote Verification Service: sqvs K8s Extensions Custom Controller: k8s-extensions-controller K8s Extensions Extended Scheduler: k8s-extensions-scheduler SGX Agent: sgx_agent SKC Library: skc_library Bootstrap scripts and answer file under <working directory>/k8s/manifests : Pre-req: pre-requisites.sh Bootstrap DB: skc-bootstrap-db-services.sh Answer file: isecl-skc-k8s.env Bootstrap: skc-bootstrap.sh","title":"Building from Source - OCI images &amp; K8s Manifests"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#hardware-considerations","text":"","title":"Hardware Considerations"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#supported-hardware","text":"Intel\u00ae Xeon\u00ae SP products those support SGX","title":"Supported Hardware"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#bios-requirements","text":"Intel\u00ae SGX-TEM BIOS requirements are outlined in the latest Intel\u00ae SGX Platforms BIOS Writer's Guide Intel\u00ae SGX should be enabled in BIOS menu (Intel\u00ae SGX is Disabled by default on Ice Lake). Intel\u00ae SGX BIOS requirements include exposing Flexible Launch Control menu.","title":"BIOS Requirements"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#os-requirements-intel-sgx-does-not-supported-on-32-bit-os","text":"Linux: RHEL 8.2 Linux: Ubuntu 18.04","title":"OS Requirements (Intel\u00ae SGX does not supported on 32-bit OS):"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#requirements-for-containerized-deployment-with-k8s","text":"","title":"Requirements for Containerized Deployment with K8s"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#operating-system","text":"RHEL 8.2 Ubuntu 18.04","title":"Operating System:"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#kubernetes","text":"Single-node: microk8s (1.17.17) Multi-node: kubeadm (1.17.17)","title":"Kubernetes"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#container-runtime","text":"Docker 19.03.13 on RHEL 8.2 Docker 19.03.5 on Ubuntu 18.04","title":"Container Runtime"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#storage","text":"hostPath for Single Node microk8s for all services and agents NFS for Multi Node kubeadm for all services, hostPath for sgx_agent and skc_library","title":"Storage:"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#recommended-service-layout","text":"The Intel\u00ae SecL-DC services can be installed in a variety of layouts, partially depending on the use cases desired and the OS of the server(s) to be protected. In general, the Intel\u00ae SecL-DC applications can be divided into management services that are deployed on the network on the management plane, and host or node components that must be installed on each protected server. Management services can typically be deployed anywhere with network access to all the protected servers. This could be a set of individual VMs per service; containers; or all installed on a single physical or virtual machine. Node components must be installed on each protected physical server. Typically, this is needed for Linux deployments. For a POC environment, all the management services can be installed on a single machine or VM. This Includes: Certificate Management Service (CMS) Authentication and Authorization Service (AAS) SGX Caching Service (SCS) SGX Host Verification Service (SHVS) Integration HUB (IHUB) Key Broker Service (KBS) with backend key management SGX Quote Verification Service (SQVS) SGX Agent & SKC Library needs to be installed on SGX Enabled Machine. Isecl-K8s-extensions must be installed on separate VM.","title":"Recommended Service Layout"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#recommended-service-layout-architecture-containerized-deployment-with-k8s","text":"The containerized deployment makes use of Kubernetes orchestrator for single node and multi node deployments. The supported deployment models are as below: Single Node: Multi Node: Services Deployments & Agent DaemonSets: Every service including databases will be deployed as separate K8s deployment with 1 replica, i.e(1 pod per deployment). Each deployment will be further exposed through k8s service and also will be having corresponding Persistent Volume Claims(PV) for configuration and log directories and mounted on persistent storage. In case of daemonsets/agents, the configuration and log directories will be mounted on respective Baremetal worker nodes. For stateful services which requires database like shvs, aas, scs, A separate database deployment will be created for each of such services. The data present on the database deployment will also made to persist on a NFS, through K8s persistent storage mechanism Networking within the Cluster: Networking Outside the Cluster: SKC Virtualization: Follow the Installation of Containerized Services and Agent in K8s Cluster for installation instructions once deployment model is chosen.","title":"Recommended Service Layout &amp; Architecture - Containerized Deployment with K8s"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#using-the-provided-database-installation-script","text":"Install a sample Postgresql 11 database using the install_pgdb.sh script provided in binaries directory. This script will automatically install the Postgresql database and client packages required. Create the iseclpgdb.env answer file: ISECL_PGDB_IP_INTERFACES = localhost ISECL_PGDB_PORT = 5432 ISECL_PGDB_SAVE_DB_INSTALL_LOG = true ISECL_PGDB_CERT_DNS = localhost ISECL_PGDB_CERT_IP = 127 .0.0.1 Note that the values above assume that the database will be accessed locally. If the database server will be external to the Intel\u00ae SecL services, change these values to the hostname or FQDN and IP address where the client will access the database server.","title":"Using the provided Database Installation Script"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#provisioning-the-database","text":"Each Intel\u00ae SecL service that uses a database (the Authentication and Authorization Service, the SGX host Verification Service, the SGX caching Service,) requires its own schema and access. The database must be created and initialized. Execute the install_pgdb.sh script to install the database. If a single shared database server will be used for each Intel\u00ae SecL service (for example, if all management plane services will be installed on a single VM), run install_pgdb.sh script only once and create_db.sh script for each component that uses a database. If separate database servers will be used (for example, if the management plane services will reside on separate systems and will use their own local database servers), execute the install_pgdb.sh script on each server hosting a database and create_db.sh script for each component that uses a DB. Command to install postgres DB: ./install_pgdb.sh Command to create DB for AAS/SCS/SHVS: ./create_db.sh <DB Name> <DB Username> <DB Password> Note that the db name, db username and db user password should match with the respective component environment files.","title":"Provisioning the Database"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#database-server-tls-certificate","text":"The database client for Intel\u00ae SecL services requires the database TLS certificate to authenticate communication with the database server. If the database server for a service is located on the same server that the service will run on, only the path to this certificate is needed. If the provided Postgres scripts are used, the certificate will be in /usr/local/pgsql/data/server.crt. If the database server will be run separately from the Intel\u00ae SecL service(s), the certificate will need to be copied from the database server to the service machine before installing the Intel\u00ae SecL services. The database client for Intel\u00ae SecL services will validate that the Subject Alternative Names in the database server's TLS certificate contain the hostname(s)/IP address(es) that the clients will use to access the database server. If configuring a database without using the provided scripts, ensure that these attributes are present in the database TLS certificate.","title":"Database Server TLS Certificate"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installation-of-containerized-services-and-agent-in-k8s-cluster","text":"The containerized deployment utilizes K8s orchestrator to deploy SGX components. The deployments are fairly automated once the pre-reqs are in place for K8s cluster deployments. Note The K8s manifests are declarative in nature and the same can be modified as required for SGX services deployments for single node and multi node deployments. Modifications would require specific steps to ensure services and agents get updated as per the required configuration. More details for the same present in Setup Task Flows for K8s Deployments, Configuration Update Flows for K8s Deployments and Intel Security Libraries Configuration Settings .","title":"Installation of Containerized Services and Agent in K8s Cluster"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#pre-requisites","text":"Ensure based on the deployment model , microk8s or kubeadm in installed. Supported versions in Requirements for Containerized Deployment with K8s . Docker runtime is configured for each of these deployments. Supported versions in Requirements for Containerized Deployment with K8s . The build would generate a script for platform dependencies under <working directory>/k8s/platform dependencies . Follow the deployment pre-reqs as given in the Quick Start guide based on the chosen deployment model.","title":"Pre-requisites"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#deploy-steps","text":"The deploy steps are detailed in the Quick Start guide based on the deployment model. Follow the instructions for the deployment using the scripts.","title":"Deploy Steps"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#additional-details","text":"Default Service and Agent Mount Paths - Single Node Default Service and Agent Mount Paths - Multi Node Default Service Ports NFS Setup Pre-reqs - Multi Node","title":"Additional Details"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installing-the-certificate-management-service","text":"","title":"Installing the Certificate Management Service"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#required-for","text":"The CMS is REQUIRED for all use cases.","title":"Required For"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#supported-operating-system","text":"The Intel\u00ae Security Libraries Certificate Management Service supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver).","title":"Supported Operating System"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#recommended-hardware","text":"1 vCPUs RAM: 2 GB 10 GB One network interface with network access to all Intel\u00ae SecL-DC services","title":"Recommended Hardware"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installation","text":"To install the Intel\u00ae SecL-DC Certificate Management Service: Copy the Certificate Management Service installation binary to the /root/ directory. Create the cms.env installation answer file for an unattended installation: AAS_TLS_SAN = \\< Comma-Separated list of IPs and hostnames for the AAS \\> AAS_API_URL = https:// \\< Authentication and Authorization Service IP or Hostname \\> :8444/aas/v1 SAN_LIST = < Comma-Separated list of IP addresses and hostnames for the CMS> The SAN list will be used to authenticate the Certificate Signing Request from the AAS to the CMS. Only a CSR originating from a host matching the SAN list will be honored. Later, in the AAS authservice.env installation answer file, this same SAN list will be provided for the AAS installation. These lists must match and must be valid for IPs and/or hostnames used by the AAS system. The SAN list variables also accept the wildcards \"?\" (for single-character wildcards) and \"*\" (for multiple-character wildcards) to allow address ranges or multiple FQDNs. The AAS_API_URL represents the URL for the AAS that will exist after the AAS is installed. For all configuration options and their descriptions, refer to the Intel\u00ae SecL Configuration section on the Certificate Management Service. Execute the installer binary. ./cms-v4.0.0.bin When the installation completes, the Certificate Management Service is available. The services can be verified by running cms status from the command line. cms status After installation is complete, the CMS will output a bearer token to the console. This token will be used with the AAS during installation to authenticate certificate requests to the CMS. If this token expires or otherwise needs to be recreated, use the following command: cms setup cms-auth-token \\--force In addition, the SHA384 digest of the CMS TLS certificate will be needed for installation of the remaining Intel\u00ae SecL services. The digest can be obtained using the following command: cms tlscertsha384","title":"Installation"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installing-the-authentication-and-authorization-service","text":"","title":"Installing the Authentication and Authorization Service"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#required-for_1","text":"The AAS is REQUIRED for all use cases.","title":"Required For"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#prerequisites","text":"The following must be completed before installing the Authentication and Authorization Service: The Certificate Management Service must be installed and available The Authentication and Authorization Service database must be available","title":"Prerequisites"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#package-dependencies","text":"The Intel\u00ae SecL-DC Authentication and Authorization Service (AAS) requires a Postgresql 11 database. script (install_pgdb.sh) is provided with the AAS that will install Postgresql repositories.","title":"Package Dependencies"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#supported-operating-systems","text":"The Intel\u00ae Security Libraries Authentication and Authorization Service supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver).","title":"Supported Operating Systems"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#recommended-hardware_1","text":"1 vCPUs RAM: 2 GB 10 GB One network interface with network access to all Intel\u00ae SecL-DC services","title":"Recommended Hardware"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installation_1","text":"Before AAS is installed, Database needs to be created. Use the following commands to install postgres and create AAS DB copy install_pgdb.sh and create_db.sh to /root/ directory ./install_pgdb.sh ./create_db.sh <db name> <db_user> <db_password> To install the AAS, a bearer token from the CMS is required. This bearer token is output at the end of the CMS installation. However, if a new token is needed, simply use the following command from the CMS command line: cms setup cms-auth-token --force Create the authservice.env installation answer file in /root/ directory as below: CMS_BASE_URL=https://< CMS IP or hostname>:8445/cms/v1/ CMS_TLS_CERT_SHA384=<CMS TLS certificate sha384> AAS_DB_SSLMODE=verify-full AAS_DB_HOSTNAME=<IP or hostname of database server> AAS_DB_PORT=<database port number; default is 5432> AAS_DB_NAME=<database name> AAS_DB_USERNAME=<database username> AAS_DB_PASSWORD=<database password> AAS_DB_SSLCERTSRC=<path to database TLS certificate; the default location is typically /usr/local/pgsql/data/server.crt> AAS_ADMIN_USERNAME=<username for AAS administrative user> AAS_ADMIN_PASSWORD=<password for AAS administrative user> AAS_JWT_TOKEN_DURATION_MINS=2880 SAN_LIST=<comma-separated list of IPs and hostnames for the AAS; this should match the value for the AAS_TLS_SAN in the cms.env file from the CMS installation> BEARER_TOKEN=<bearer token from CMS installation> Execute the AAS installer: ./authservice-v4.0.0.bin Note The AAS_ADMIN credentials specified in this answer file will have administrator rights for the AAS and can be used to create other users, create new roles, and assign roles to users.","title":"Installation"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#creating-users","text":"Before deployment is initiated, user account and roles must be generated for each component. Most of these accounts will be service users, used by the various Intel\u00ae SecL SKC services to work together. Creating these required users and roles is facilitated by the populate-user script.","title":"Creating Users"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#creating-users-and-roles","text":"During installation of each services, number of user accounts and roles specific to services must be generated. Most of these accounts will be service users, which is used by the various services to function together. Another set of users will be used for installation permissions, and administrative user will be created to provide the initial authentication interface for the actual user based on the organizational requirements. Creating these required users and roles is facilitated by a script that will accept credentials and configuration settings from an answer file and automate the process. Create the populate-users.env file using the following values: # SKC Components include AAS,SCS,SHVS,SQVS,SIH and SKBS. ISECL_INSTALL_COMPONENTS = AAS,SCS,SHVS,SQVS,SIH,SKBS AAS_API_URL = https://<AAS IP address or hostname>:8444/aas/v1 AAS_ADMIN_USERNAME = <AAS username> AAS_ADMIN_PASSWORD = <AAS password> IH_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Integration Hub> KBS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the Key Broker Service> SCS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the SGX Caching Service> SQVS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the SGX Quote Verification Service> SHVS_CERT_SAN_LIST = <comma-separated list of IPs and hostnames for the SGX Host Verification Service> IHUB_SERVICE_USERNAME = <Username for the Hub service user> IHUB_SERVICE_PASSWORD = <Password for the Hub service user> SCS_SERVICE_USERNAME = <Username for the SCS service user> SCS_SERVICE_PASSWORD = <Password for the SCS service user> SHVS_SERVICE_USERNAME = <Username for the SHVS service user> SHVS_SERVICE_PASSWORD = <Password for the SHVS service user> KBS_SERVICE_USERNAME = <Username for the KBS service user> KBS_SERVICE_PASSWORD = <Password for the KBS service user> INSTALL_ADMIN_USERNAME = <Username for the Admin user> INSTALL_ADMIN_PASSWORD = <Password for the Admin user> CCC_ADMIN_USERNAME = <Username for the Custom Claims Creator Admin user> CCC_ADMIN_PASSWORD = <Password for the Custom Claims Creator Admin user> Note The ISECL_INSTALL_COMPONENTS variable is a comma-separated list of all the components that will be used in your environment. Not all services are required for every use case. Include only services which are required specific to the use case. Note The SAN list variables each support wildcards( \"*\" and \"?\"). Using wildcards, domain names and entire IP ranges can be included in the SAN list, which will allow any host matching those ranges to install the relevant service. The SAN list specified here must exactly match the SAN list for the applicable service in that service\u2019s env installation file. Execute the populate-users script: ./populate-users The script will automatically generate the following users: Authentication and Authorization Service (AAS) SGX Caching Service (SCS) SGX Host Verificatin Service (SHVS) Integration HUB (IHUB) Key Broker Service (KBS) with backend key management Installation Admin User CCC Admin User These user accounts will be used during installation of each components of SGX Attestation or SKC. In general, whenever credentials are required by an installation answer file, the variable name should match the name of the corresponding variable used in the populate-users.env file. The populate-users script will also output an installation token. This token has all privileges needed for installation of the services, and uses the credentials provided with the INSTALL_ADMIN_USERNAME and INSTALL_ADMIN_PASSWORD . The remaining Intel \u00ae SecL-DC services require this token (set as the BEARER_TOKEN variable in the installation env files) to grant the appropriate privileges for installation. By default this token will be valid for two hours; the populate-users script can be rerun with the same populate-users.env file to regenerate the token if more time is required, or the INSTALL_ADMIN_USERNAME and INSTALL_ADMIN_PASSWORD can be used to generate an authentication token.","title":"Creating Users and Roles"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installing-the-caching-service","text":"","title":"Installing the Caching Service"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#required-for_2","text":"The SCS is REQUIRED for the following use cases.","title":"Required For"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#prerequisites-csp-enterprise","text":"The following must be completed before installing the SGX Caching Service The Certificate Management Service must be installed and available The Authentication and Authorization Service must be installed and available User needs to subscribe to Intel\\'s Provisioning Certificate Service to obtain an API Key The SGX Caching Service database must be available","title":"Prerequisites (CSP &amp; Enterprise)"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#package-dependencies_1","text":"The Intel\u00ae SecL-DC SGX Caching Service (SCS) requires a Postgresql 11 database. A set of scripts (install_pgdb.sh and create_db.sh) is provided with the SCS that will automatically add the Postgresql repositories and install/configure a sample database. If this script will not be used, a Postgresql 11 database must be installed by the user before executing the SCS installation.","title":"Package Dependencies"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#supported-operating-system_1","text":"The Intel\u00ae Security Libraries SGX Caching Service supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver).","title":"Supported Operating System"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#recommended-hardware_2","text":"4 vCPUs RAM: 8 GB 100 GB One network interface with network access to all managed servers. Internet connection is needed for connecting to Intel PCS Server.","title":"Recommended Hardware"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installation_2","text":"Before SCS is installed, Database needs to be created. Use the following commands to install postgres and create SCS DB copy install_pgdb.sh and create_db.sh to /root/ directory ./install_pgdb.sh (if services reside on separate VM) ./create_db.sh <db name> <db_user> <db_password> Copy the SCS installation binary to the /root/ directory. Create the scs.env installation answer file in /root/ directory as below: SCS_DB_USERNAME = <database username> SCS_DB_PASSWORD = <database password> SCS_DB_HOSTNAME = <IP or hostname of database server> SCS_DB_PORT = <Database port ; 5432 by default> SCS_DB_NAME = <name of the SCS database ; pgscsdb by default> SCS_DB_SSLCERTSRC = <path to database TLS certificate ; the default location is typically /usr/local/pgsql/data/server.crt> INTEL_PROVISIONING_SERVER = <hostname of INTEL PCS Server> INTEL_PROVISIONING_SERVER_API_KEY = <subscription key> SCS_REFRESH_HOURS = < time in hours to refresh SGX collaterals ; 1 hour by default> SCS_ADMIN_USERNAME = <username for SCS service account> SCS_ADMIN_PASSWORD = <password for SCS service account> CMS_BASE_URL = https://<IP or hostname to CMS>:8445/cms/v1/ CMS_TLS_CERT_SHA384 = <sha384 of CMS TLS certificate> AAS_API_URL = https://<IP or hostname to AAS>:8444/aas/v1 RETRY_COUNT = 3 WAIT_TIME = 1 SAN_LIST = <comma-separated list of IPs and hostnames for the SCS BEARER_TOKEN = <Installation token> Update the BEARER_TOKEN with the TOKEN obtained after running populate-users.sh script. Execute the SCS installer binary: ./scs-v4.0.0.bin","title":"Installation"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installing-the-sgx-host-verification-service","text":"","title":"Installing the SGX Host Verification Service"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#required-for_3","text":"If SGX Host Verification Service API URL is specified in SGX Agent env file, then SGX Agent will push the platform enablement information and TCB status to SHVS at regular interval.","title":"Required For"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#prerequisites_1","text":"The following must be completed before installing the SGX Host Verification Service: The Certificate Management Service must be installed and available The Authentication and Authorization Service must be installed and available The SGX Host Verification Service database must be available","title":"Prerequisites"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#package-dependencies_2","text":"The Intel\u00ae Security Libraries SGX Host Verification Service requires the following packages and their dependencies: Postgres* client and server 11.6 (server component optional if an external Postgres database is used) Golang packages If they are not already installed, the SGX Host Verification Service installer attempts to install these automatically using the package manager. Automatic installation requires access to package repositories (the RHEL/Ubuntu subscription repositories, the EPEL repository, or a suitable mirror), which may require an Internet connection. If the packages are to be installed from the package repository, be sure to update the repository package lists before installation.","title":"Package Dependencies"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#supported-operating-systems_1","text":"The Intel\u00ae Security Libraries SGX Host Verification Service supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver).","title":"Supported Operating Systems"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#recommended-hardware_3","text":"4 vCPUs RAM: 8 GB 100 GB One network interface with network access to all managed servers","title":"Recommended Hardware"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installation_3","text":"Before SHVS is installed, Database needs to be created. Use the following commands to install postgres and create SHVS DB copy install_pgdb.sh and create_db.sh to /root/ directory ./install_pgdb.sh (if services reside on separate VM) ./create_db.sh <db name> <db_user> <db_password> To install the SGX Host Verification Service, follow these steps: Copy the SGX Host Verification Service installation binary to the /root/ directory. Create the shvs.env installation answer file in /root/ directory as below. A sample minimal shvs.env file is provided below. For all configuration options and their descriptions, refer to the Intel\u00ae SecL Configuration section on the SGX Host Verification Service. SHVS_DB_HOSTNAME=<hostname or IP address to database server> SHVS_DB_USERNAME=<Database administrative username> SHVS_DB_PORT=<Database port, default is 5432> SHVS_DB_PASSWORD=<Database password> SHVS_DB_NAME=<Database schema> SHVS_ADMIN_USERNAME=<SGX Host Verification Service username> SHVS_ADMIN_PASSWORD=<SGX HostVerification Service password> CMS_TLS_CERT_SHA384=<Certificate Management Service TLS digest> SHVS_DB_SSLCERTSRC=/usr/local/pgsql/data/server.crt SHVS_SCHEDULER_TIMER=10 #Maximum allowed time before a platform enablement record in SHVS database is considered as stale SHVS_HOST_PLATFORM_EXPIRY_TIME=240 SHVS_AUTO_REFRESH_TIMER=120 BEARER_TOKEN=<Installation token> AAS_API_URL=https://<Authentication and Authorization Service IP or Hostname>:8444/aas/v1 CMS_BASE_URL=https://<Certificate Management Service IP or Hostname>:8445/cms/v1/ SCS_BASE_URL=https://<SGX Caching Service IP or Hostname>:9000/scs/sgx/ SAN_LIST=<Comma-separated list of IP addresses and hostnames for the SHVS> Update the BEARER_TOKEN with the TOKEN obtained after running populate-users.sh script Execute the installer binary. ./shvs-v4.0.0.bin When the installation completes, the SGX Host Verification Service is available. The service can be verified by running shvs status from the SGX Host Verification Service command line. \\# shvs status","title":"Installation"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installing-the-sgx-agent","text":"","title":"Installing the SGX Agent"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#required-for_4","text":"The SGX Agent is REQUIRED for all use cases. The SGX Agent pushes SGX platform data to SGX Caching Service (SCS). SGX Agent gets current TCB Status for the platform from SCS. If SGX Host Verification Service (SHVS) URL is configured, the SGX Agent pushes platform enablement information and TCB Status to SHVS.","title":"Required for"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#prerequisites_2","text":"The following must be completed before installing the SGX Agent: Certificate Management Service, Authentication and Authorization Service,SGX Caching Service and SGX Host Verification Service must be installed and available. Make sure system date and time of SGX machine and CSP machine both are in sync. Also, if the system is configured to read the RTC time in the local time zone, then use RTC in UTC by running timedatectl set-local-rtc 0 command on both the machine. Otherwise SGX Agent deployment will fail with certificate expiry error.","title":"Prerequisites"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#package-dependencies_3","text":"The Intel\u00ae Security Libraries SGX Agent Service requires the following packages and their dependencies: Golang packages","title":"Package Dependencies"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#supported-operating-systems_2","text":"Red Hat Enterprise Linux 8.2 Ubuntu 18.04.5 LTS(Bionic Beaver). Recommended Hardware Intel\u00ae Xeon\u00ae SP (Ice Lake-SP)","title":"Supported Operating Systems"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installation_4","text":"Copy sgx_agent.tar, sgx_agent.sha2 and agent_untar.sh from binaries directoy to a directory in SGX compute node ./agent_untar.sh Edit agent.conf with the following CSP system IP address where CMS, AAS, SHVS and SCS services deployed Network Port numbers for CMS, AAS, SCS and SHVS CSP Admin credentials (same which are provided in service configuration file. for ex: csp_skc.conf, orchestrator.conf or skc.conf) Token validity period in days CMS TLS SHA Value (Run \"cms tlscertsha384\" on CSP system) Save and Close Note In case orchestration support is not needed, please comment/delete SHVS_IP in agent.conf available in same folder ./deploy_sgx_agent.sh","title":"Installation"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installing-the-sqvs","text":"","title":"Installing the SQVS"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#required-for_5","text":"SGX ECDSA Attestation / SGX Quote Verification by KBS","title":"Required for"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#prerequisites_3","text":"The following must be completed before installing the SQVS: Certificate Management Service, Authentication and Authorization Service and SGX Caching Service must be installed and available.","title":"Prerequisites"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#package-dependencies_4","text":"The Intel\u00ae Security Libraries Quote Verification Service requires the following packages and their dependencies: Golang packages","title":"Package Dependencies"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#supported-operating-systems_3","text":"Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver).","title":"Supported Operating Systems"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#recommended-hardware_4","text":"4 vCPUs RAM: 8 GB 100 GB One network interface with network access to all managed servers","title":"Recommended Hardware"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installation_5","text":"To install the SQVS Service, follow these steps: Copy the SQVS installation binary to the ~/root directory Copy the Root CA Certificate of the Intel PCS Service (Refer to below Note) from sgx-verification-service/dist/linux/ directory as trusted_rootca.pem to the /tmp directory Note Retrieve appropriate Trusted RootCA certificate files for SGX platform (trusted_rootca.pem for pre-production systems using IceLake Sandbox PCS, trusted_rootca_icx_prod.pem for production systems using IceLake Live PCS and trusted_rootca_clx_prod.pem for CascadeLake production systems using Live PCS Server) from dist/linux directory in SQVS repository Create the sqvs.env installation answer file in /root/ directory as below A sample minimal sqvs.env file is provided below. For all configuration options and their descriptions, refer to the Configuration section on the SGX Quote Verification Service. SGX_TRUSTED_ROOT_CA_PATH=< Path where trusted root ca cert for PCS is stored, by default /tmp/trusted_rootca.pem > SCS_BASE_URL=https://< SCS IP or Hostname >:9000/scs/sgx/certification/v1 CMS_TLS_CERT_SHA384=< Certificate Management Service TLS digest > BEARER_TOKEN=< Installation token > AAS_API_URL=https://< Authentication and Authorization Service IP or Hostname >:8444/aas/v1 CMS_BASE_URL=https://< Certificate Management Service IP or Hostname >:8445/cms/v1/ SAN_LIST=< *Comma-separated list of IP addresses and hostnames for the SQVS* > SQVS_NOSETUP=false SQVS_PORT=12000 SQVS_LOGLEVEL=info SQVS_INCLUDE_TOKEN=true Update the BEARER_TOKEN with the TOKEN obtained after running populate-users.sh script Execute the sqvs installer binary. ./sqvs-v4.0.0.bin When the installation completes, the SGX Quote Verification Service is available. The service can be verified by sqvs status from the sqvs command line. \\# sqvs status","title":"Installation"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#setup-k8s-cluster-and-deploy-isecl-k8s-extensions","text":"Setup master and worker node for k8s. Worker node should be setup on SGX enabled host machine. Master node can be any system. To setup k8 cluster on RHEL 8.2, follow https://phoenixnap.com/kb/how-to-install-kubernetes-on-centos To setup k8 cluster on Ubuntu 18.04, follow the \"Install, Enable and start the Docker daemon\" section in Ubuntu Quick Start Guide - https://github.com/intel-secl/docs/blob/v4.0/develop/quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu.md Once done, go to https://vitux.com/install-and-deploy-kubernetes-on-ubuntu/ and follow from step 3 onwards. Once the master/worker setup is done, follow below steps on Master Node:","title":"Setup K8S Cluster and Deploy Isecl-k8s-extensions"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#untar-packages-and-push-oci-images-to-registry","text":"Copy tar output isecl-k8s-extensions-*.tar.gz from build system's binaries folder to /opt/ directory on the Master Node and extract the contents. cd /opt/ tar -xvzf isecl-k8s-extensions-*.tar.gz cd isecl-k8s-extensions/ Configure private registry Push images to private registry using skopeo command, (this can be done from build vm also) skopeo copy oci-archive:isecl-k8s-controller-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-controller:v4.0.0 skopeo copy oci-archive:isecl-k8s-scheduler-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-scheduler:v4.0.0 Add the image names in isecl-controller.yml and isecl-scheduler.yml in /opt/isecl-k8s-extensions/yamls with full image name including registry IP/hostname (e.g : /isecl-k8s-scheduler:v4.0.0). It will automatically pull the images from registry.","title":"Untar packages and push OCI images to registry"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#deploy-isecl-controller","text":"Create hostattributes.crd.isecl.intel.com crd kubectl apply -f yamls/crd-1.17.yaml Check whether the crd is created kubectl get crds Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterrolebinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole=system:node --user=system:serviceaccount:isecl:isecl Fetch token required for ihub installation and follow below steps to update ihub.env, kubectl get secrets -n isecl kubectl describe secret default-token-<name> -n isecl For IHUB installation, make sure to update below configuration in /root/binaries/env/ihub.env before installing ihub on CSP VM: Copy /etc/kubernetes/pki/apiserver.crt from master node to /root on CSP VM. Update KUBERNETES_CERT_FILE. Get k8s token in master, using above commands and update KUBERNETES_TOKEN Update the value of CRD name KUBERNETES_CRD=custom-isecl-sgx","title":"Deploy isecl-controller"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#deploy-isecl-scheduler","text":"The isecl-scheduler default configuration is provided for common cluster support in /opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml. Variables HVS_IHUB_PUBLIC_KEY_PATH and SGX_IHUB_PUBLIC_KEY_PATH are by default set to default paths. Please use and set only required variables based on the use case. For example, if only sgx based attestation is required then remove/comment HVS_IHUB_PUBLIC_KEY_PATH variables. Install cfssl and cfssljson on Kubernetes Control Plane #Download cfssl to /usr/local/bin/ wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl #Download cfssljson to /usr/local/bin wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create tls key pair for isecl-scheduler service, which is signed by k8s apiserver.crt cd /opt/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \"<K8_MASTER_IP>\",\"<K8_MASTER_HOST>\" -c /etc/kubernetes/pki/ca.crt -k /etc/kubernetes/pki/ca.key After iHub deployment, copy /etc/ihub/ihub_public_key.pem from ihub to /opt/isecl-k8s-extensions/ directory on k8 master vm. Also, copy tls key pair generated in previous step to secrets directory. mkdir secrets cp /opt/isecl-k8s-extensions/server.key secrets/ cp /opt/isecl-k8s-extensions/server.crt secrets/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem cp /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem secrets/ Note Prefix the attestation type for ihub_public_key.pem before copying to secrets folder. Create kubernetes secrets scheduler-secret for isecl-scheduler kubectl create secret generic scheduler-certs --namespace isecl --from-file=secrets Deploy isecl-scheduler kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl","title":"Deploy isecl-scheduler"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#configure-kube-scheduler-to-establish-communication-with-isecl-scheduler","text":"Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec: containers: - command: - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml. Restart Kubelet which restart all the k8s services including kube base schedular systemctl restart kubelet Check if CRD Data is populated kubectl get -o json hostattributes.crd.isecl.intel.com","title":"Configure kube-scheduler to establish communication with isecl-scheduler"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installing-the-integration-hub","text":"Note: The Integration Hub is only required to integrate Intel\u00ae SecL with third-party scheduler services, such as Kubernetes. The Integration Hub is not required for usage models that do not require Intel\u00ae SecL security attributes to be pushed to an integration endpoint.","title":"Installing the Integration Hub"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#required-for_6","text":"The Integration Hub is REQUIRED the for enabling support for orchestration support.","title":"Required For"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#prerequisites_4","text":"The Intel\u00ae Security Libraries Integration Hub can be run on a VM or on a bare-metal server. The Integration Hub may be installed on the same server (physical or VM) as the SGX Host Verification Service. SGX Caching Service must be installed and available. The SGX Host Verification Service must be installed and available The Authentication and Authorization Service must be installed and available The Certificate Management Service must be installed and available","title":"Prerequisites"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#package-dependencies_5","text":"The Intel\u00ae SecL Integration Hub requires a number of packages and their dependencies: Golang packages If these are not already installed, the Integration Hub installer attempts to install these packages automatically using the package manager. Automatic installation requires access to package repositories (the RHEL/Ubuntu subscription repositories, the EPEL repository, or a suitable mirror), which may require an Internet connection. If the packages are to be installed from the package repository, be sure to update your repository package lists before installation.","title":"Package Dependencies"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#supported-operating-systems_4","text":"Red Hat Enterprise Linux 8.2 Ubuntu 18.04.5 LTS(Bionic Beaver).","title":"Supported Operating Systems"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#recommended-hardware_5","text":"1 vCPUs RAM: 2 GB 1 GB free space to install the Integration Hub (database and log space requirements are dependent on the number of managed servers). One network interface with network access to the SGX Host Verification Service. One network interface with network access to any integration endpoints (for example, OpenStack Nova).","title":"Recommended Hardware"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installing-the-integration-hub_1","text":"To install the SGX Integration Hub, follow these steps: Copy the Integration Hub installation binary to the /root/ directory. Create the ihub.env installation answer file in /root/ directory as below IHUB_SERVICE_USERNAME=< IHUB service user username > IHUB_SERVICE_PASSWORD=< IHUB service user password > SHVS_BASE_URL=< https://< SHVS IP or Hostname >:13000/sgx-hvs/v2 CMS_TLS_CERT_SHA384=< CMS TLS digest > BEARER_TOKEN=<Installation token> AAS_API_URL=https://< AAS IP or Hostname >:8444/aas/v1 CMS_BASE_URL=https://< CMS IP or Hostname >:8445/cms/v1 POLL_INTERVAL_MINUTES=2 TLS_SAN_LIST=< comma separated list of IPs and hostnames for the IHUB > TENANT=< tenant-type e.g. KUBERNETES or OPENSTACK > # Kubernetes Integration Credentials - required for Kubernetes integration only KUBERNETES_URL=< https://< Kubernetes IP >:6443/> KUBERNETES_CRD=custom-isecl-sgx KUBERNETES_TOKEN=< K8S token > KUBERNETES_CERT_FILE =< Path of Kubernetes master node certificate > # OpenStack Integration Credentials - required for OpenStack integration only OPENSTACK_AUTH_URL=<OpenStack Keystone URL; typically http://openstack-ip:5000/> OPENSTACK_PLACEMENT_URL=<OpenStack Nova Placement API URL; typically http://openstack-ip:8778/> OPENSTACK_USERNAME=< OpenStack username > OPENSTACK_PASSWORD=< OpenStack password > Create Integrated Hub Service user account and Roles. A sample script is provided in the appendix section for reference. Update the BEARER_TOKEN with the TOKEN obtained after running populate-users.sh script Execute the installer binary. ./ihub-v4.0.0.bin In case installation fails, its recommended to run the following command to clear failed service instance systemctl reset-failed Copy IHUB public key to the master node and restart kubelet. scp -r /etc/ihub/ihub_public_key.pem <master-node IP>:/opt/isecl-k8s-extensions/ systemctl restart kubelet Run this command to validate if the data has been pushed to CRD: kubectl get -o json hostattributes.crd.isecl.intel.com Run this command to validate that the labels have been populated: kubectl get nodes --show-labels. Sample labels: EPC-Memory=2.0GB,FLC-Enabled=true,SGX-Enabled=true,SGX-Supported=true,SgxTrustExpiry=2020-11-09T08.07.43Z,TCBUpToDate=true Create sample yml file for nginx workload and add SGX labels to it such as: apiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: SGX-Enabled operator: In values: - \"true\" - key: EPC-Memory operator: In values: - \"2.0GB\" containers: - name: nginx image: nginx ports: - containerPort: 80 Validate if pod can be launched on the node. Run following commands: kubectl apply -f pod.yml kubectl get pods kubectl describe pods nginx Pod should be in running state and launched on the host as per values in pod.yml. Validate running below commands on sgx host: docker ps","title":"Installing the Integration Hub"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#integration-with-openstack-supported-only-on-rhel-82-not-supported-on-ubuntu-1804","text":"OpenStack can now use \u201cTraits\u201d to provide qualitative data about Nova Compute hosts to establish Trait requirements. The Integration Hub continually push SGX data to the OpenStack Traits resources. This means OpenStack scheduler natively supports workload scheduling incorporating SGX Host information, including SGX enabled/disabled, SGX supported/not supported, FLC enabled/not enabled, EPC memory size, TCB status upto date/not. The OpenStack Placement Service will automatically attempt to place images with Trait requirements on compute nodes that have those Traits. Note This control only applies to instances launched using the OpenStack scheduler, and the Traits functions will not affect manually-launched instances where a specific Compute Node is defined (since this does not use the scheduler at all). Intel SecL-DC uses existing OpenStack interfaces and does not modify OpenStack code. The datacenter owner or OpenStack administrator is responsible for the security of the OpenStack workload scheduling process in general, and Intel recommends following published OpenStack security best practices. Setting Image Traits Image Traits define the policy for which Traits are required for that instance to be launched on a Nova Compute node.By setting these Traits to \u201crequired\u201d the OpenStack scheduler will require the same Traits to be present on a Nova Compute node in order to launch instances. To set the Image Traits for Intel SecL-DC,a specific naming convention is used. This naming convention will match the Traits that the Integration Hub will automatically push to OpenStack. Two types of Traits are currently supported \u2013 one Trait is used to require that the Compute Node must be SGX supported and the other Trait is used to require specific SGXkey/value pairs. Required Image trait for SGX Enabled Host: CUSTOM_ISECL_SGX_ENABLED_TRUE=required These Traits can be set using CLI commands for OpenStack Glance: openstack image set --property trait:CUSTOM_ISECL_SGX_ENABLED_TRUE=required <image name> To veiw the Traits that has been set: openstack image show List the set of resources mapped to the Openstack openstack resource provider list To view the traits enabled for the SGX Host: openstack resource provider trait list <uuid of the host which the openstack resoruce provider lists> Create the instances openstack server create --flavor tiny --image <image name> --net vmnet <vm instance name> Instances should be created and the status should be \"Active\". Instance should be launched successfully. openstack server list To remove a Trait that is not required for an Image: openstack image unset --property trait:CUSTOM_ISECL_SGX_ENABLED_TRUE openstack image unset --property trait:CUSTOM_ISECL_SGX_ENABLED_FALSE Scheduling Instances Once Trait requirements are set for Images and the Integration Hub is configured to push attributes to OpenStack, instances can be launched in OpenStack as normal. As long as the OpenStack Nova scheduler is used to schedule the workloads, only compliant Compute Nodes will be scheduled to run instances of controlled Images. Note This control only applies to instances launched using the OpenStack scheduler and the Traits functions will not affect manually-launched instances where a specific Compute Node is defined (since this does not use the scheduler at all). Intel SecL-DC uses existing OpenStack interfaces and does not modify OpenStack code. The datacenter owner or OpenStack administrator is responsible for the security of the OpenStack workload scheduling process in general and Intel recommends following published OpenStack security best practices.","title":"Integration with OpenStack (Supported only on RHEL 8.2, not supported on Ubuntu 18.04)"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installing-the-key-broker-service","text":"","title":"Installing the Key Broker Service"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#required-for_7","text":"The KBS is REQUIRED for - Storing Application Keys and Verifying the SGX Quote","title":"Required for"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#prerequisites_5","text":"The following must be completed before installing the Key Broker: The Authentication and Authorization Service must be installed and available The Certificate Management Service must be installed and available If a 3 rd -party Key Management server will be used following must be completed before installing the Key Broker: A KMIP 3 rd -party Key management Server must be available. The Key Broker will require the KMIP server\u2019s client certificate, client key and root ca certificate. This key and certificate will be available in KMIP server. Note The Key Broker has been validated using the pykmip 0.9.1 KMIP server as a 3 rd -party Key Management Server. While any general KMIP 2.0-compliant Key Management Server should work, implementation differences among KMIP providers may prevent functionality with specific providers.","title":"Prerequisites"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#package-dependencies_6","text":"N/A","title":"Package Dependencies"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#supported-operating-systems_5","text":"supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver).","title":"Supported Operating Systems"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#recommended-hardware_6","text":"NA","title":"Recommended Hardware"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installation_6","text":"Copy the Key Broker installation binary to the /root/ directory. Create the installation answer file kbs.env /root/ directory as below: KBS_SERVICE_USERNAME=< KBS service user username > KBS_SERVICE_PASSWORD=< KBS service user password > SERVER_PORT=9443 AAS_API_URL=https://<AAS IP or hostname>:8444/aas/v1 CMS_BASE_URL=https://<CMS IP or hostname>:8445/cms/v1/ SQVS_URL=https://<SQVS IP or hostname>:12000/svs/v1 ### KEY_MANAGER is set to KMIP KEY_MANAGER=KMIP ENDPOINT_URL=https://<KBS Hostname>:9443/v1 TLS_COMMON_NAME=\"KBS TLS Certificate\" SKC_CHALLENGE_TYPE=\"SGX\" CMS_TLS_CERT_SHA384=<SHA384 hash of CMS TLS certificate> TLS_SAN_LIST=<KBS Hostname/IP> BEARER_TOKEN=<Installation token from AAS> ## Session Expiry Time Between KBS and SKC Library in Minutes SESSION_EXPIRY_TIME=60 KMIP_SERVER_IP=<IP address of KMIP server> KMIP_SERVER_PORT=<Port number of KMIP server> ## KMIP_VERSION variable can be used to mention KMIP protocol version. ## This is an OPTIONAL field, default value is set to '2.0'. KBS supports KMIP version '1.4' and '2.0'. KMIP_VERSION=<kmip version> ## KMIP_HOSTNAME can be used to configure TLS config with ServerName. ## KMIP server certificate should contain SAN(IP/DNS) or valid COMMON NAME and this value can be provided in KMIP_HOSTNAME. Only FQDN names are allowed. ## This is an OPTIONAL field; if KMIP_HOSTNAME is not provided then KMIP_SERVER_IP will be considered as ServerName in TLS configuration. KMIP_HOSTNAME=<Hostname of KMIP server> ## KMIP supports authentication mechanism to authenticate requestor. This is an OPTIONAL field. ## This feature can be added to KBS by updating kbs.env with KMIP_USERNAME and KMIP_PASSWORD. ## These are OPTIONAL variables. PyKMIP doesn't supports this feature. This feature is validated in Thales cipher trust manager. KMIP_USERNAME=<Username of KMIP server> KMIP_PASSWORD=<Password of KMIP server> ### Retrieve the following certificates and keys from the KMIP server KMIP_CLIENT_KEY_PATH=<path>/client_key.pem KMIP_ROOT_CERT_PATH=<path>/root_certificate.pem KMIP_CLIENT_CERT_PATH=<path>/client_certificate.pem Update the BEARER_TOKEN with the TOKEN obtained after running populate-users.sh script Execute the KBS installer. ./kbs-4.0.0.bin Note When a 3 rd -party Key Management Server is used, KBS supports only association of RSA key. User needs to create RSA key in backend kmip server and note the Private key Id. User also needs to create the tls certificate for the private key (it will be used during key transfer). rsa-create.py available in kbs_scripts can be used to create the private key and generate the certificate by providing the kmip server ip and certificates path in the script.","title":"Installation"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installing-the-skc-library","text":"","title":"Installing the SKC Library"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#required-for_8","text":"The SKC_Library enables secure transfer of application keys from KBS after performing SGX attestation. It stores the keys in the SGX enclave and performs crypto operations ensuring the keys are never exposed in use, at rest and in transit outside of enclave.","title":"Required For"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#package-dependencies_7","text":"The Intel\u00ae Security Libraries SKC Library requires the following packages and their dependencies: Openssl Curl","title":"Package Dependencies"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#supported-operation-system","text":"The Intel\u00ae Security Libraries SKC Library supports Red Hat Enterprise Linux 8.2 and Ubuntu 18.04.5 LTS(Bionic Beaver).","title":"Supported Operation System"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#recommended-hardware_7","text":"Icelake Server with SGX enabled in BIOS RAM: 8 GB 100 GB One network interface with network access to the Key Broker","title":"Recommended Hardware"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#installation_7","text":"Copy skc_library.tar, skc_library.sha2 and skclib_untar.sh from binaries directoy to a directory in SGX compute node ./skclib_untar.sh Update create_roles.conf with the following - IP address of AAS deployed on Enterprise system - Admin account credentials of AAS deployed on Enterprise system. These credentials should match with the AAS admin credentials provided in authservice.env on enterprise side. - Permission string to be embedded into skc_libraty client TLS Certificate - For Each SKC Library installation on a SGX compute node, please change SKC_USER and SKC_USER_PASSWORD Save and Close ./skc_library_create_roles.sh Copy the token printed on console. Update skc_library.conf with the following - IP address for CMS and KBS services deployed on Enterprise system - CSP_CMS_IP should point to the IP of CMS service deployed on CSP system - CSP_SCS_IP should point to the IP of SCS service deployed on CSP system - Hostname of the Enterprise system where KBS is deployed - Network Port numbers for CMS and SCS services deployed on CSP system - Network Port numbers for CMS and KBS services deployed on Enterprise system - For Each SKC Library installation on a SGX compute node, please change SKC_USER (should be same as SKC_USER provided in create_roles.conf) - SKC_TOKEN with the token copied from previous step Save and Close ./deploy_skc_library.sh","title":"Installation"},{"location":"product-guides/SGX%20Infrastructure/3Intel%C2%AE%20Security%20Libraries%20Installation/#deploying-skc-library-as-a-container-supported-only-on-rhel-82-not-supported-on-ubuntu-1804","text":"Use the following steps to configure SKC library running in a container and to validate key transfer in container on bare metal and inside a VM on SGX enabled hosts. Note: All the configuration files required for SKC Library container are modified in the resources directory only 1. Docker should be installed, enabled and services should be active 2. In the build System, SKC Library tar file \"<skc-lib*>.tar\" required to load is located in the \"/root/workspace/skc_library\" directory. 3. Copy \"resources\" folder from \"workspace/skc_library/container/resources\" to the \"/root/\" directory of SGX host. Inside the resources folder all the key transfer flow related files will be available. 4. Update sgx_default_qcnl.conf file inside resources folder with SCS IP and SCS port and update hosts file present in same folder with KBS IP and hostname. 5. Generate the RSA key in the kbs host and copy it to SGX host. 6. Refer to openssl and nginx sub sections of QSG in the \"Configuration for NGINX testing\" to configure nginx.conf and openssl.conf present resource in the directory. 7. Update keyID in the keys.txt and nginx.conf. 8. Under [core] section of pkcs11-apimodule.ini in the \"/root/resources/\" directory add preload_keys=/root/keys.txt. 9. Update skc_library.conf with IP addresses where SKC services are deployed. 10. On the SGX Compute node, load the skc library docker image provided in the tar file. docker load < <SKC_Library>.tar 11. Provide valid paramenets in the docker run command and execute the docker run command. Update the genertaed RSA Key ID and <keys>.crt in the resources directory. docker run -p 8080:2443 -p 80:8080 --mount type=bind,source=/root/<KBS_cert>.crt,target=/root/<KBS_cert>.crt --mount type=bind,source=/root/resources/sgx_default_qcnl.conf,target=/etc/sgx_default_qcnl.conf --mount type=bind,source=/root/resources/nginx.conf,target=/etc/nginx/nginx.conf --mount type=bind,source=/root/resources/keys.txt,target=/root/keys.txt,readonly --mount type=bind,source=/root/resources/pkcs11-apimodule.ini,target=/opt/skc/etc/pkcs11-apimodule.ini,readonly --mount type=bind,source=/root/resources/openssl.cnf,target=/etc/pki/tls/openssl.cnf --mount type=bind,source=/root/resources/skc_library.conf,target=/skc_library.conf --add-host=<SHC_HOSTNAME>:<SGX_HOST_IP> --add-host=<KBS_Hostname>:<KBS host IP> --mount type=bind,source=/dev/sgx,target=/dev/sgx --cap-add=SYS_MODULE --privileged=true <SKC_LIBRARY_IMAGE_NAME> Note: In the above docker run command, source refers to the actual path of the files located on the host and the target always refers to the files which would be mounted inside the container 12. Restore index.html for the transferred key inside the container Get the container id using \"docker ps\" command docker exec -it <container_id> /bin/sh Download index.html wget https://localhost:2443 --no-check-certificate","title":"Deploying SKC Library as a Container (Supported only on RHEL 8.2, not supported on Ubuntu 18.04)"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/","text":"Authentication Authentication is centrally managed by the Authentication and Authorization Service (AAS). This service uses a Bearer Token authentication method. This service also centralizes the creation of roles and users, allowing much easier management of users, passwords, and permissions across all Intel\u00ae SecL-DC services. To make an API request to an Intel\u00ae SecL-DC service, an authentication token is required. API requests must now include an Authorization header with a valid token The token is issued by AAS and expires after a set amount of time. This token may be used with any Intel\u00ae SecL-DC service and will carry the appropriate permissions for the role(s) assigned to the account the token was generated for. The SKC solution involves AAS deployments for 2 different domains: the CSP domain and the tenant domain. There is no trust relationship between the 2 deployments. In SKC, the accounts of the SGX Services are created at install time. However, CSP admin users must obtain AAS tokens to invoke admin APIs in the SGX Host Verification Service (SHVS), the SGX Hub, the SGX Caching Service (SCS) and AAS. Similarly, the tenant admin needs AAS tokens to invoke Create, Read, Update and Delete (CRUD) APIs in KBS and admin APIs in AAS. The following sections present how to use AAS APIs to create tokens and manage users. Create Token To request a new token from the AAS: POST https:// \\< AAS IP or hostname \\> :8444/aas/v1/token { \\\" username \\\" : \\\"\\< username \\>\\\" , \\\" password \\\" : \\\"\\< password \\>\\\" } The response will be a token that can be used in the Authorization header for other requests. The length of time for which the token will be valid is configured on the AAS using the key ~AAS_JWT_TOKEN_DURATION_MINS~ (in the installation answer file during installation) or aas.jwt.token.duration.mins (configured on the AAS after installation). In both cases the value is the length of time in minutes that issued tokens will remain valid before expiring. User Management Users in Intel\u00ae SecL-DC are centrally managed by the Authentication and Authorization Service (AAS). Any user may be assigned roles for any service, allowing user accounts to be fully defined by the tasks needed Username and Password Requirement Passwords have the following constraints: cannot be empty - ie must at least have one character maximum length of 255 characters Usernames have the following requirements: Format: username[\\@host_name[domain]] [\\@host_name[domain]] is optional username shall be minimum of 2 and maximum of 255 characters username allowed characters are alphanumeric, ., -, _ - but cannot start with -. Domain name must meet requirements of a host name or fully qualified internet host name (Update it relevant to SKC) Create User POST https:// \\< IP or hostname of AAS \\> :8444/aas/v1/users Authorization: Bearer \\< token \\> { \\\" username \\\" : \\\"\\< username \\>\\\" , \\\" password \\\" : \\\"\\< password \\>\\\" } Search Users by Username GET https:// \\< IP or hostname of AAS \\> :8444/aas/v1/users?name = \\< value \\> Authorization: Bearer \\< token \\> Change User Password PATCH https:// \\< IP or hostname of AAS \\> :8444/aas/v1/users/changepassword { \\\" username \\\" : \\\"\\< username \\>\\\" , \\\" old_password \\\" : \\\"\\< old_password \\>\\\" , \\\" new_password \\\" : \\\"\\< new_password \\>\\\" , \\\" password_confirm \\\" : \\\"\\< new_password \\>\\\" } Delete User DELETE https:// \\< IP or hostname of AAS \\> :8444/aas/v1/users/ \\< User ID \\> Authorization: Bearer \\< token \\> Roles and Permission Permissions in Intel\u00ae SecL-DC are managed by Roles. Roles are a set of predefined permissions applicable to a specific service. Any number of Roles may be applied to a User. While new Roles can be created, each Intel\u00ae SecL service defines permissions that are applicable to specific predetermined Roles. This means that only pre-defined Roles will actually have any permissions. Role creation is intended to allow Intel\u00ae SecL-DC services to define their permissions while allowing role and user management to be centrally managed on the AAS. When a new service is installed, it will use the Role creation functions to define roles applicable for that service in the AAS. Create Roles POST https:// \\< AAS IP or Hostname \\> :8444/aas/v1/roles Authorization: Bearer \\< token \\> { \\\" service \\\" : \\\"\\< Service name \\>\\\" , \\\" name \\\" : \\\"\\< Role Name \\>\\\" . \"permissions\" : \\[\\< array of permissions \\>\\] } Service field contains a minimum of 1 and maximum of 20 characters. Allowed characters are alphanumeric plus the special charecters -, _, @, ., , Name field contains a minimum of 1 and maximum of 40 characters. Allowed characters are alphanumeric plus the special characters -, _, @, ., , Service and Name fields are mandatory Context field is optional and can contain up to 512 characters. Allowed characters are alphanumeric plus -, _, @, ., ,,=,;,:,* Permissions field is optional and allow up to a maximum of 512 characters. The Permissions array must a comma-separated list of permissions formatted as resource:action: Permissions required to execute specific API requests are listed with the API resource and method definitions in the API documentation. Search Roles GET https:// \\< AAS IP or Hostname \\> :8444/aas/v1/roles? \\< parameter \\> = \\< value \\> Authorization: Bearer \\< token \\> Search parameters supported: Service = \\< name of service \\> Name = \\< role name \\> Context = \\< context \\> contextContains = \\< partial \"context\" string \\> allContexts = \\< true or false \\> filter = false Delete Role DELETE https:// \\< AAS IP or Hostname \\> :8444/aas/v1/roles/ \\< role ID \\> Authorization: Bearer \\< token \\> Assign Role to User POST https:// \\< AAS IP or Hostname \\> :8444/aas/v1/users/ \\< user ID \\> /roles Authorization: Bearer \\< token \\> { \\\" role_ids \\\" : \\[\\\"\\< comma-separated list of role IDs \\>\\\"\\] } List Roles Assigned to User GET https:// \\< AAS IP or Hostname \\> :8444/aas/v1/users/ \\< user ID \\> /roles Authorization: Bearer \\< token \\> Remove Role from User DELETE https:// \\< AAS IP or Hostname \\> :8444/aas/v1/users/ \\< user ID \\> /roles/ \\< role ID \\> Authorization: Bearer \\< token \\> Role Definitions Following are the set of roles which are required during installation and runtime. Role Name Permissions Utility < SHVS:HostDataUpdater: > Used by the SGX_Agent to push host data to the SHVS < SHVS:HostsListReader: > Used by the IHUB to retrieve the list of hosts from SHVS < SHVS:HostDataReader: > Used by the IHUB to retrieve platform-data from SHVS < CMS:CertApprover:CN=SHVS TLS Certificate;SAN= ;CERTTYPE=TLS> Used by the SHVS to retrieve TLS Certificate from CMS < CMS:CertApprover:CN=Integration HUB TLS Certificate;SAN= ;CERTTYPE=TLS> Used by the IHUB to retrieve TLS Certificate from CMS < SCS:HostDataUpdater: > Used by the SGX_Agent to push the platform-info to SCS < SCS:HostDataReader: > Used by the SGX_Agent to retrieve the TCB status info from SCS < SCS:CacheManager: > Used by the SCS admin to refresh the platform info < CMS:CertApprover:CN=SCS TLS Certificate;SAN= ;CERTTYPE=TLS> Used by the SCS to retrieve TLS Certificate from CMS < KBS:KeyTransfer:permissions=nginx,USA > Used by the SKC Library user for Key Transfer < CMS:CertApprover:CN=skcuser;CERTTYPE=TLS-Client> Used by the SKC Library user to retrieve TLS-Client Certificate from CMS < CMS:CertApprover:CN=KBS TLS Certificate;SAN= ;CERTTYPE=TLS> Used by the KBS to retrieve TLS Certificate from CMS AAS: Administrator : :* Administrator role for the AAS only. Has all permissions for AAS resources, including the ability to create or delete users and roles AAS: RoleManager AAS: [roles:create: , roles:retrieve: , roles:search: , roles:delete: ] AAS role that allows all actions for Roles but cannot create or delete Users or assign Roles to Users. AAS: UserManager AAS: [users:create: , users:retrieve: , users:store: , users:search: , users:delete:*] AAS role with all permissions for Users but has no ability to create Roles or assign Roles to Users. AAS: UserRoleManager AAS: [user_roles:create: , user_roles:retrieve: , user_roles:search: , user_roles:delete: ] AAS role with permissions to assign Roles to Users but cannot create delete or modify Users or Roles. < SHVS:HostListManager:> Used by the SHVS admin to delete the hosts. < SQVS:QuoteVerifier: > Used by the KBS service user for quote verification SGX Agent The SGX Agent communicates with SGX Caching Service (SCS) and SGX Host Verification Service (SHVS) directly. Authentication has been centralized with the new Authentication and Authorization Service.","title":"Authentication"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#authentication","text":"Authentication is centrally managed by the Authentication and Authorization Service (AAS). This service uses a Bearer Token authentication method. This service also centralizes the creation of roles and users, allowing much easier management of users, passwords, and permissions across all Intel\u00ae SecL-DC services. To make an API request to an Intel\u00ae SecL-DC service, an authentication token is required. API requests must now include an Authorization header with a valid token The token is issued by AAS and expires after a set amount of time. This token may be used with any Intel\u00ae SecL-DC service and will carry the appropriate permissions for the role(s) assigned to the account the token was generated for. The SKC solution involves AAS deployments for 2 different domains: the CSP domain and the tenant domain. There is no trust relationship between the 2 deployments. In SKC, the accounts of the SGX Services are created at install time. However, CSP admin users must obtain AAS tokens to invoke admin APIs in the SGX Host Verification Service (SHVS), the SGX Hub, the SGX Caching Service (SCS) and AAS. Similarly, the tenant admin needs AAS tokens to invoke Create, Read, Update and Delete (CRUD) APIs in KBS and admin APIs in AAS. The following sections present how to use AAS APIs to create tokens and manage users.","title":"Authentication"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#create-token","text":"To request a new token from the AAS: POST https:// \\< AAS IP or hostname \\> :8444/aas/v1/token { \\\" username \\\" : \\\"\\< username \\>\\\" , \\\" password \\\" : \\\"\\< password \\>\\\" } The response will be a token that can be used in the Authorization header for other requests. The length of time for which the token will be valid is configured on the AAS using the key ~AAS_JWT_TOKEN_DURATION_MINS~ (in the installation answer file during installation) or aas.jwt.token.duration.mins (configured on the AAS after installation). In both cases the value is the length of time in minutes that issued tokens will remain valid before expiring.","title":"Create Token"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#user-management","text":"Users in Intel\u00ae SecL-DC are centrally managed by the Authentication and Authorization Service (AAS). Any user may be assigned roles for any service, allowing user accounts to be fully defined by the tasks needed","title":"User Management"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#username-and-password-requirement","text":"Passwords have the following constraints: cannot be empty - ie must at least have one character maximum length of 255 characters Usernames have the following requirements: Format: username[\\@host_name[domain]] [\\@host_name[domain]] is optional username shall be minimum of 2 and maximum of 255 characters username allowed characters are alphanumeric, ., -, _ - but cannot start with -. Domain name must meet requirements of a host name or fully qualified internet host name (Update it relevant to SKC)","title":"Username and Password Requirement"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#create-user","text":"POST https:// \\< IP or hostname of AAS \\> :8444/aas/v1/users Authorization: Bearer \\< token \\> { \\\" username \\\" : \\\"\\< username \\>\\\" , \\\" password \\\" : \\\"\\< password \\>\\\" }","title":"Create User"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#search-users-by-username","text":"GET https:// \\< IP or hostname of AAS \\> :8444/aas/v1/users?name = \\< value \\> Authorization: Bearer \\< token \\>","title":"Search Users by Username"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#change-user-password","text":"PATCH https:// \\< IP or hostname of AAS \\> :8444/aas/v1/users/changepassword { \\\" username \\\" : \\\"\\< username \\>\\\" , \\\" old_password \\\" : \\\"\\< old_password \\>\\\" , \\\" new_password \\\" : \\\"\\< new_password \\>\\\" , \\\" password_confirm \\\" : \\\"\\< new_password \\>\\\" }","title":"Change User Password"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#delete-user","text":"DELETE https:// \\< IP or hostname of AAS \\> :8444/aas/v1/users/ \\< User ID \\> Authorization: Bearer \\< token \\>","title":"Delete User"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#roles-and-permission","text":"Permissions in Intel\u00ae SecL-DC are managed by Roles. Roles are a set of predefined permissions applicable to a specific service. Any number of Roles may be applied to a User. While new Roles can be created, each Intel\u00ae SecL service defines permissions that are applicable to specific predetermined Roles. This means that only pre-defined Roles will actually have any permissions. Role creation is intended to allow Intel\u00ae SecL-DC services to define their permissions while allowing role and user management to be centrally managed on the AAS. When a new service is installed, it will use the Role creation functions to define roles applicable for that service in the AAS.","title":"Roles and Permission"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#create-roles","text":"POST https:// \\< AAS IP or Hostname \\> :8444/aas/v1/roles Authorization: Bearer \\< token \\> { \\\" service \\\" : \\\"\\< Service name \\>\\\" , \\\" name \\\" : \\\"\\< Role Name \\>\\\" . \"permissions\" : \\[\\< array of permissions \\>\\] } Service field contains a minimum of 1 and maximum of 20 characters. Allowed characters are alphanumeric plus the special charecters -, _, @, ., , Name field contains a minimum of 1 and maximum of 40 characters. Allowed characters are alphanumeric plus the special characters -, _, @, ., , Service and Name fields are mandatory Context field is optional and can contain up to 512 characters. Allowed characters are alphanumeric plus -, _, @, ., ,,=,;,:,* Permissions field is optional and allow up to a maximum of 512 characters. The Permissions array must a comma-separated list of permissions formatted as resource:action: Permissions required to execute specific API requests are listed with the API resource and method definitions in the API documentation.","title":"Create Roles"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#search-roles","text":"GET https:// \\< AAS IP or Hostname \\> :8444/aas/v1/roles? \\< parameter \\> = \\< value \\> Authorization: Bearer \\< token \\> Search parameters supported: Service = \\< name of service \\> Name = \\< role name \\> Context = \\< context \\> contextContains = \\< partial \"context\" string \\> allContexts = \\< true or false \\> filter = false","title":"Search Roles"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#delete-role","text":"DELETE https:// \\< AAS IP or Hostname \\> :8444/aas/v1/roles/ \\< role ID \\> Authorization: Bearer \\< token \\>","title":"Delete Role"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#assign-role-to-user","text":"POST https:// \\< AAS IP or Hostname \\> :8444/aas/v1/users/ \\< user ID \\> /roles Authorization: Bearer \\< token \\> { \\\" role_ids \\\" : \\[\\\"\\< comma-separated list of role IDs \\>\\\"\\] }","title":"Assign Role to User"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#list-roles-assigned-to-user","text":"GET https:// \\< AAS IP or Hostname \\> :8444/aas/v1/users/ \\< user ID \\> /roles Authorization: Bearer \\< token \\>","title":"List Roles Assigned to User"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#remove-role-from-user","text":"DELETE https:// \\< AAS IP or Hostname \\> :8444/aas/v1/users/ \\< user ID \\> /roles/ \\< role ID \\> Authorization: Bearer \\< token \\>","title":"Remove Role from User"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#role-definitions","text":"Following are the set of roles which are required during installation and runtime. Role Name Permissions Utility < SHVS:HostDataUpdater: > Used by the SGX_Agent to push host data to the SHVS < SHVS:HostsListReader: > Used by the IHUB to retrieve the list of hosts from SHVS < SHVS:HostDataReader: > Used by the IHUB to retrieve platform-data from SHVS < CMS:CertApprover:CN=SHVS TLS Certificate;SAN= ;CERTTYPE=TLS> Used by the SHVS to retrieve TLS Certificate from CMS < CMS:CertApprover:CN=Integration HUB TLS Certificate;SAN= ;CERTTYPE=TLS> Used by the IHUB to retrieve TLS Certificate from CMS < SCS:HostDataUpdater: > Used by the SGX_Agent to push the platform-info to SCS < SCS:HostDataReader: > Used by the SGX_Agent to retrieve the TCB status info from SCS < SCS:CacheManager: > Used by the SCS admin to refresh the platform info < CMS:CertApprover:CN=SCS TLS Certificate;SAN= ;CERTTYPE=TLS> Used by the SCS to retrieve TLS Certificate from CMS < KBS:KeyTransfer:permissions=nginx,USA > Used by the SKC Library user for Key Transfer < CMS:CertApprover:CN=skcuser;CERTTYPE=TLS-Client> Used by the SKC Library user to retrieve TLS-Client Certificate from CMS < CMS:CertApprover:CN=KBS TLS Certificate;SAN= ;CERTTYPE=TLS> Used by the KBS to retrieve TLS Certificate from CMS AAS: Administrator : :* Administrator role for the AAS only. Has all permissions for AAS resources, including the ability to create or delete users and roles AAS: RoleManager AAS: [roles:create: , roles:retrieve: , roles:search: , roles:delete: ] AAS role that allows all actions for Roles but cannot create or delete Users or assign Roles to Users. AAS: UserManager AAS: [users:create: , users:retrieve: , users:store: , users:search: , users:delete:*] AAS role with all permissions for Users but has no ability to create Roles or assign Roles to Users. AAS: UserRoleManager AAS: [user_roles:create: , user_roles:retrieve: , user_roles:search: , user_roles:delete: ] AAS role with permissions to assign Roles to Users but cannot create delete or modify Users or Roles. < SHVS:HostListManager:> Used by the SHVS admin to delete the hosts. < SQVS:QuoteVerifier: > Used by the KBS service user for quote verification","title":"Role Definitions"},{"location":"product-guides/SGX%20Infrastructure/4Authentication/#sgx-agent","text":"The SGX Agent communicates with SGX Caching Service (SCS) and SGX Host Verification Service (SHVS) directly. Authentication has been centralized with the new Authentication and Authorization Service.","title":"SGX Agent"},{"location":"product-guides/SGX%20Infrastructure/5SGX%20Features%20Provisioning/","text":"SGX Features Provisioning Host Registration Host Registration creates a host record with host information in the SGX Host Verification Service database when SGX Agent update SGX enablement information for the first time.","title":"SGX Features Provisioning"},{"location":"product-guides/SGX%20Infrastructure/5SGX%20Features%20Provisioning/#sgx-features-provisioning","text":"","title":"SGX Features Provisioning"},{"location":"product-guides/SGX%20Infrastructure/5SGX%20Features%20Provisioning/#host-registration","text":"Host Registration creates a host record with host information in the SGX Host Verification Service database when SGX Agent update SGX enablement information for the first time.","title":"Host Registration"},{"location":"product-guides/SGX%20Infrastructure/6Setup%20Task%20Flows%20for%20K8s%20Deployments/","text":"Setup Task Flows for K8s Deployments Setup tasks flows have been updated to have K8s native flow to be more agnostic to K8s workflows. Following would be the flow for setup task. The setup task options are detailed under Intel Security Libraries Configuration Settings for each service and agent User would create a new configMap object with the environment variables specific to the setup task. The Setup task variables would be documented in the Product Guide Users can provide variables for more than one setup task If variables involve BEARER_TOKEN then user need to delete the corresponding secret using kubectl delete secret <service_name>-secret -n isecl , update the new token inside secrets.txt file and re-create the secret using kubectl create secret generic <service_name>-secret --from-file=secrets.txt --namespace=isecl , else update the variables in configMap.yml Users need to add SETUP_TASK: \"<setup task name>/<comma separated setup task name>\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f -","title":"Setup Task Flows for K8s Deployments"},{"location":"product-guides/SGX%20Infrastructure/6Setup%20Task%20Flows%20for%20K8s%20Deployments/#setup-task-flows-for-k8s-deployments","text":"Setup tasks flows have been updated to have K8s native flow to be more agnostic to K8s workflows. Following would be the flow for setup task. The setup task options are detailed under Intel Security Libraries Configuration Settings for each service and agent User would create a new configMap object with the environment variables specific to the setup task. The Setup task variables would be documented in the Product Guide Users can provide variables for more than one setup task If variables involve BEARER_TOKEN then user need to delete the corresponding secret using kubectl delete secret <service_name>-secret -n isecl , update the new token inside secrets.txt file and re-create the secret using kubectl create secret generic <service_name>-secret --from-file=secrets.txt --namespace=isecl , else update the variables in configMap.yml Users need to add SETUP_TASK: \"<setup task name>/<comma separated setup task name>\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f -","title":"Setup Task Flows for K8s Deployments"},{"location":"product-guides/SGX%20Infrastructure/7Configuration%20Update%20Flows%20for%20K8s%20Deployments/","text":"Configuration Update Flows for K8s Deployments Configuration Update flows have been updated to have K8s native flow to be more agnostic to K8s workflows using configMap only. Following would be the flow for configuration update. The config task options are are detailed under Intel Security Libraries Configuration Settings for each service and agent User would create a new configMap object using existing one and update the new values. The list of config variables would be documented in the Product Guide Users can update variables for more than one Users need to add SETUP_TASK: \"update-service-config\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f - Note Incase of agents, setup tasks or configuration updates done through above flows will be applied for all the agents running on different BMs. In order to run setup task or update configuration for individual agents, then user need to perform kubectl exec -it <pod_name> /bin/bash into a particular agent pod and run the specific setup task.","title":"Configuration Update Flows for K8s Deployments"},{"location":"product-guides/SGX%20Infrastructure/7Configuration%20Update%20Flows%20for%20K8s%20Deployments/#configuration-update-flows-for-k8s-deployments","text":"Configuration Update flows have been updated to have K8s native flow to be more agnostic to K8s workflows using configMap only. Following would be the flow for configuration update. The config task options are are detailed under Intel Security Libraries Configuration Settings for each service and agent User would create a new configMap object using existing one and update the new values. The list of config variables would be documented in the Product Guide Users can update variables for more than one Users need to add SETUP_TASK: \"update-service-config\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f - Note Incase of agents, setup tasks or configuration updates done through above flows will be applied for all the agents running on different BMs. In order to run setup task or update configuration for individual agents, then user need to perform kubectl exec -it <pod_name> /bin/bash into a particular agent pod and run the specific setup task.","title":"Configuration Update Flows for K8s Deployments"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/","text":"Intel Security Libraries Configuration Settings Note All the answer file options would remain common for containerized K8s deployments with the except of URLS where Kubernetes DNS would be used. The respective configMap.yml for each service and agent would carry the defaults for the same when built under <working directory>/k8s/manifests/<service/agent/db names> SGX Host Verification Service Installation Answer File Options Key Sample Value Description CMS_BASE_URL https://< IP address or hostname for CMS >:8445/cms/v1/ Base URL of the CMS AAS_API_URL https://< IP address or hostname for AAS >:8444/aas/v1 Base URL of the AAS SCS_BASE_URL https://< IP or hostname of SCS >:9000/scs/sgx/ Base URL of SCS SHVS_DB_PORT 5432 Defines the port number for communication with the database server. By default, with a local database server installation, this port will be set to 5432. SHVS_DB_NAME pgshvsdb Defines the schema name of the database. If a remote database connection will be used, this schema must be created in the remote database before installing the SGX Host Verification Service SHVS_DB_USERNAME aasdbuser Username for accessing the database. If a remote database connection will be used, this user/password must be created and granted all permissions for the database schema before installing the SGX Host Verification Service. SHVS_DB_PASSWORD aasdbpassword Password for accessing the database. If a remote database connection will be used, this user/password must be created and granted all permissions for the database schema before installing the SGX Host Verification Service. SHVS_DB_HOSTNAME localhost Defines the database server IP address or hostname. This should be the loopback address for local database server installations but should be the IP address or hostname of the database server if a remote database will be used. SAN_LIST 127.0.0.1,localhost Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service SHVS_ADMIN_USERNAME shvsuser@shvs Username for a new user to be created during installation. SHVS_ADMIN_PASSWORD shvspassword Password for the user to be created during installation. CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest> SHA384 hash of the CMS TLS certificate BEARER_TOKEN Installation token from AAS SHVS_PORT 13000 SGX Host Verification Service HTTP Port SHVS_SCHEDULER_TIMER 10 SHVS Scheduler timeout SHVS_HOST_PLATFORM_EXPIRY_TIME 240 SHVS Host Info Expiry time SHVS_AUTO_REFRESH_TIMER 120 SHVS Auto-refresh timeout Configuration Options The SGX Host Verification Service configuration is in path /etc/shvs/config.yml. Command-Line Options The SGX Host Verification Service supports several command-line options that can be executed only as the Root user: Syntax: shvs \\<command\\> Available Commands Help shvs help Displays the list of available CLI commands. Start shvs start Starts the SGX Host Verification service Stop shvs stop Stops the SGX Host Verification service Status shvs status Reports whether the service is currently running. Uninstall shvs uninstall \\[\\--purge\\] Removes the service. Use --purge option to remove configuration directory(/etc/shvs/) Version shvs version Shows the version of the service. Setup [task] Runs a specific setup task. Syntax: shvs setup [task] Setup tasks and its Configuration Options for SGX Host Verification Service Available Tasks for setup: all Runs all setup tasks Required env variables: - get required env variables from all the setup tasks Optional env variables: - get optional env variables from all the setup tasks shvs setup database - Available arguments are: SHVS_DB_HOSTNAME SHVS_DB_PORT SHVS_DB_USERNAME SHVS_DB_PASSWORD SHVS_DB_NAME SHVS_DB_SSLMODE <disable | allow | prefer | require | verify-ca | verify-full> SHVS_DB_SSLCERT path to where the certificate file of database. Only applicable for db-sslmode = <verify-ca | verify-full. If left empty, the cert will be copied to /etc/shvs/shvs-dbcert.pem alternatively, set environment variable - SHVS_DB_SSLCERTSRC <path to where the database ssl/tls certificate file> mandatory if db-sslcert does not already exist alternatively, set environment variable update_service_config Updates Service Configuration Required env variables: - SHVS_PORT : SGX Host Verification Service port - SHVS_SERVER_READ_TIMEOUT : SGX Host Verification Service Read Timeout - SHVS_SERVER_READ_HEADER_TIMEOUT : SGX Host Verification Service Read Header Timeout Duration - SHVS_SERVER_WRITE_TIMEOUT : SGX Host Verification Service Request Write Timeout Duration - SHVS_SERVER_IDLE_TIMEOUT : SGX Host Verification Service Request Idle Timeout - SHVS_SERVER_MAX_HEADER_BYTES : SGX Host Verification Service Max Length Of Request Header Bytes - SHVS_LOG_LEVEL : SGX Host Verification Service Log Level - SHVS_LOG_MAX_LENGTH : SGX Host Verification Service Log maximum length - SHVS_ENABLE_CONSOLE_LOG : SGX Host Verification Service Enable standard output - SHVS_ADMIN_USERNAME : SHVS Service Username - SHVS_ADMIN_PASSWORD : SHVS Service Password - SHVS_SCHEDULER_TIMER : SHVS Scheduler Timeout Seconds - SHVS_AUTO_REFRESH_TIMER : SHVS autoRefresh Timeout Seconds - SHVS_HOST_PLATFORM_EXPIRY_TIME : SHVS Host Platform Expiry Time in seconds - SCS_BASE_URL : SGX Caching Service URL - AAS_API_URL : AAS API URL download_ca_cert Download CMS root CA certificate Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that SHVS is talking to the right CMS instance download_cert TLS Generates Key pair and CSR, gets it signed from CMS Required env variable if SHVS_NOSETUP = true or variable not set in config.yml: - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that SHVS is talking to the right CMS instance Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - BEARER_TOKEN = <token> : for authenticating with CMS - SAN_LIST = <san> : list of hosts which needs access to service Optional env variables specific to setup task are: - KEY_PATH = <key_path> : Path of file where TLS key needs to be stored - CERT_PATH = <cert_path> : Path of file/directory where TLS certificate needs to be stored Directory Layout The SGX Host Verification Service installs by default to /opt/shvs with the following folders. Bin This folder contains executable scripts. Configuration This folder /etc/shvs contains certificates, keys, and configuration files. Logs This folder contains log files: /var/log/shvs/ SGX Agent Installation Answer File Options Key Sample Value Description SCS_BASE_URL https://< AAS IP or Hostname>:9000/scs/sgx/ The url used during setup to request information from SCS. CMS_BASE_URL https://< CMS IP or hostname>:8445/cms/v1/ API URL for Certificate Management Service (CMS). SHVS_BASE_URL https://< SHVS IP or hostname>:13000/sgx-hvs/v2/ The url used during setup to request information from SHVS. BEARER_TOKEN Long Lived JWT from AAS that contains \"install\" permissions needed to access ISecL services during provisioning and registration CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest> SHA384 Hash for verifying the CMS TLS certificate. SHVS_UPDATE_INTERVAL 120 Interval for SHVS updates in minutes. Values should be in the range of 1 minutes to 120 minutues. SGX_AGENT_NOSETUP false Skips setup during installation if set to true Configuration Options The SGX Agent configuration is in path /etc/sgx_agent/config.yml. Command-Line Options The SGX Agent supports several command-line options that can be executed only as the Root user: Syntax: sgx_agent \\<command\\> Available Commands Help Show the help message. Start sgx_agent start Start the SGX Agent service. Stop sgx_agent stop Stop the SGX Agent service. Status sgx_agent status Get the status of the SGX Agent Service. Uninstall sgx_agent uninstall \\--purge Removes the service. Use --purge option to remove configuration directory(/etc/sgx_agent/) Version sgx_agent version Reports the version of the service. Setup [task] Runs a specific setup task. Syntax: sgx_agent setup [task] Setup Tasks and its Configuration Options for SGX Agent Available Tasks for setup: all Runs all setup tasks Required env variables: - get required env variables from all the setup tasks Optional env variables: - get optional env variables from all the setup tasks update_service_config Updates Service Configuration Required env variables: - SCS_BASE_URL : SCS Base URL - SGX_AGENT_LOGLEVEL : SGX_AGENT Log Level - SGX_AGENT_LOG_MAX_LENGTH : SGX Agent Log maximum length - SGX_AGENT_ENABLE_CONSOLE_LOG : SGX Agent Enable standard output - SHVS_UPDATE_INTERVAL : SHVS update interval in minutes - WAIT_TIME : Time between each retries to PCS - RETRY_COUNT : Push Data Retry Count to SCS - SHVS_BASE_URL : HVS Base URL - BEARER_TOKEN : BEARER TOKEN download_ca_cert Download CMS root CA certificate Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that SGX-Agent is talking to the right CMS instance Directory Layout Linux The Linux SGX Agent installs by default to /opt/sgx_agent, with the following subfolders: Bin Contains executables and scripts. Configuration Contains the config.yml configuration file. Logs This folder contains log files: /var/log/sgx_agent Integration Hub Installation Answer File Options Key sample Value Description AAS_API_URL https://< Authentication and Authorization Service IP or Hostname>:8444/aas/v1 Base URL for the AAS CMS_BASE_URL https://< Certificate Management Service IP or Hostname>:8445/cms/v1 Base URL for the CMS SHVS_BASE_URL https://< SGX Host Verification Service IP or hostname>:13000/sgx-hvs/v2/ Base URL of SHVS IHUB_SERVICE_USERNAME ihubuser@ihub Database username IHUB_SERVICE_PASSWORD ihubpassword Database password CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest> SHA384 digest of the CMS TLS certificate BEARER_TOKEN Installation token TENANT KUBERNETES Tenant Orchaestrator KUBERNETES_URL https://< Kubernetes Master Node IP or Hostname> :6443 Kubernetes Master node URL KUBERNETES_CRD custom-isecl-sgx CRD Name to be used TLS_SAN_LIST 127.0.0.1, localhost Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service. KUBERNETES_TOKEN Token from Kubernetes Master Node KUBERNETES_CERT_FILE /root/apiserver.crt Kubernetes server certificate path POLL_INTERVAL_MINUTES 2 IHUB Polling Interval in Minutes INSTANCE_NAME ihub IHUB default instance name Configuration Options The Integration Hub configuration can be found in /etc/ihub/config.yml. Command-Line Options The Integrtion HUB supports several command-line options that can be executed only as the Root user: Syntax: ihub \\<command\\> Available Commands Help ihub help Displays the list of available CLI commands Start ihub start Start the service Stop ihub stop stops the service Status ihub status Reports whether the service is currently running. Uninstall ihub uninstall \\[\\--purge\\] \\[\\--exec\\] Removes the service. Use --purge option to remove configuration directory(/etc/ihub/). Use --exec option to remove ihub instance specific directories Version ihub version Reports the version of the service. Setup [task] Runs a specific setup task. Syntax: ihub setup [task] Setup Tasks and its Configuration Options for Integration Hub Available Tasks for setup: all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls attestation-service-connection Establish Attestation service connection tenant-service-connection Establish Tenant service connection create-signing-key Create signing key for IHUB update-service-config Sets or Updates the Service configuration Following environment variables are required for \"download-ca-cert\" CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ CMS_TLS_CERT_SHA384 SHA384 hash value of CMS TLS certificate Following environment variables are required in \"download-cert-tls\" CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ BEARER_TOKEN Bearer token for accessing CMS api Following environment variables are optionally used in download-cert-tls TLS_CERT_FILE The file to which certificate is saved TLS_KEY_FILE The file to which private key is saved TLS_COMMON_NAME The common name of signed certificate TLS_SAN_LIST Comma separated list of hostnames to add to Certificate, including IP addresses and DNS names Following environment variables are required for 'attestation-service-connection' setup: SHVS_BASE_URL Base URL for the SGX Host Verification Service Following environment variables are required for 'tenant-service-connection' setup: TENANT Type of Tenant Service ( OpenStack or Kubernetes ) Following environment variables are required for Kubernetes tenant: KUBERNETES_TOKEN Token for Kubernetes deployment KUBERNETES_CERT_FILE Certificate path for Kubernetes deployment KUBERNETES_URL URL for Kubernetes deployment KUBERNETES_CRD CRD Name for Kubernetes deployment Following environment variables are required for OpenStack tenant: OPENSTACK_PLACEMENT_URL Placement API endpoint for OpenStack deployment OPENSTACK_USERNAME UserName for OpenStack deployment OPENSTACK_PASSWORD Password for OpenStack deployment OPENSTACK_AUTH_URL Keystone API endpoint for OpenStack deployment Following environment variables are required for update-service-config setup: LOG_LEVEL Log level LOG_MAX_LENGTH Max length of log statement LOG_ENABLE_STDOUT Enable console log AAS_BASE_URL AAS Base URL SERVICE_USERNAME The service username as configured in AAS SERVICE_PASSWORD The service password as configured in AAS Directory Layout The Integration HUB installs by default to /opt/ihub with the following folders. Bin This folder contains executable scripts. Configuration This folder /etc/ihub/ contains certificates, keys, and configuration files. Logs This folder contains log files: /var/log/ihub/ Certificate Management Service Installation Answer File Options Key Sample Value Description CMS_PORT 8445 Default Port where Certificate Management Service Runs CMS_NOSETUP false Determines whether \u201csetup\u201d will be executed after installation. Typically this is set to \u201cfalse\u201d to install and perform setup in one action. The \u201ctrue\u201d option is intended for building the service as a container, where the installation would be part of the image build, and setup would be performed when the container starts for the first time to generate any persistent data. AAS_API_URL https://< AAS Hostname or IP address>:8444/aas/v1 URL to connect to the AAS, used during setup for authentication. AAS_TLS_SAN < Comma-separated list of IPs/hostnames for the AAS> SAN list populated in special JWT token; this token is used by AAS to get TLS certificate signed from CMS. SAN list in this token and CSR generated by AAS must match. Configuration Options The CMS configuration can be found in /etc/cms/config.yml. Command-Line Options The Certificate Management Service supports several command-line options that can be executed only as the Root user: Syntax: cms \\<command\\> Available Commands Help cms help Displays the list of available CLI commands. Start cms start Starts the services. Stop cms stop Stops the service. Status cms status Reports whether the service is currently running. Uninstall cms uninstall \\[\\--purge\\] Uninstalls the service, including the deletion of all files and folders. Version cms version Reports the version of the service. Tlscertsha384 cms tlscertsha384 Shows the SHA384 digest of the TLS certificate. Setup [task] Runs a specific setup task. Syntax: cms setup [task] Available Tasks for setup: cms setup server [--port=\\<port>] Setup http server on \\<port> Environment variable CMS_PORT=\\<port> can be set alternatively cms setup root_ca [--force] Create its own self signed Root CA keypair in /etc/cms for quality of life Option [--force] overwrites any existing files, and always generate new Root CA keypair cms setup tls [--force] [--host_names=\\<host_names>] Create its own root_ca signed TLS keypair in /etc/cms for quality of life Option [--force] overwrites any existing files, and always generate root_ca signed TLS keypair Argument \\<host_names> is a list of host names used by local machine, seperated by comma Environment variable CMS_HOST_NAMES=\\<host_names> can be set alternatively cms setup cms-auth-token [--force] Create its own self signed JWT keypair in /etc/cms/jwt for quality of life Option [--force] overwrites any existing files, and always generate new JWT keypair and token Setup Tasks and its Configuration Options for Certificate Management Service Available Tasks for setup: all Runs all setup tasks root-ca Creates a self signed Root CA key pair in /etc/cms/root-ca/ for quality of life intermediate-ca Creates a Root CA signed intermediate CA key pair ( signing, tls-server and tls-client ) in /etc/cms/intermediate-ca/ for quality of life tls Creates an intermediate-ca signed TLS key pair in /etc/cms for quality of life cms-auth-token Create its own self signed JWT key pair in /etc/cms/jwt for quality of life update-service-config Sets or Updates the Service configuration Following environment variables are required for 'tls' setup: SAN_LIST TLS SAN list Following environment variables are required for 'authToken' setup: AAS_JWT_CN Common Name for JWT Signing Certificate used in Authentication and Authorization Service AAS_TLS_CN Common Name for TLS Signing Certificate used in Authentication and Authorization Service AAS_TLS_SAN TLS SAN list for Authentication and Authorization Service Following environment variables are required for 'update-service-config' setup: AAS_BASE_URL AAS Base URL SERVER_PORT The Port on which Server Listens to SERVER_READ_TIMEOUT Request Read Timeout Duration in Seconds SERVER_READ_HEADER_TIMEOUT Request Read Header Timeout Duration in Seconds SERVER_IDLE_TIMEOUT Request Idle Timeout in Seconds LOG_LEVEL Log level LOG_MAX_LENGTH Max length of log statement SERVER_WRITE_TIMEOUT Request Write Timeout Duration in Seconds SERVER_MAX_HEADER_BYTES Max Length Of Request Header in Bytes LOG_ENABLE_STDOUT Enable console log TOKEN_DURATION_MINS Validity of token duration Following environment variables are required for 'root-ca' setup: CMS_CA_PROVINCE CA Certificate Province CMS_CA_COUNTRY CA Certificate Country CMS_CA_CERT_VALIDITY CA Certificate Validity CMS_CA_ORGANIZATION CA Certificate Organization CMS_CA_LOCALITY CA Certificate Locality Following environment variables are required for 'intermediate-ca' setup: CMS_CA_PROVINCE CA Certificate Province CMS_CA_COUNTRY CA Certificate Country CMS_CA_CERT_VALIDITY CA Certificate Validity CMS_CA_ORGANIZATION CA Certificate Organization CMS_CA_LOCALITY CA Certificate Locality Directory Layout The Certificate Management Service installs by default to /opt/cms with the following folders. Bin This folder contains executable scripts. Configuration This folder /etc/cms contains certificates, keys, and configuration files. Logs This folder contains log files: /var/log/cms/ Cacerts This folder contains the CMS root CA certificate. Authentication and Authorization Service Installation Answer File Options Key Sample Value Description CMS_BASE_URL https://< cms IP or hostname>/cms/v1/ Provides the URL for the CMS. AAS_NOSETUP false Determines whether \u201csetup\u201d will be executed after installation. Typically this is set to \u201cfalse\u201d to install and perform setup in one action. The \u201ctrue\u201d option is intended for building the service as a container, where the installation would be part of the image build, and setup would be performed when the container starts for the first time to generate any persistent data. AAS_DB_HOSTNAME localhost Hostname or IP address of the AAS database AAS_DB_PORT 5432 Database port number AAS_DB_NAME pgdb Database name AAS_DB_USERNAME aasdbuser Database username AAS_DB_PASSWORD aasdbpassd Database password AAS_DB_SSLMODE verify-full AAS_DB_SSLCERTSRC /usr/local/pgsql/data/server.crt Required if the \u201cAAS_DB_SSLMODE\u201d is set to \u201cverify-ca.\u201d Defines the location of the database SSL certificate. AAS_DB_SSLCERT < path_to_cert_file_on_system > The AAS_DB_SSLCERTSRC variable defines the source location of the database SSL certificate; this variable determines the local location. If the former option is used without specifying this option, the service will copy the SSL certificate to the default configuration directory. AAS_ADMIN_USERNAME admin@aas Defines a new AAS administrative user. This user will be able to create new users, new roles, and new role-user mappings. This user will have the AAS:Administrator role. AAS_ADMIN_PASSWORD aasAdminPass Password for the new AAS admin user AAS_JWT_CERT_SUBJECT \"AAS JWT Signing Certificate\" Defines the subject of the JWT signing certificate. AAS_JWT_TOKEN_DURATION 5 Defines the amount of time in minutes that an issued token will be valid. SAN_LIST 127.0.0.1,localhost Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service. BEARER_TOKEN Installation Token from AAS. Configuration Options The AAS configuration can be found in /etc/authservice/config.yml. Command-Line Options The AAS supports several command-line options that can be executed only as the Root user: Syntax: authservice \\<command\\> Available Commands Help authservice help Displays the list of available CLI commands. Start authservice start Starts the service. Stop authservice stop Stops the service. Status authservice status Displays the current status of the service. Uninstall authservice uninstall \\[\\--purge\\] Removes the service. Use the \"--purge\" flag to also delete all data. Version authservice version Shows the version of the service. Setup [task] Executes a specific setup task. Can be used to change the current configuration. Syntax: authservice setup [task] Available Tasks for setup: authservice setup all Runs all setup tasks authservice setup database [-force] [-arguments=\\<argument_value>] Available arguments are: db-host alternatively, set environment variable AAS_DB_HOSTNAME db-port alternatively, set environment variable AAS_DB_PORT db-user alternatively, set environment variable AAS_DB_USERNAME db-pass alternatively, set environment variable AAS_DB_PASSWORD db-name alternatively, set environment variable AAS_DB_NAME db-sslmode \\<disable|allow|prefer|require|verify-ca|verify-full> alternatively, set environment variable AAS_DB_SSLMODE db-sslcert path to where the certificate file of database. Only applicable for db-sslmode=\\<verify-ca|verify-full. If left empty, the cert will be copied to /etc/authservice/tdcertdb.pem alternatively, set environment variable AAS_DB_SSLCERT db-sslcertsrc \\<path to where the database ssl/tls certificate file> mandatory if db-sslcert does not already exist alternatively, set environment variable AAS_DB_SSLCERTSRC Run this command with environment variable AAS_DB_REPORT_MAX_ROWS and AAS_DB_REPORT_NUM_ROTATIONS can update db rotation arguments authservice setup server [--port=\\<port>] Setup http server on \\<port> Environment variable AAS_PORT=\\<port> can be set alternatively authservice setup tls [--force] [--host_names=\\<host_names>] Use the key and certificate provided in /etc/threat-detection if files exist Otherwise create its own self-signed TLS keypair in /etc/authservice for quality of life Option [--force] overwrites any existing files, and always generate self-signed keypair Argument \\<host_names> is a list of host names used by local machine, seperated by comma Environment variable AAS_TLS_HOST_NAMES=\\<host_names> can be set alternatively authservice setup admin [--user=\\<username>] [-pass=\\<password>] Environment variable AAS_ADMIN_USERNAME=\\<username> can be set alternatively Environment variable AAS_ADMIN_PASSWORD=\\<password> can be set alternatively authservice setup jwt Create jwt signing key and jwt certificate signed by CMS Environment variable CMS_BASE_URL=\\<url> for CMS API url Environment variable AAS_JWT_CERT_CN=\\<CERTIFICATE SUBJECT> AAS JWT Certificate Subject Environment variable AAS_JWT_INCLUDE_KEYID=\\<KEY ID> AAS include key id in JWT Token Environment variable AAS_JWT_TOKEN_DURATION_MINS=\\<DURATION> JWT Token validation minutes Environment variable BEARER_TOKEN=\\<token> for authenticating with CMS Setup Tasks and its Configuration Options for Authentication and Authorization Service Available Tasks for setup: all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls database Setup authservice database admin Add authservice admin username and password to database and assign respective roles to the user jwt Create jwt signing key and jwt certificate signed by CMS update-service-config Sets or Updates the Service configuration Following environment variables are required for 'download-ca-cert' CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ CMS_TLS_CERT_SHA384 SHA384 hash value of CMS TLS certificate Following environment variables are required in 'download-cert-tls' CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ BEARER_TOKEN Bearer token for accessing CMS api Following environment variables are optionally used in download-cert-tls TLS_CERT_FILE The file to which certificate is saved TLS_KEY_FILE The file to which private key is saved TLS_COMMON_NAME The common name of signed certificate TLS_SAN_LIST Comma separated list of hostnames to add to Certificate, including IP addresses and DNS names Following environment variables are required for 'Database' related setups: DB_VENDOR Vendor of database, or use AAS_DB_VENDOR alternatively DB_HOST Database host name, or use AAS_DB_HOSTNAME alternatively DB_USERNAME Database username, or use AAS_DB_USERNAME alternatively DB_PASSWORD Database password, or use AAS_DB_PASSWORD alternatively DB_SSL_MODE Database SSL mode, or use AAS_DB_SSL_MODE alternatively DB_SSL_CERT Database SSL certificate, or use AAS_DB_SSLCERT alternatively DB_PORT Database port, or use AAS_DB_PORT alternatively DB_NAME Database name, or use AAS_DB_NAME alternatively DB_SSL_CERT_SOURCE Database SSL certificate to be copied from, or use AAS_DB_SSLCERTSRC alternatively DB_CONN_RETRY_ATTEMPTS Database connection retry attempts DB_CONN_RETRY_TIME Database connection retry time Following environment variables are required for 'admin' setup: AAS_ADMIN_USERNAME Authentication and Authorization Service Admin Username AAS_ADMIN_PASSWORD Authentication and Authorization Service Admin Password Following environment variables are required in 'jwt' CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ BEARER_TOKEN Bearer token for accessing CMS api Following environment variables are optionally used in jwt CERT_FILE The file to which certificate is saved KEY_FILE The file to which private key is saved COMMON_NAME The common name of signed certificate Following environment variables are required for 'update-service-config' setup: AUTH_DEFENDER_LOCKOUT_DURATION_MINS Auth defender lockout duration in minutes SERVER_MAX_HEADER_BYTES Max Length Of Request Header in Bytes JWT_INCLUDE_KID Includes JWT Key Id for token validation SERVER_READ_HEADER_TIMEOUT Request Read Header Timeout Duration in Seconds AUTH_DEFENDER_MAX_ATTEMPTS Auth defender maximum attempts SERVER_PORT The Port on which Server Listens to SERVER_READ_TIMEOUT Request Read Timeout Duration in Seconds LOG_MAX_LENGTH Max length of log statement LOG_ENABLE_STDOUT Enable console log JWT_CERT_COMMON_NAME Common Name for JWT Certificate SERVER_WRITE_TIMEOUT Request Write Timeout Duration in Seconds SERVER_IDLE_TIMEOUT Request Idle Timeout in Seconds LOG_LEVEL Log level JWT_TOKEN_DURATION_MINS Validity of token duration AUTH_DEFENDER_INTERVAL_MINS Auth defender interval in minutes Directory Layout The Authentication and Authorization Service installs by default to /opt/authservice with the following folders. Bin Contains executable scripts and binaries. Configuration This folder /etc/authservice contains certificates, keys, and configuration files. Logs This folder contains log files: /var/log/authservice Dbscripts This folder /opt/authservice/dbscripts Contains database scripts Key Broker Service Installation Answer File Options Variable Name Default Value Notes CMS_BASE_URL https://< CMS IP or hostname >:8445/cms/v1/ Required for generating TLS certificate AAS_API_URL https://< AAS IP or hostname >:8444/aas/v1 AAS service url SQVS_URL https://< SQVS IP or hostname >:12000/svs/v1/ Required to get the SGX Quote verified CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest > SHA384 digest of CMS TLS certificate BEARER_TOKEN JWT token for installation user KBS_SERVICE_USERNAME admin@kms KBS Service Username KBS_SERVICE_PASSWORD kmsAdminPass KBS Service User Password ENDPOINT_URL https://< KBS Hostname >:9443/v1 KBS Endpoint URL TLS_COMMON_NAME KBS TLS Certificate KBS TLS Certificate common-name SERVER_PORT 9443 KBS Secure Port SKC_CHALLENGE_TYPE SGX Challenge Type TLS_SAN_LIST < KBS IP/Hostname > IP addresses/hostnames to be included in SAN list. KEY_MANAGER KMIP Key Manager Backend to store keys Configuration Options The Key Broker Service configuration is in path /etc/kbs/config.yml. Command-Line Options The Key Broker Service supports several command-line options that can be executed only as the Root user: Syntax: kbs \\<command\\> Available Commands Help kbs help Displays the list of available CLI commands. Start kbs start Starts the service Stop kbs stop Stops the service Status kbs status Displays the current status of the service. Uninstall kbs uninstall \\[\\--purge\\] Removes the service Version kbs version Displays the version of the service Setup [task] Runs a specific setup task. Syntax: kbs setup [task] Setup Tasks and its Configuration Options for Key Broker Service Available Tasks for setup: all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls create-default-key-transfer-policy Create default key transfer policy for KBS update-service-config Sets or Updates the Service configuration Following environment variables are required for 'update-service-config' setup: SERVICE_USERNAME The service username as configured in AAS AAS_BASE_URL AAS Base URL KMIP_ROOT_CERT_PATH KMIP Root Certificate path KMIP_SERVER_IP IP of KMIP server KMIP_CLIENT_KEY_PATH KMIP Client key path SERVER_READ_TIMEOUT Request Read Timeout Duration in Seconds SERVER_READ_HEADER_TIMEOUT Request Read Header Timeout Duration in Seconds SERVER_IDLE_TIMEOUT Request Idle Timeout in Seconds SQVS_URL SQVS URL SESSION_EXPIRY_TIME Session Expiry Time SERVER_PORT The Port on which Server Listens to LOG_LEVEL Log level LOG_MAX_LENGTH Max length of log statement LOG_ENABLE_STDOUT Enable console log KMIP_SERVER_PORT PORT of KMIP server KMIP_CLIENT_CERT_PATH KMIP Client certificate path SERVER_WRITE_TIMEOUT Request Write Timeout Duration in Seconds SERVER_MAX_HEADER_BYTES Max Length Of Request Header in Bytes SERVICE_PASSWORD The service password as configured in AAS SKC_CHALLENGE_TYPE SKC challenge type Following environment variables are required for 'download-ca-cert' CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ CMS_TLS_CERT_SHA384 SHA384 hash value of CMS TLS certificate Following environment variables are required in 'download-cert-tls' CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ BEARER_TOKEN Bearer token for accessing CMS api Following environment variables are optionally used in download-cert-tls TLS_KEY_FILE The file to which private key is saved TLS_COMMON_NAME The common name of signed certificate TLS_CERT_FILE The file to which certificate is saved TLS_SAN_LIST Comma separated list of hostnames to add to Certificate, including IP addresses and DNS names Directory Layout The Key Broker Service installs by default to /opt/kbs with the following folders. Bin Contains executable scripts and binaries. Configuration This folder /etc/kbs contains certificates, keys, and configuration files. Logs This folder contains log files: /var/log/kbs SGX Caching Service Installation Answer File Options Key Sample Value Description CMS_BASE_URL https://< CMS IP or hostname >:8445/cms/v1/ CMS URL for Certificate Management Service AAS_API_URL https://< AAS IP or hostname >:8444/aas/v1 API URL for Authentication Authorization Service SCS_ADMIN_USERNAME scsuser@scs SCS Service username SCS_ADMIN_PASSWORD scspassword SCS Service password BEARER_TOKEN Installation Token from AAS CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest > SHA384 Hash sum for verifying the CMS TLS certificate. INTEL_PROVISIONING_SERVER https://sbx.api.trustedservices.intel.com/sgx/certification/v3 Intel pcs server url INTEL_PROVISIONING_SERVER_API_KEY < Add your API subscription key > Intel PCS Server API subscription key SCS_REFRESH_HOURS 1 hour Time after which the SGX collaterals in SCS db get refreshed from Intel PCS server RETRY_COUNT 3 Number Of times to connect to PCS if PCS service is not accessible WAIT_TIME 1 Number Of Seconds between retries to connect to PCS SCS_DB_HOSTNAME localhost SCS Databse hostname SCS_DB_PORT 5432 SCS Database port SCS_DB_NAME pgscsdb SCS Database name SCS_DB_USERNAME aasdbuser SCS Database username SCS_DB_PASSWORD aasdbpassword SCS Database password SCS_DB_SSLCERTSRC /usr/local/pgsql/data/server.crt SAN_LIST 127.0.0.1,localhost Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service. Configuration Options The SGX Caching Service configuration can be found in /etc/scs/config.yml. Command-Line Options The SGX Caching Service supports several command-line options that can be executed only as the Root user: Syntax: scs \\<command\\> Available Commands Help scs help Displays the list of available CLI commands. Start scs start Starts the SGX Caching Service Stop scs stop Stops the SGX Caching Service Status scs status Reports whether the SGX Caching Service is currently running Uninstall scs uninstall \\[\\--purge\\] uninstall the SGX Caching Service. --purge option needs to be applied to remove configuration files Version scs version Reports the version of the scs Setup [task] Runs a specific setup task. Syntax: scs setup [task] Setup Tasks and its Configuration Options for SGX Caching Service Avaliable Tasks for setup: all Runs all setup tasks Required env variables: - get required env variables from all the setup tasks Optional env variables: - get optional env variables from all the setup tasks scs setup database - Avaliable arguments are: - SCS_DB_HOSTNAME - SCS_DB_PORT - SCS_DB_USERNAME - SCS_DB_PASSWORD - SCS_DB_NAME - SCS_DB_SSLMODE <disable | allow | prefer | require | verify-ca | verify-full> - SCS_DB_SSLCERT path to where the certificate file of database. Only applicable for db-sslmode = <verify-ca | verify-full. If left empty, the cert will be copied to /etc/scs/tdcertdb.pem - SCS_DB_SSLCERTSRC <path to where the database ssl/tls certificate file> mandatory if db-sslcert does not already exist update_service_config Updates Service Configuration Required env variables: - SCS_PORT : SGX Caching Service port - SCS_SERVER_READ_TIMEOUT : SGX Caching Service Read Timeout - SCS_SERVER_READ_HEADER_TIMEOUT : SGX Caching Service Read Header Timeout Duration - SCS_SERVER_WRITE_TIMEOUT : SGX Caching Service Request Write Timeout Duration - SCS_SERVER_IDLE_TIMEOUT : SGX Caching Service Request Idle Timeout - SCS_SERVER_MAX_HEADER_BYTES : SGX Caching Service Max Length Of Request Header Bytes - INTEL_PROVISIONING_SERVER : Intel ECDSA Provisioning Server URL - INTEL_PROVISIONING_SERVER_API_KEY : Intel ECDSA Provisioning Server API Subscription key - SCS_LOGLEVEL : SGX Caching Service Log Level - SCS_LOG_MAX_LENGTH : SGX Caching Service Log maximum length - SCS_ENABLE_CONSOLE_LOG : SGX Caching Service Enable standard output - SCS_REFRESH_HOURS : SCS Automatic Refresh of SGX Data - RETRY_COUNT : Number of retry to PCS server - WAIT_TIME : Duration Time between each retries to PCS - AAS_API_URL : AAS API URL download_ca_cert Download CMS root CA certificate Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that AAS is talking to the right CMS instance download_cert TLS Generates Key pair and CSR, gets it signed from CMS Required env variable if SCS_NOSETUP = true or variable not set in config.yml: - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that AAS is talking to the right CMS instance Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - BEARER_TOKEN = <token> : for authenticating with CMS - SAN_LIST = <san> : list of hosts which needs access to service Optional env variables specific to setup task are: - KEY_PATH = <key_path> : Path of file where TLS key needs to be stored - CERT_PATH = <cert_path> : Path of file/directory where TLS certificate needs to be stored Directory Layout The SGX Caching Service installs by default to /opt/scs with the following folders. Bin Contains SGX Caching Service executable binary. Configuration This folder /etc/scs contains certificates, keys, and configuration files. Logs This folder contains log files: /var/log/scs SGX Quote Verification Service Installation Answer File Options Key Sample Value Description CMS_BASE_URL https://< CMS IP address or hostname >:8445/cms/v1/ Defines the base URL for the CMS owned by the image owner. Note that this CMS may be different from the CMS used for other components. AAS_API_URL https://< AAS IP address or hostname >:8444/aas/v1 Defines the baseurl for the AAS owned by the image owner. Note that this AAS may be different from the AAS used for other components. SCS_BASE_URL https://< SCS IP address or hostname >:9000/scs/sgx/certification/v1/ The SCS url is needed. SGX_TRUSTED_ROOT_CA_PATH /tmp/trusted_rootca.pem The path to SGX root ca used to verify quote CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest > SHA384 hash of the CMS TLS certificate BEARER_TOKEN Token from CMS with permissions used for installation. SQVS_LOG_LEVEL INFO (default), DEBUG Defines the log level for the SQVS. Defaults to INFO. SQVS_PORT 12000 SQVS Secure Port SQVS_NOSETUP false Skips setup during installation if set to true SAN_LIST 127.0.0.1,localhost Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service. SQVS_INCLUDE_TOKEN true If true, SQVS will authenticate KBS before Quote Verifiation Configuration Options The SGX Quote Verification Service configuration can be found in /etc/sqvs/config.yml. Command-Line Options The SGX Quote Verifiction Service supports several command-line options that can be executed only as the Root user: Syntax: sqvs \\<command\\> Available Commands Help sqvs help Displays the list of available CLI commands. Start sqvs start Starts the SGX Quote Verification Service Stop sqvs stop Stops the SGX Quote Verification Service Status sqvs status Reports whether the SGX Quote Verification Service is currently running. Uninstall sqvs uninstall \\[\\--purge\\] uninstalls the SGX Quote Verification Service. --purge option needs to be applied to remove configuration files Version sqvs version Reports the version of the sqvs Setup [task] Runs a specific setup task. Syntax: sqvs setup [task] Setup Tasks and its Configuration Options for SGX Quote Verification Service Available Tasks for setup: Required env variables: - get required env variables from all the setup tasks Optional env variables: - get optional env variables from all the setup tasks update_service_config Updates Service Configuration Required env variables: - SQVS_PORT : SGX Verification Service port - SQVS_SERVER_READ_TIMEOUT : SGX Verification Service Read Timeout - SQVS_SERVER_READ_HEADER_TIMEOUT : SGX Verification Service Read Header Timeout Duration - SQVS_SERVER_WRITE_TIMEOUT : SGX Verification Service Request Write Timeout Duration - SQVS_SERVER_IDLE_TIMEOUT : SGX Verification Service Request Idle Timeout - SQVS_SERVER_MAX_HEADER_BYTES : SGX Verification Service Max Length Of Request Header Bytes - SQVS_LOGLEVEL : SGX Verification Service Log Level - SQVS_LOG_MAX_LENGTH : SGX Verification Service Log maximum length - SQVS_ENABLE_CONSOLE_LOG : SGX Verification Service Enable standard output - SQVS_INCLUDE_TOKEN : Boolean value to decide whether to use token based auth or no auth for quote verifier API - SGX_TRUSTED_ROOT_CA_PATH : SQVS Trusted Root CA - SCS_BASE_URL : SGX Caching Service URL - AAS_API_URL : AAS API URL download_ca_cert Download CMS root CA certificate Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that AAS is talking to the right CMS instance download_cert TLS Generates Key pair and CSR, gets it signed from CMS - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that AAS is talking to the right CMS instance Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - BEARER_TOKEN = <token> : for authenticating with CMS - SAN_LIST = <san> : list of hosts which needs access to service Optional env variables specific to setup task are: - KEY_PATH = <key_path> : Path of file where TLS key needs to be stored - CERT_PATH = <cert_path> : Path of file/directory where TLS certificate needs to be stored Directory Layout The SGX Quote Verification Service installs by default to /opt/sqvs with the following folders. Bin This folder contains executable scripts. Configuration This folder /etc/sqvs contains certificates, keys, and configuration files. Logs This folder contains log files: /var/log/sqvs","title":"Intel Security Libraries Configuration Settings"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#intel-security-libraries-configuration-settings","text":"Note All the answer file options would remain common for containerized K8s deployments with the except of URLS where Kubernetes DNS would be used. The respective configMap.yml for each service and agent would carry the defaults for the same when built under <working directory>/k8s/manifests/<service/agent/db names>","title":"Intel Security Libraries Configuration Settings"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#sgx-host-verification-service","text":"","title":"SGX Host Verification Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#installation-answer-file-options","text":"Key Sample Value Description CMS_BASE_URL https://< IP address or hostname for CMS >:8445/cms/v1/ Base URL of the CMS AAS_API_URL https://< IP address or hostname for AAS >:8444/aas/v1 Base URL of the AAS SCS_BASE_URL https://< IP or hostname of SCS >:9000/scs/sgx/ Base URL of SCS SHVS_DB_PORT 5432 Defines the port number for communication with the database server. By default, with a local database server installation, this port will be set to 5432. SHVS_DB_NAME pgshvsdb Defines the schema name of the database. If a remote database connection will be used, this schema must be created in the remote database before installing the SGX Host Verification Service SHVS_DB_USERNAME aasdbuser Username for accessing the database. If a remote database connection will be used, this user/password must be created and granted all permissions for the database schema before installing the SGX Host Verification Service. SHVS_DB_PASSWORD aasdbpassword Password for accessing the database. If a remote database connection will be used, this user/password must be created and granted all permissions for the database schema before installing the SGX Host Verification Service. SHVS_DB_HOSTNAME localhost Defines the database server IP address or hostname. This should be the loopback address for local database server installations but should be the IP address or hostname of the database server if a remote database will be used. SAN_LIST 127.0.0.1,localhost Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service SHVS_ADMIN_USERNAME shvsuser@shvs Username for a new user to be created during installation. SHVS_ADMIN_PASSWORD shvspassword Password for the user to be created during installation. CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest> SHA384 hash of the CMS TLS certificate BEARER_TOKEN Installation token from AAS SHVS_PORT 13000 SGX Host Verification Service HTTP Port SHVS_SCHEDULER_TIMER 10 SHVS Scheduler timeout SHVS_HOST_PLATFORM_EXPIRY_TIME 240 SHVS Host Info Expiry time SHVS_AUTO_REFRESH_TIMER 120 SHVS Auto-refresh timeout","title":"Installation Answer File Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration-options","text":"The SGX Host Verification Service configuration is in path /etc/shvs/config.yml.","title":"Configuration Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#command-line-options","text":"The SGX Host Verification Service supports several command-line options that can be executed only as the Root user: Syntax: shvs \\<command\\>","title":"Command-Line Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#available-commands","text":"","title":"Available Commands"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#help","text":"shvs help Displays the list of available CLI commands.","title":"Help"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#start","text":"shvs start Starts the SGX Host Verification service","title":"Start"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#stop","text":"shvs stop Stops the SGX Host Verification service","title":"Stop"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#status","text":"shvs status Reports whether the service is currently running.","title":"Status"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#uninstall","text":"shvs uninstall \\[\\--purge\\] Removes the service. Use --purge option to remove configuration directory(/etc/shvs/)","title":"Uninstall"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#version","text":"shvs version Shows the version of the service.","title":"Version"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-task","text":"Runs a specific setup task. Syntax: shvs setup [task]","title":"Setup [task]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-tasks-and-its-configuration-options-for-sgx-host-verification-service","text":"Available Tasks for setup: all Runs all setup tasks Required env variables: - get required env variables from all the setup tasks Optional env variables: - get optional env variables from all the setup tasks shvs setup database - Available arguments are: SHVS_DB_HOSTNAME SHVS_DB_PORT SHVS_DB_USERNAME SHVS_DB_PASSWORD SHVS_DB_NAME SHVS_DB_SSLMODE <disable | allow | prefer | require | verify-ca | verify-full> SHVS_DB_SSLCERT path to where the certificate file of database. Only applicable for db-sslmode = <verify-ca | verify-full. If left empty, the cert will be copied to /etc/shvs/shvs-dbcert.pem alternatively, set environment variable - SHVS_DB_SSLCERTSRC <path to where the database ssl/tls certificate file> mandatory if db-sslcert does not already exist alternatively, set environment variable update_service_config Updates Service Configuration Required env variables: - SHVS_PORT : SGX Host Verification Service port - SHVS_SERVER_READ_TIMEOUT : SGX Host Verification Service Read Timeout - SHVS_SERVER_READ_HEADER_TIMEOUT : SGX Host Verification Service Read Header Timeout Duration - SHVS_SERVER_WRITE_TIMEOUT : SGX Host Verification Service Request Write Timeout Duration - SHVS_SERVER_IDLE_TIMEOUT : SGX Host Verification Service Request Idle Timeout - SHVS_SERVER_MAX_HEADER_BYTES : SGX Host Verification Service Max Length Of Request Header Bytes - SHVS_LOG_LEVEL : SGX Host Verification Service Log Level - SHVS_LOG_MAX_LENGTH : SGX Host Verification Service Log maximum length - SHVS_ENABLE_CONSOLE_LOG : SGX Host Verification Service Enable standard output - SHVS_ADMIN_USERNAME : SHVS Service Username - SHVS_ADMIN_PASSWORD : SHVS Service Password - SHVS_SCHEDULER_TIMER : SHVS Scheduler Timeout Seconds - SHVS_AUTO_REFRESH_TIMER : SHVS autoRefresh Timeout Seconds - SHVS_HOST_PLATFORM_EXPIRY_TIME : SHVS Host Platform Expiry Time in seconds - SCS_BASE_URL : SGX Caching Service URL - AAS_API_URL : AAS API URL download_ca_cert Download CMS root CA certificate Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that SHVS is talking to the right CMS instance download_cert TLS Generates Key pair and CSR, gets it signed from CMS Required env variable if SHVS_NOSETUP = true or variable not set in config.yml: - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that SHVS is talking to the right CMS instance Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - BEARER_TOKEN = <token> : for authenticating with CMS - SAN_LIST = <san> : list of hosts which needs access to service Optional env variables specific to setup task are: - KEY_PATH = <key_path> : Path of file where TLS key needs to be stored - CERT_PATH = <cert_path> : Path of file/directory where TLS certificate needs to be stored","title":"Setup tasks and its Configuration Options for SGX Host Verification Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#directory-layout","text":"The SGX Host Verification Service installs by default to /opt/shvs with the following folders.","title":"Directory Layout"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#bin","text":"This folder contains executable scripts.","title":"Bin"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration","text":"This folder /etc/shvs contains certificates, keys, and configuration files.","title":"Configuration"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#logs","text":"This folder contains log files: /var/log/shvs/","title":"Logs"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#sgx-agent","text":"","title":"SGX Agent"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#installation-answer-file-options_1","text":"Key Sample Value Description SCS_BASE_URL https://< AAS IP or Hostname>:9000/scs/sgx/ The url used during setup to request information from SCS. CMS_BASE_URL https://< CMS IP or hostname>:8445/cms/v1/ API URL for Certificate Management Service (CMS). SHVS_BASE_URL https://< SHVS IP or hostname>:13000/sgx-hvs/v2/ The url used during setup to request information from SHVS. BEARER_TOKEN Long Lived JWT from AAS that contains \"install\" permissions needed to access ISecL services during provisioning and registration CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest> SHA384 Hash for verifying the CMS TLS certificate. SHVS_UPDATE_INTERVAL 120 Interval for SHVS updates in minutes. Values should be in the range of 1 minutes to 120 minutues. SGX_AGENT_NOSETUP false Skips setup during installation if set to true","title":"Installation Answer File Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration-options_1","text":"The SGX Agent configuration is in path /etc/sgx_agent/config.yml.","title":"Configuration Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#command-line-options_1","text":"The SGX Agent supports several command-line options that can be executed only as the Root user: Syntax: sgx_agent \\<command\\>","title":"Command-Line Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#available-commands_1","text":"","title":"Available Commands"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#help_1","text":"Show the help message.","title":"Help"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#start_1","text":"sgx_agent start Start the SGX Agent service.","title":"Start"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#stop_1","text":"sgx_agent stop Stop the SGX Agent service.","title":"Stop"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#status_1","text":"sgx_agent status Get the status of the SGX Agent Service.","title":"Status"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#uninstall_1","text":"sgx_agent uninstall \\--purge Removes the service. Use --purge option to remove configuration directory(/etc/sgx_agent/)","title":"Uninstall"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#version_1","text":"sgx_agent version Reports the version of the service.","title":"Version"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-task_1","text":"Runs a specific setup task. Syntax: sgx_agent setup [task]","title":"Setup [task]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-tasks-and-its-configuration-options-for-sgx-agent","text":"Available Tasks for setup: all Runs all setup tasks Required env variables: - get required env variables from all the setup tasks Optional env variables: - get optional env variables from all the setup tasks update_service_config Updates Service Configuration Required env variables: - SCS_BASE_URL : SCS Base URL - SGX_AGENT_LOGLEVEL : SGX_AGENT Log Level - SGX_AGENT_LOG_MAX_LENGTH : SGX Agent Log maximum length - SGX_AGENT_ENABLE_CONSOLE_LOG : SGX Agent Enable standard output - SHVS_UPDATE_INTERVAL : SHVS update interval in minutes - WAIT_TIME : Time between each retries to PCS - RETRY_COUNT : Push Data Retry Count to SCS - SHVS_BASE_URL : HVS Base URL - BEARER_TOKEN : BEARER TOKEN download_ca_cert Download CMS root CA certificate Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that SGX-Agent is talking to the right CMS instance","title":"Setup Tasks and its Configuration Options for SGX Agent"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#directory-layout_1","text":"","title":"Directory Layout"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#linux","text":"The Linux SGX Agent installs by default to /opt/sgx_agent, with the following subfolders:","title":"Linux"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#bin_1","text":"Contains executables and scripts.","title":"Bin"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration_1","text":"Contains the config.yml configuration file.","title":"Configuration"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#logs_1","text":"This folder contains log files: /var/log/sgx_agent","title":"Logs"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#integration-hub","text":"","title":"Integration Hub"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#installation-answer-file-options_2","text":"Key sample Value Description AAS_API_URL https://< Authentication and Authorization Service IP or Hostname>:8444/aas/v1 Base URL for the AAS CMS_BASE_URL https://< Certificate Management Service IP or Hostname>:8445/cms/v1 Base URL for the CMS SHVS_BASE_URL https://< SGX Host Verification Service IP or hostname>:13000/sgx-hvs/v2/ Base URL of SHVS IHUB_SERVICE_USERNAME ihubuser@ihub Database username IHUB_SERVICE_PASSWORD ihubpassword Database password CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest> SHA384 digest of the CMS TLS certificate BEARER_TOKEN Installation token TENANT KUBERNETES Tenant Orchaestrator KUBERNETES_URL https://< Kubernetes Master Node IP or Hostname> :6443 Kubernetes Master node URL KUBERNETES_CRD custom-isecl-sgx CRD Name to be used TLS_SAN_LIST 127.0.0.1, localhost Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service. KUBERNETES_TOKEN Token from Kubernetes Master Node KUBERNETES_CERT_FILE /root/apiserver.crt Kubernetes server certificate path POLL_INTERVAL_MINUTES 2 IHUB Polling Interval in Minutes INSTANCE_NAME ihub IHUB default instance name","title":"Installation Answer File Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration-options_2","text":"The Integration Hub configuration can be found in /etc/ihub/config.yml.","title":"Configuration Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#command-line-options_2","text":"The Integrtion HUB supports several command-line options that can be executed only as the Root user: Syntax: ihub \\<command\\>","title":"Command-Line Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#available-commands_2","text":"","title":"Available Commands"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#help_2","text":"ihub help Displays the list of available CLI commands","title":"Help"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#start_2","text":"ihub start Start the service","title":"Start"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#stop_2","text":"ihub stop stops the service","title":"Stop"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#status_2","text":"ihub status Reports whether the service is currently running.","title":"Status"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#uninstall_2","text":"ihub uninstall \\[\\--purge\\] \\[\\--exec\\] Removes the service. Use --purge option to remove configuration directory(/etc/ihub/). Use --exec option to remove ihub instance specific directories","title":"Uninstall"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#version_2","text":"ihub version Reports the version of the service.","title":"Version"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-task_2","text":"Runs a specific setup task. Syntax: ihub setup [task]","title":"Setup [task]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-tasks-and-its-configuration-options-for-integration-hub","text":"Available Tasks for setup: all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls attestation-service-connection Establish Attestation service connection tenant-service-connection Establish Tenant service connection create-signing-key Create signing key for IHUB update-service-config Sets or Updates the Service configuration Following environment variables are required for \"download-ca-cert\" CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ CMS_TLS_CERT_SHA384 SHA384 hash value of CMS TLS certificate Following environment variables are required in \"download-cert-tls\" CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ BEARER_TOKEN Bearer token for accessing CMS api Following environment variables are optionally used in download-cert-tls TLS_CERT_FILE The file to which certificate is saved TLS_KEY_FILE The file to which private key is saved TLS_COMMON_NAME The common name of signed certificate TLS_SAN_LIST Comma separated list of hostnames to add to Certificate, including IP addresses and DNS names Following environment variables are required for 'attestation-service-connection' setup: SHVS_BASE_URL Base URL for the SGX Host Verification Service Following environment variables are required for 'tenant-service-connection' setup: TENANT Type of Tenant Service ( OpenStack or Kubernetes ) Following environment variables are required for Kubernetes tenant: KUBERNETES_TOKEN Token for Kubernetes deployment KUBERNETES_CERT_FILE Certificate path for Kubernetes deployment KUBERNETES_URL URL for Kubernetes deployment KUBERNETES_CRD CRD Name for Kubernetes deployment Following environment variables are required for OpenStack tenant: OPENSTACK_PLACEMENT_URL Placement API endpoint for OpenStack deployment OPENSTACK_USERNAME UserName for OpenStack deployment OPENSTACK_PASSWORD Password for OpenStack deployment OPENSTACK_AUTH_URL Keystone API endpoint for OpenStack deployment Following environment variables are required for update-service-config setup: LOG_LEVEL Log level LOG_MAX_LENGTH Max length of log statement LOG_ENABLE_STDOUT Enable console log AAS_BASE_URL AAS Base URL SERVICE_USERNAME The service username as configured in AAS SERVICE_PASSWORD The service password as configured in AAS","title":"Setup Tasks and its Configuration Options for Integration Hub"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#directory-layout_2","text":"The Integration HUB installs by default to /opt/ihub with the following folders.","title":"Directory Layout"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#bin_2","text":"This folder contains executable scripts.","title":"Bin"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration_2","text":"This folder /etc/ihub/ contains certificates, keys, and configuration files.","title":"Configuration"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#logs_2","text":"This folder contains log files: /var/log/ihub/","title":"Logs"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#certificate-management-service","text":"","title":"Certificate Management Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#installation-answer-file-options_3","text":"Key Sample Value Description CMS_PORT 8445 Default Port where Certificate Management Service Runs CMS_NOSETUP false Determines whether \u201csetup\u201d will be executed after installation. Typically this is set to \u201cfalse\u201d to install and perform setup in one action. The \u201ctrue\u201d option is intended for building the service as a container, where the installation would be part of the image build, and setup would be performed when the container starts for the first time to generate any persistent data. AAS_API_URL https://< AAS Hostname or IP address>:8444/aas/v1 URL to connect to the AAS, used during setup for authentication. AAS_TLS_SAN < Comma-separated list of IPs/hostnames for the AAS> SAN list populated in special JWT token; this token is used by AAS to get TLS certificate signed from CMS. SAN list in this token and CSR generated by AAS must match.","title":"Installation Answer File Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration-options_3","text":"The CMS configuration can be found in /etc/cms/config.yml.","title":"Configuration Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#command-line-options_3","text":"The Certificate Management Service supports several command-line options that can be executed only as the Root user: Syntax: cms \\<command\\>","title":"Command-Line Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#available-commands_3","text":"","title":"Available Commands"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#help_3","text":"cms help Displays the list of available CLI commands.","title":"Help"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#start_3","text":"cms start Starts the services.","title":"Start"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#stop_3","text":"cms stop Stops the service.","title":"Stop"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#status_3","text":"cms status Reports whether the service is currently running.","title":"Status"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#uninstall_3","text":"cms uninstall \\[\\--purge\\] Uninstalls the service, including the deletion of all files and folders.","title":"Uninstall"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#version_3","text":"cms version Reports the version of the service.","title":"Version"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#tlscertsha384","text":"cms tlscertsha384 Shows the SHA384 digest of the TLS certificate.","title":"Tlscertsha384"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-task_3","text":"Runs a specific setup task. Syntax: cms setup [task] Available Tasks for setup:","title":"Setup [task]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#cms-setup-server-portport","text":"Setup http server on \\<port> Environment variable CMS_PORT=\\<port> can be set alternatively","title":"cms setup server [--port=\\&lt;port>]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#cms-setup-root_ca-force","text":"Create its own self signed Root CA keypair in /etc/cms for quality of life Option [--force] overwrites any existing files, and always generate new Root CA keypair","title":"cms setup root_ca [--force]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#cms-setup-tls-force-host_nameshost_names","text":"Create its own root_ca signed TLS keypair in /etc/cms for quality of life Option [--force] overwrites any existing files, and always generate root_ca signed TLS keypair Argument \\<host_names> is a list of host names used by local machine, seperated by comma Environment variable CMS_HOST_NAMES=\\<host_names> can be set alternatively","title":"cms setup tls [--force] [--host_names=\\&lt;host_names>]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#cms-setup-cms-auth-token-force","text":"Create its own self signed JWT keypair in /etc/cms/jwt for quality of life Option [--force] overwrites any existing files, and always generate new JWT keypair and token","title":"cms setup cms-auth-token [--force]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-tasks-and-its-configuration-options-for-certificate-management-service","text":"Available Tasks for setup: all Runs all setup tasks root-ca Creates a self signed Root CA key pair in /etc/cms/root-ca/ for quality of life intermediate-ca Creates a Root CA signed intermediate CA key pair ( signing, tls-server and tls-client ) in /etc/cms/intermediate-ca/ for quality of life tls Creates an intermediate-ca signed TLS key pair in /etc/cms for quality of life cms-auth-token Create its own self signed JWT key pair in /etc/cms/jwt for quality of life update-service-config Sets or Updates the Service configuration Following environment variables are required for 'tls' setup: SAN_LIST TLS SAN list Following environment variables are required for 'authToken' setup: AAS_JWT_CN Common Name for JWT Signing Certificate used in Authentication and Authorization Service AAS_TLS_CN Common Name for TLS Signing Certificate used in Authentication and Authorization Service AAS_TLS_SAN TLS SAN list for Authentication and Authorization Service Following environment variables are required for 'update-service-config' setup: AAS_BASE_URL AAS Base URL SERVER_PORT The Port on which Server Listens to SERVER_READ_TIMEOUT Request Read Timeout Duration in Seconds SERVER_READ_HEADER_TIMEOUT Request Read Header Timeout Duration in Seconds SERVER_IDLE_TIMEOUT Request Idle Timeout in Seconds LOG_LEVEL Log level LOG_MAX_LENGTH Max length of log statement SERVER_WRITE_TIMEOUT Request Write Timeout Duration in Seconds SERVER_MAX_HEADER_BYTES Max Length Of Request Header in Bytes LOG_ENABLE_STDOUT Enable console log TOKEN_DURATION_MINS Validity of token duration Following environment variables are required for 'root-ca' setup: CMS_CA_PROVINCE CA Certificate Province CMS_CA_COUNTRY CA Certificate Country CMS_CA_CERT_VALIDITY CA Certificate Validity CMS_CA_ORGANIZATION CA Certificate Organization CMS_CA_LOCALITY CA Certificate Locality Following environment variables are required for 'intermediate-ca' setup: CMS_CA_PROVINCE CA Certificate Province CMS_CA_COUNTRY CA Certificate Country CMS_CA_CERT_VALIDITY CA Certificate Validity CMS_CA_ORGANIZATION CA Certificate Organization CMS_CA_LOCALITY CA Certificate Locality","title":"Setup Tasks and its Configuration Options for Certificate Management Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#directory-layout_3","text":"The Certificate Management Service installs by default to /opt/cms with the following folders.","title":"Directory Layout"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#bin_3","text":"This folder contains executable scripts.","title":"Bin"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration_3","text":"This folder /etc/cms contains certificates, keys, and configuration files.","title":"Configuration"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#logs_3","text":"This folder contains log files: /var/log/cms/","title":"Logs"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#cacerts","text":"This folder contains the CMS root CA certificate.","title":"Cacerts"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#authentication-and-authorization-service","text":"","title":"Authentication and Authorization Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#installation-answer-file-options_4","text":"Key Sample Value Description CMS_BASE_URL https://< cms IP or hostname>/cms/v1/ Provides the URL for the CMS. AAS_NOSETUP false Determines whether \u201csetup\u201d will be executed after installation. Typically this is set to \u201cfalse\u201d to install and perform setup in one action. The \u201ctrue\u201d option is intended for building the service as a container, where the installation would be part of the image build, and setup would be performed when the container starts for the first time to generate any persistent data. AAS_DB_HOSTNAME localhost Hostname or IP address of the AAS database AAS_DB_PORT 5432 Database port number AAS_DB_NAME pgdb Database name AAS_DB_USERNAME aasdbuser Database username AAS_DB_PASSWORD aasdbpassd Database password AAS_DB_SSLMODE verify-full AAS_DB_SSLCERTSRC /usr/local/pgsql/data/server.crt Required if the \u201cAAS_DB_SSLMODE\u201d is set to \u201cverify-ca.\u201d Defines the location of the database SSL certificate. AAS_DB_SSLCERT < path_to_cert_file_on_system > The AAS_DB_SSLCERTSRC variable defines the source location of the database SSL certificate; this variable determines the local location. If the former option is used without specifying this option, the service will copy the SSL certificate to the default configuration directory. AAS_ADMIN_USERNAME admin@aas Defines a new AAS administrative user. This user will be able to create new users, new roles, and new role-user mappings. This user will have the AAS:Administrator role. AAS_ADMIN_PASSWORD aasAdminPass Password for the new AAS admin user AAS_JWT_CERT_SUBJECT \"AAS JWT Signing Certificate\" Defines the subject of the JWT signing certificate. AAS_JWT_TOKEN_DURATION 5 Defines the amount of time in minutes that an issued token will be valid. SAN_LIST 127.0.0.1,localhost Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service. BEARER_TOKEN Installation Token from AAS.","title":"Installation Answer File Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration-options_4","text":"The AAS configuration can be found in /etc/authservice/config.yml.","title":"Configuration Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#command-line-options_4","text":"The AAS supports several command-line options that can be executed only as the Root user: Syntax: authservice \\<command\\>","title":"Command-Line Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#available-commands_4","text":"","title":"Available Commands"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#help_4","text":"authservice help Displays the list of available CLI commands.","title":"Help"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#start_4","text":"authservice start Starts the service.","title":"Start"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#stop_4","text":"authservice stop Stops the service.","title":"Stop"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#status_4","text":"authservice status Displays the current status of the service.","title":"Status"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#uninstall_4","text":"authservice uninstall \\[\\--purge\\] Removes the service. Use the \"--purge\" flag to also delete all data.","title":"Uninstall"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#version_4","text":"authservice version Shows the version of the service.","title":"Version"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-task_4","text":"Executes a specific setup task. Can be used to change the current configuration. Syntax: authservice setup [task] Available Tasks for setup:","title":"Setup [task]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#authservice-setup-all","text":"Runs all setup tasks","title":"authservice setup all"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#authservice-setup-database-force-argumentsargument_value","text":"Available arguments are: db-host alternatively, set environment variable AAS_DB_HOSTNAME db-port alternatively, set environment variable AAS_DB_PORT db-user alternatively, set environment variable AAS_DB_USERNAME db-pass alternatively, set environment variable AAS_DB_PASSWORD db-name alternatively, set environment variable AAS_DB_NAME db-sslmode \\<disable|allow|prefer|require|verify-ca|verify-full> alternatively, set environment variable AAS_DB_SSLMODE db-sslcert path to where the certificate file of database. Only applicable for db-sslmode=\\<verify-ca|verify-full. If left empty, the cert will be copied to /etc/authservice/tdcertdb.pem alternatively, set environment variable AAS_DB_SSLCERT db-sslcertsrc \\<path to where the database ssl/tls certificate file> mandatory if db-sslcert does not already exist alternatively, set environment variable AAS_DB_SSLCERTSRC Run this command with environment variable AAS_DB_REPORT_MAX_ROWS and AAS_DB_REPORT_NUM_ROTATIONS can update db rotation arguments","title":"authservice setup database [-force] [-arguments=\\&lt;argument_value>]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#authservice-setup-server-portport","text":"Setup http server on \\<port> Environment variable AAS_PORT=\\<port> can be set alternatively authservice setup tls [--force] [--host_names=\\<host_names>] Use the key and certificate provided in /etc/threat-detection if files exist Otherwise create its own self-signed TLS keypair in /etc/authservice for quality of life Option [--force] overwrites any existing files, and always generate self-signed keypair Argument \\<host_names> is a list of host names used by local machine, seperated by comma Environment variable AAS_TLS_HOST_NAMES=\\<host_names> can be set alternatively","title":"authservice setup server [--port=\\&lt;port>]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#authservice-setup-admin-userusername-passpassword","text":"Environment variable AAS_ADMIN_USERNAME=\\<username> can be set alternatively Environment variable AAS_ADMIN_PASSWORD=\\<password> can be set alternatively","title":"authservice setup admin [--user=\\&lt;username>] [-pass=\\&lt;password>]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#authservice-setup-jwt","text":"Create jwt signing key and jwt certificate signed by CMS Environment variable CMS_BASE_URL=\\<url> for CMS API url Environment variable AAS_JWT_CERT_CN=\\<CERTIFICATE SUBJECT> AAS JWT","title":"authservice setup jwt"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#certificate-subject","text":"Environment variable AAS_JWT_INCLUDE_KEYID=\\<KEY ID> AAS include key id in JWT Token Environment variable AAS_JWT_TOKEN_DURATION_MINS=\\<DURATION> JWT Token validation minutes Environment variable BEARER_TOKEN=\\<token> for authenticating with CMS","title":"Certificate Subject"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-tasks-and-its-configuration-options-for-authentication-and-authorization-service","text":"Available Tasks for setup: all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls database Setup authservice database admin Add authservice admin username and password to database and assign respective roles to the user jwt Create jwt signing key and jwt certificate signed by CMS update-service-config Sets or Updates the Service configuration Following environment variables are required for 'download-ca-cert' CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ CMS_TLS_CERT_SHA384 SHA384 hash value of CMS TLS certificate Following environment variables are required in 'download-cert-tls' CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ BEARER_TOKEN Bearer token for accessing CMS api Following environment variables are optionally used in download-cert-tls TLS_CERT_FILE The file to which certificate is saved TLS_KEY_FILE The file to which private key is saved TLS_COMMON_NAME The common name of signed certificate TLS_SAN_LIST Comma separated list of hostnames to add to Certificate, including IP addresses and DNS names Following environment variables are required for 'Database' related setups: DB_VENDOR Vendor of database, or use AAS_DB_VENDOR alternatively DB_HOST Database host name, or use AAS_DB_HOSTNAME alternatively DB_USERNAME Database username, or use AAS_DB_USERNAME alternatively DB_PASSWORD Database password, or use AAS_DB_PASSWORD alternatively DB_SSL_MODE Database SSL mode, or use AAS_DB_SSL_MODE alternatively DB_SSL_CERT Database SSL certificate, or use AAS_DB_SSLCERT alternatively DB_PORT Database port, or use AAS_DB_PORT alternatively DB_NAME Database name, or use AAS_DB_NAME alternatively DB_SSL_CERT_SOURCE Database SSL certificate to be copied from, or use AAS_DB_SSLCERTSRC alternatively DB_CONN_RETRY_ATTEMPTS Database connection retry attempts DB_CONN_RETRY_TIME Database connection retry time Following environment variables are required for 'admin' setup: AAS_ADMIN_USERNAME Authentication and Authorization Service Admin Username AAS_ADMIN_PASSWORD Authentication and Authorization Service Admin Password Following environment variables are required in 'jwt' CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ BEARER_TOKEN Bearer token for accessing CMS api Following environment variables are optionally used in jwt CERT_FILE The file to which certificate is saved KEY_FILE The file to which private key is saved COMMON_NAME The common name of signed certificate Following environment variables are required for 'update-service-config' setup: AUTH_DEFENDER_LOCKOUT_DURATION_MINS Auth defender lockout duration in minutes SERVER_MAX_HEADER_BYTES Max Length Of Request Header in Bytes JWT_INCLUDE_KID Includes JWT Key Id for token validation SERVER_READ_HEADER_TIMEOUT Request Read Header Timeout Duration in Seconds AUTH_DEFENDER_MAX_ATTEMPTS Auth defender maximum attempts SERVER_PORT The Port on which Server Listens to SERVER_READ_TIMEOUT Request Read Timeout Duration in Seconds LOG_MAX_LENGTH Max length of log statement LOG_ENABLE_STDOUT Enable console log JWT_CERT_COMMON_NAME Common Name for JWT Certificate SERVER_WRITE_TIMEOUT Request Write Timeout Duration in Seconds SERVER_IDLE_TIMEOUT Request Idle Timeout in Seconds LOG_LEVEL Log level JWT_TOKEN_DURATION_MINS Validity of token duration AUTH_DEFENDER_INTERVAL_MINS Auth defender interval in minutes","title":"Setup Tasks and its Configuration Options for Authentication and Authorization Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#directory-layout_4","text":"The Authentication and Authorization Service installs by default to /opt/authservice with the following folders.","title":"Directory Layout"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#bin_4","text":"Contains executable scripts and binaries.","title":"Bin"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration_4","text":"This folder /etc/authservice contains certificates, keys, and configuration files.","title":"Configuration"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#logs_4","text":"This folder contains log files: /var/log/authservice","title":"Logs"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#dbscripts","text":"This folder /opt/authservice/dbscripts Contains database scripts","title":"Dbscripts"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#key-broker-service","text":"","title":"Key Broker Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#installation-answer-file-options_5","text":"Variable Name Default Value Notes CMS_BASE_URL https://< CMS IP or hostname >:8445/cms/v1/ Required for generating TLS certificate AAS_API_URL https://< AAS IP or hostname >:8444/aas/v1 AAS service url SQVS_URL https://< SQVS IP or hostname >:12000/svs/v1/ Required to get the SGX Quote verified CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest > SHA384 digest of CMS TLS certificate BEARER_TOKEN JWT token for installation user KBS_SERVICE_USERNAME admin@kms KBS Service Username KBS_SERVICE_PASSWORD kmsAdminPass KBS Service User Password ENDPOINT_URL https://< KBS Hostname >:9443/v1 KBS Endpoint URL TLS_COMMON_NAME KBS TLS Certificate KBS TLS Certificate common-name SERVER_PORT 9443 KBS Secure Port SKC_CHALLENGE_TYPE SGX Challenge Type TLS_SAN_LIST < KBS IP/Hostname > IP addresses/hostnames to be included in SAN list. KEY_MANAGER KMIP Key Manager Backend to store keys","title":"Installation Answer File Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration-options_5","text":"The Key Broker Service configuration is in path /etc/kbs/config.yml.","title":"Configuration Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#command-line-options_5","text":"The Key Broker Service supports several command-line options that can be executed only as the Root user: Syntax: kbs \\<command\\>","title":"Command-Line Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#available-commands_5","text":"","title":"Available Commands"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#help_5","text":"kbs help Displays the list of available CLI commands.","title":"Help"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#start_5","text":"kbs start Starts the service","title":"Start"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#stop_5","text":"kbs stop Stops the service","title":"Stop"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#status_5","text":"kbs status Displays the current status of the service.","title":"Status"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#uninstall_5","text":"kbs uninstall \\[\\--purge\\] Removes the service","title":"Uninstall"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#version_5","text":"kbs version Displays the version of the service","title":"Version"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-task_5","text":"Runs a specific setup task. Syntax: kbs setup [task]","title":"Setup [task]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-tasks-and-its-configuration-options-for-key-broker-service","text":"Available Tasks for setup: all Runs all setup tasks download-ca-cert Download CMS root CA certificate download-cert-tls Download CA certificate from CMS for tls create-default-key-transfer-policy Create default key transfer policy for KBS update-service-config Sets or Updates the Service configuration Following environment variables are required for 'update-service-config' setup: SERVICE_USERNAME The service username as configured in AAS AAS_BASE_URL AAS Base URL KMIP_ROOT_CERT_PATH KMIP Root Certificate path KMIP_SERVER_IP IP of KMIP server KMIP_CLIENT_KEY_PATH KMIP Client key path SERVER_READ_TIMEOUT Request Read Timeout Duration in Seconds SERVER_READ_HEADER_TIMEOUT Request Read Header Timeout Duration in Seconds SERVER_IDLE_TIMEOUT Request Idle Timeout in Seconds SQVS_URL SQVS URL SESSION_EXPIRY_TIME Session Expiry Time SERVER_PORT The Port on which Server Listens to LOG_LEVEL Log level LOG_MAX_LENGTH Max length of log statement LOG_ENABLE_STDOUT Enable console log KMIP_SERVER_PORT PORT of KMIP server KMIP_CLIENT_CERT_PATH KMIP Client certificate path SERVER_WRITE_TIMEOUT Request Write Timeout Duration in Seconds SERVER_MAX_HEADER_BYTES Max Length Of Request Header in Bytes SERVICE_PASSWORD The service password as configured in AAS SKC_CHALLENGE_TYPE SKC challenge type Following environment variables are required for 'download-ca-cert' CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ CMS_TLS_CERT_SHA384 SHA384 hash value of CMS TLS certificate Following environment variables are required in 'download-cert-tls' CMS_BASE_URL CMS base URL in the format https:// {{ cms }} : {{ cms_port }} /cms/v1/ BEARER_TOKEN Bearer token for accessing CMS api Following environment variables are optionally used in download-cert-tls TLS_KEY_FILE The file to which private key is saved TLS_COMMON_NAME The common name of signed certificate TLS_CERT_FILE The file to which certificate is saved TLS_SAN_LIST Comma separated list of hostnames to add to Certificate, including IP addresses and DNS names","title":"Setup Tasks and its Configuration Options for Key Broker Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#directory-layout_5","text":"The Key Broker Service installs by default to /opt/kbs with the following folders.","title":"Directory Layout"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#bin_5","text":"Contains executable scripts and binaries.","title":"Bin"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration_5","text":"This folder /etc/kbs contains certificates, keys, and configuration files.","title":"Configuration"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#logs_5","text":"This folder contains log files: /var/log/kbs","title":"Logs"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#sgx-caching-service","text":"","title":"SGX Caching Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#installation-answer-file-options_6","text":"Key Sample Value Description CMS_BASE_URL https://< CMS IP or hostname >:8445/cms/v1/ CMS URL for Certificate Management Service AAS_API_URL https://< AAS IP or hostname >:8444/aas/v1 API URL for Authentication Authorization Service SCS_ADMIN_USERNAME scsuser@scs SCS Service username SCS_ADMIN_PASSWORD scspassword SCS Service password BEARER_TOKEN Installation Token from AAS CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest > SHA384 Hash sum for verifying the CMS TLS certificate. INTEL_PROVISIONING_SERVER https://sbx.api.trustedservices.intel.com/sgx/certification/v3 Intel pcs server url INTEL_PROVISIONING_SERVER_API_KEY < Add your API subscription key > Intel PCS Server API subscription key SCS_REFRESH_HOURS 1 hour Time after which the SGX collaterals in SCS db get refreshed from Intel PCS server RETRY_COUNT 3 Number Of times to connect to PCS if PCS service is not accessible WAIT_TIME 1 Number Of Seconds between retries to connect to PCS SCS_DB_HOSTNAME localhost SCS Databse hostname SCS_DB_PORT 5432 SCS Database port SCS_DB_NAME pgscsdb SCS Database name SCS_DB_USERNAME aasdbuser SCS Database username SCS_DB_PASSWORD aasdbpassword SCS Database password SCS_DB_SSLCERTSRC /usr/local/pgsql/data/server.crt SAN_LIST 127.0.0.1,localhost Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service.","title":"Installation Answer File Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration-options_6","text":"The SGX Caching Service configuration can be found in /etc/scs/config.yml.","title":"Configuration Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#command-line-options_6","text":"The SGX Caching Service supports several command-line options that can be executed only as the Root user: Syntax: scs \\<command\\>","title":"Command-Line Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#available-commands_6","text":"","title":"Available Commands"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#help_6","text":"scs help Displays the list of available CLI commands.","title":"Help"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#start_6","text":"scs start Starts the SGX Caching Service","title":"Start"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#stop_6","text":"scs stop Stops the SGX Caching Service","title":"Stop"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#status_6","text":"scs status Reports whether the SGX Caching Service is currently running","title":"Status"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#uninstall_6","text":"scs uninstall \\[\\--purge\\] uninstall the SGX Caching Service. --purge option needs to be applied to remove configuration files","title":"Uninstall"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#version_6","text":"scs version Reports the version of the scs","title":"Version"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-task_6","text":"Runs a specific setup task. Syntax: scs setup [task]","title":"Setup [task]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-tasks-and-its-configuration-options-for-sgx-caching-service","text":"Avaliable Tasks for setup: all Runs all setup tasks Required env variables: - get required env variables from all the setup tasks Optional env variables: - get optional env variables from all the setup tasks scs setup database - Avaliable arguments are: - SCS_DB_HOSTNAME - SCS_DB_PORT - SCS_DB_USERNAME - SCS_DB_PASSWORD - SCS_DB_NAME - SCS_DB_SSLMODE <disable | allow | prefer | require | verify-ca | verify-full> - SCS_DB_SSLCERT path to where the certificate file of database. Only applicable for db-sslmode = <verify-ca | verify-full. If left empty, the cert will be copied to /etc/scs/tdcertdb.pem - SCS_DB_SSLCERTSRC <path to where the database ssl/tls certificate file> mandatory if db-sslcert does not already exist update_service_config Updates Service Configuration Required env variables: - SCS_PORT : SGX Caching Service port - SCS_SERVER_READ_TIMEOUT : SGX Caching Service Read Timeout - SCS_SERVER_READ_HEADER_TIMEOUT : SGX Caching Service Read Header Timeout Duration - SCS_SERVER_WRITE_TIMEOUT : SGX Caching Service Request Write Timeout Duration - SCS_SERVER_IDLE_TIMEOUT : SGX Caching Service Request Idle Timeout - SCS_SERVER_MAX_HEADER_BYTES : SGX Caching Service Max Length Of Request Header Bytes - INTEL_PROVISIONING_SERVER : Intel ECDSA Provisioning Server URL - INTEL_PROVISIONING_SERVER_API_KEY : Intel ECDSA Provisioning Server API Subscription key - SCS_LOGLEVEL : SGX Caching Service Log Level - SCS_LOG_MAX_LENGTH : SGX Caching Service Log maximum length - SCS_ENABLE_CONSOLE_LOG : SGX Caching Service Enable standard output - SCS_REFRESH_HOURS : SCS Automatic Refresh of SGX Data - RETRY_COUNT : Number of retry to PCS server - WAIT_TIME : Duration Time between each retries to PCS - AAS_API_URL : AAS API URL download_ca_cert Download CMS root CA certificate Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that AAS is talking to the right CMS instance download_cert TLS Generates Key pair and CSR, gets it signed from CMS Required env variable if SCS_NOSETUP = true or variable not set in config.yml: - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that AAS is talking to the right CMS instance Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - BEARER_TOKEN = <token> : for authenticating with CMS - SAN_LIST = <san> : list of hosts which needs access to service Optional env variables specific to setup task are: - KEY_PATH = <key_path> : Path of file where TLS key needs to be stored - CERT_PATH = <cert_path> : Path of file/directory where TLS certificate needs to be stored","title":"Setup Tasks and its Configuration Options for SGX Caching Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#directory-layout_6","text":"The SGX Caching Service installs by default to /opt/scs with the following folders.","title":"Directory Layout"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#bin_6","text":"Contains SGX Caching Service executable binary.","title":"Bin"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration_6","text":"This folder /etc/scs contains certificates, keys, and configuration files.","title":"Configuration"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#logs_6","text":"This folder contains log files: /var/log/scs","title":"Logs"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#sgx-quote-verification-service","text":"","title":"SGX Quote Verification Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#installation-answer-file-options_7","text":"Key Sample Value Description CMS_BASE_URL https://< CMS IP address or hostname >:8445/cms/v1/ Defines the base URL for the CMS owned by the image owner. Note that this CMS may be different from the CMS used for other components. AAS_API_URL https://< AAS IP address or hostname >:8444/aas/v1 Defines the baseurl for the AAS owned by the image owner. Note that this AAS may be different from the AAS used for other components. SCS_BASE_URL https://< SCS IP address or hostname >:9000/scs/sgx/certification/v1/ The SCS url is needed. SGX_TRUSTED_ROOT_CA_PATH /tmp/trusted_rootca.pem The path to SGX root ca used to verify quote CMS_TLS_CERT_SHA384 < Certificate Management Service TLS digest > SHA384 hash of the CMS TLS certificate BEARER_TOKEN Token from CMS with permissions used for installation. SQVS_LOG_LEVEL INFO (default), DEBUG Defines the log level for the SQVS. Defaults to INFO. SQVS_PORT 12000 SQVS Secure Port SQVS_NOSETUP false Skips setup during installation if set to true SAN_LIST 127.0.0.1,localhost Comma-separated list of IP addresses and hostnames that will be valid connection points for the service. Requests sent to the service using an IP or hostname not in this list will be denied, even if it resolves to this service. SQVS_INCLUDE_TOKEN true If true, SQVS will authenticate KBS before Quote Verifiation","title":"Installation Answer File Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration-options_7","text":"The SGX Quote Verification Service configuration can be found in /etc/sqvs/config.yml.","title":"Configuration Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#command-line-options_7","text":"The SGX Quote Verifiction Service supports several command-line options that can be executed only as the Root user: Syntax: sqvs \\<command\\>","title":"Command-Line Options"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#available-commands_7","text":"","title":"Available Commands"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#help_7","text":"sqvs help Displays the list of available CLI commands.","title":"Help"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#start_7","text":"sqvs start Starts the SGX Quote Verification Service","title":"Start"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#stop_7","text":"sqvs stop Stops the SGX Quote Verification Service","title":"Stop"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#status_7","text":"sqvs status Reports whether the SGX Quote Verification Service is currently running.","title":"Status"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#uninstall_7","text":"sqvs uninstall \\[\\--purge\\] uninstalls the SGX Quote Verification Service. --purge option needs to be applied to remove configuration files","title":"Uninstall"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#version_7","text":"sqvs version Reports the version of the sqvs","title":"Version"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-task_7","text":"Runs a specific setup task. Syntax: sqvs setup [task]","title":"Setup [task]"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#setup-tasks-and-its-configuration-options-for-sgx-quote-verification-service","text":"Available Tasks for setup: Required env variables: - get required env variables from all the setup tasks Optional env variables: - get optional env variables from all the setup tasks update_service_config Updates Service Configuration Required env variables: - SQVS_PORT : SGX Verification Service port - SQVS_SERVER_READ_TIMEOUT : SGX Verification Service Read Timeout - SQVS_SERVER_READ_HEADER_TIMEOUT : SGX Verification Service Read Header Timeout Duration - SQVS_SERVER_WRITE_TIMEOUT : SGX Verification Service Request Write Timeout Duration - SQVS_SERVER_IDLE_TIMEOUT : SGX Verification Service Request Idle Timeout - SQVS_SERVER_MAX_HEADER_BYTES : SGX Verification Service Max Length Of Request Header Bytes - SQVS_LOGLEVEL : SGX Verification Service Log Level - SQVS_LOG_MAX_LENGTH : SGX Verification Service Log maximum length - SQVS_ENABLE_CONSOLE_LOG : SGX Verification Service Enable standard output - SQVS_INCLUDE_TOKEN : Boolean value to decide whether to use token based auth or no auth for quote verifier API - SGX_TRUSTED_ROOT_CA_PATH : SQVS Trusted Root CA - SCS_BASE_URL : SGX Caching Service URL - AAS_API_URL : AAS API URL download_ca_cert Download CMS root CA certificate Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that AAS is talking to the right CMS instance download_cert TLS Generates Key pair and CSR, gets it signed from CMS - CMS_TLS_CERT_SHA384 = <CMS TLS cert sha384 hash> : to ensure that AAS is talking to the right CMS instance Required env variables specific to setup task are: - CMS_BASE_URL = <url> : for CMS API url - BEARER_TOKEN = <token> : for authenticating with CMS - SAN_LIST = <san> : list of hosts which needs access to service Optional env variables specific to setup task are: - KEY_PATH = <key_path> : Path of file where TLS key needs to be stored - CERT_PATH = <cert_path> : Path of file/directory where TLS certificate needs to be stored","title":"Setup Tasks and its Configuration Options for SGX Quote Verification Service"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#directory-layout_7","text":"The SGX Quote Verification Service installs by default to /opt/sqvs with the following folders.","title":"Directory Layout"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#bin_7","text":"This folder contains executable scripts.","title":"Bin"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#configuration_7","text":"This folder /etc/sqvs contains certificates, keys, and configuration files.","title":"Configuration"},{"location":"product-guides/SGX%20Infrastructure/8Intel%20Security%20Libraries%20Configuration%20Settings/#logs_7","text":"This folder contains log files: /var/log/sqvs","title":"Logs"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/","text":"Uninstallation This section describes steps used for uninstalling Intel SecL-DC services. Certificate Management Service To uninstall the Certificate Management Service, run the following command: cms uninstall \\--purge Removes following directories: /opt/cms /run/cms /var/log/cms /etc/cms Authentication and Authorization Service To uninstall the Authentication and Authorization Service, run the following command: authservice uninstall \\--purge Removes following directories: /opt/authservice /run/authservice /var/log/authservice /etc/authservice SGX Host Verification Service To uninstall the SGX Host Verification Service, run the following command: shvs uninstall \\--purge Removes following directories: /opt/shvs /run/shvs /var/log/shvs /etc/shvs SGX_Agent To uninstall the SGX Agent, run the following command: sgx_agent uninstall \\--purge Removes following directories: /opt/sgx_agent /run/sgx_agent /var/log/sgx_agent /etc/sgx_agent Integration Hub To uninstall the Integration Hub, run the following command: ihub uninstall \\--purge Removes the following directories: /opt/ihub /run/ihub /var/log/ihub /etc/ihub SGX Caching Service To uninstall the SGX Caching Service , run the following command: scs uninstall \\--purge Removes the following directories: /opt/scs /run/scs /var/log/scs /etc/scs SGX Quote Verification Service To uninstall the SGX Quote Verification Service, run the following command: sqvs uninstall \\--purge Removes the following directories: /opt/sqvs /run/sqvs /var/log/sqvs /etc/sqvs Key Broker Service To uninstall the Key Broker Service , run the following command: kbs uninstall \\--purge Removes the following directories: /opt/kbs /run/kbs /var/log/kbs /etc/kbs SKC Library To uninstall the SKC Library, run the following command: ./opt/skc/devops/scripts/uninstall.sh Removes the following directories: /opt/skc isecl-k8s-extensions Cluster admin can uninstall the isecl-k8s-extensions by running following commands: kubectl delete svc isecl-scheduler-svc -n isecl kubectl delete deployment isecl-controller isecl-scheduler -n isecl kubectl delete crds hostattributes.crd.isecl.intel.com rm -rf /opt/isecl-k8s-extensions rm -rf /var/log/isecl-k8s-extensions TLS Certificates TLS certificates for each service are issued by the Certificate Management Service during installation. If the CMS root certificate is changed, or to regenerate the TLS certificate for a given service, use the following commands (note: environment variables will need to be set; typically these are the same variables set in the service installation .env file): <servicename> download_ca_cert Download CMS root CA certificate Environment variable CMS_BASE_URL=\\<url> for CMS API url <servicename> download_cert TLS Generates Key pair and CSR, gets it signed from CMS Environment variable CMS_BASE_URL=\\<url> for CMS API url Environment variable BEARER_TOKEN=\\<token> for authenticating with CMS Environment variable KEY_PATH=\\<key_path> to override default specified in config Environment variable CERT_PATH=\\<cert_path> to override default specified in config","title":"Uninstallation"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#uninstallation","text":"This section describes steps used for uninstalling Intel SecL-DC services.","title":"Uninstallation"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#certificate-management-service","text":"To uninstall the Certificate Management Service, run the following command: cms uninstall \\--purge Removes following directories: /opt/cms /run/cms /var/log/cms /etc/cms","title":"Certificate Management Service"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#authentication-and-authorization-service","text":"To uninstall the Authentication and Authorization Service, run the following command: authservice uninstall \\--purge Removes following directories: /opt/authservice /run/authservice /var/log/authservice /etc/authservice","title":"Authentication and Authorization Service"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#sgx-host-verification-service","text":"To uninstall the SGX Host Verification Service, run the following command: shvs uninstall \\--purge Removes following directories: /opt/shvs /run/shvs /var/log/shvs /etc/shvs","title":"SGX Host Verification Service"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#sgx_agent","text":"To uninstall the SGX Agent, run the following command: sgx_agent uninstall \\--purge Removes following directories: /opt/sgx_agent /run/sgx_agent /var/log/sgx_agent /etc/sgx_agent","title":"SGX_Agent"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#integration-hub","text":"To uninstall the Integration Hub, run the following command: ihub uninstall \\--purge Removes the following directories: /opt/ihub /run/ihub /var/log/ihub /etc/ihub","title":"Integration Hub"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#sgx-caching-service","text":"To uninstall the SGX Caching Service , run the following command: scs uninstall \\--purge Removes the following directories: /opt/scs /run/scs /var/log/scs /etc/scs","title":"SGX Caching Service"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#sgx-quote-verification-service","text":"To uninstall the SGX Quote Verification Service, run the following command: sqvs uninstall \\--purge Removes the following directories: /opt/sqvs /run/sqvs /var/log/sqvs /etc/sqvs","title":"SGX Quote Verification Service"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#key-broker-service","text":"To uninstall the Key Broker Service , run the following command: kbs uninstall \\--purge Removes the following directories: /opt/kbs /run/kbs /var/log/kbs /etc/kbs","title":"Key Broker Service"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#skc-library","text":"To uninstall the SKC Library, run the following command: ./opt/skc/devops/scripts/uninstall.sh Removes the following directories: /opt/skc","title":"SKC Library"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#isecl-k8s-extensions","text":"Cluster admin can uninstall the isecl-k8s-extensions by running following commands: kubectl delete svc isecl-scheduler-svc -n isecl kubectl delete deployment isecl-controller isecl-scheduler -n isecl kubectl delete crds hostattributes.crd.isecl.intel.com rm -rf /opt/isecl-k8s-extensions rm -rf /var/log/isecl-k8s-extensions","title":"isecl-k8s-extensions"},{"location":"product-guides/SGX%20Infrastructure/9Uninstallation/#tls-certificates","text":"TLS certificates for each service are issued by the Certificate Management Service during installation. If the CMS root certificate is changed, or to regenerate the TLS certificate for a given service, use the following commands (note: environment variables will need to be set; typically these are the same variables set in the service installation .env file): <servicename> download_ca_cert Download CMS root CA certificate Environment variable CMS_BASE_URL=\\<url> for CMS API url <servicename> download_cert TLS Generates Key pair and CSR, gets it signed from CMS Environment variable CMS_BASE_URL=\\<url> for CMS API url Environment variable BEARER_TOKEN=\\<token> for authenticating with CMS Environment variable KEY_PATH=\\<key_path> to override default specified in config Environment variable CERT_PATH=\\<cert_path> to override default specified in config","title":"TLS Certificates"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/1Hardware%20%26%20OS%20Requirements/","text":"Hardware & OS Requirements Physical Server requirements Intel\u00ae SecL-DC supports and uses a variety of Intel security features, but there are some key requirements to consider before beginning an installation. Most important among these is the Root of Trust configuration. This involves deciding what combination of TXT, Boot Guard, tboot, and UEFI Secure Boot to enable on platforms that will be attested using Intel\u00ae SecL. Note At least one \"Static Root of Trust\" mechanism must be used (TXT and/or BtG). For Legacy BIOS systems, tboot must be used. For UEFI mode systems, UEFI SecureBoot must be used* Use the chart below for a guide to acceptable configuration options. Only dTPM is supported on Intel\u00ae SecL-DC platform hardware. OS Requirements RHEL 8.3 OS rhel-8-for-x86_64-baseos-rpms and rhel-8-for-x86_64-appstream-rpms repositories need to be enabled on build machine and remote machines Date and time should be in sync across the machines User Access The services need to be built & installed as root user. Ensure root privileges are present for the user to work with Intel\u00ae SecL-DC. Note When using Ansible role for deployment, Ansible needs to be able to talk to remote machines as root user for successful deployment All Intel\u00ae SecL-DC service & agent ports should be allowed in firewall rules.","title":"Hardware & OS Requirements"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/1Hardware%20%26%20OS%20Requirements/#hardware-os-requirements","text":"","title":"Hardware &amp; OS Requirements"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/1Hardware%20%26%20OS%20Requirements/#physical-server-requirements","text":"Intel\u00ae SecL-DC supports and uses a variety of Intel security features, but there are some key requirements to consider before beginning an installation. Most important among these is the Root of Trust configuration. This involves deciding what combination of TXT, Boot Guard, tboot, and UEFI Secure Boot to enable on platforms that will be attested using Intel\u00ae SecL. Note At least one \"Static Root of Trust\" mechanism must be used (TXT and/or BtG). For Legacy BIOS systems, tboot must be used. For UEFI mode systems, UEFI SecureBoot must be used* Use the chart below for a guide to acceptable configuration options. Only dTPM is supported on Intel\u00ae SecL-DC platform hardware.","title":"Physical Server requirements"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/1Hardware%20%26%20OS%20Requirements/#os-requirements","text":"RHEL 8.3 OS rhel-8-for-x86_64-baseos-rpms and rhel-8-for-x86_64-appstream-rpms repositories need to be enabled on build machine and remote machines Date and time should be in sync across the machines","title":"OS Requirements"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/1Hardware%20%26%20OS%20Requirements/#user-access","text":"The services need to be built & installed as root user. Ensure root privileges are present for the user to work with Intel\u00ae SecL-DC. Note When using Ansible role for deployment, Ansible needs to be able to talk to remote machines as root user for successful deployment All Intel\u00ae SecL-DC service & agent ports should be allowed in firewall rules.","title":"User Access"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/2Deployment%20Model/","text":"Deployment Model Build + Deployment Machine CSP - ISecL Services Machine CSP - Physical Server as per supported configurations Enterprise - ISecL Services Machine","title":"Deployment Model"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/2Deployment%20Model/#deployment-model","text":"Build + Deployment Machine CSP - ISecL Services Machine CSP - Physical Server as per supported configurations Enterprise - ISecL Services Machine","title":"Deployment Model"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/3Build%20Services%20and%20packages/","text":"Build Services and packages The below steps needs to be carried out on the Build and Deployment VM Pre-requisites The repos can be built only as root user RHEL 8.3 Machine for building repos Enable the following RHEL repos: rhel-8-for-x86_64-appstream-rpms rhel-8-for-x86_64-baseos-rpms Install basic utilities for getting started dnf install git wget tar python3 yum-utils Create symlink for python3 ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip Install repo tool tmpdir = $( mktemp -d ) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir /repo /usr/local/bin rm -rf $tmpdir Golang installation wget https://dl.google.com/go/go1.14.4.linux-amd64.tar.gz tar -xzf go1.14.4.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT = /usr/local/go export PATH = $GOROOT /bin: $PATH rm -rf go1.14.4.linux-amd64.tar.gz Building Foundational Security Usecase Sync the repo mkdir -p /root/intel-secl/build/fs && cd /root/intel-secl/build/fs repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/fs.xml repo sync Run the pre-requisites setup script cd utils/build/foundational-security/ chmod +x fs-prereq.sh ./fs-prereq.sh -s Build all repos cd /root/intel-secl/build/fs/ make binaries Built Binaries /root/intel-secl/build/fs/binaries Workload Security Usecase VM Confidentiality Sync the repo mkdir -p /root/intel-secl/build/vmc && cd /root/intel-secl/build/vmc repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/vmc.xml repo sync Run the pre-req script cd utils/build/workload-security chmod +x ws-prereq.sh ./ws-prereq.sh -v Build repo cd /root/intel-secl/build/vmc/ make all Built Binaries /root/intel-secl/build/vmc/binaries/ Container Confidentiality with CRIO Runtime Sync the repo mkdir -p /root/intel-secl/build/cc-crio && cd /root/intel-secl/build/cc-crio repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/cc-crio.xml repo sync Run the pre-requisites script cd utils/build/workload-security chmod +x ws-prereq.sh ./ws-prereq.sh -c Enable and start the Docker daemon systemctl enable docker systemctl start docker Ignore the below steps if not running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" #Reload docker systemctl daemon-reload systemctl restart docker Download go dependencies cd /root/ go get github.com/cpuguy83/go-md2man mv /root/go/bin/go-md2man /usr/bin/ Build the repos cd /root/intel-secl/build/cc-crio make binaries Note The crio use case uses containerd that is bundled with docker-ce-19.03.13 during build time. As of this release , the version being used is containerd-1.4.4 . If the remote docker-ce repo gets updated for newer containerd version, then the version of containerd might be incompatible for building crio use case. It is recommended to use the version 1.4.4 in that case. Built binaries /root/intel-secl/build/cc-crio/binaries/","title":"Build Services and packages"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/3Build%20Services%20and%20packages/#build-services-and-packages","text":"The below steps needs to be carried out on the Build and Deployment VM","title":"Build Services and packages"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/3Build%20Services%20and%20packages/#pre-requisites","text":"The repos can be built only as root user RHEL 8.3 Machine for building repos Enable the following RHEL repos: rhel-8-for-x86_64-appstream-rpms rhel-8-for-x86_64-baseos-rpms Install basic utilities for getting started dnf install git wget tar python3 yum-utils Create symlink for python3 ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip Install repo tool tmpdir = $( mktemp -d ) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir /repo /usr/local/bin rm -rf $tmpdir Golang installation wget https://dl.google.com/go/go1.14.4.linux-amd64.tar.gz tar -xzf go1.14.4.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT = /usr/local/go export PATH = $GOROOT /bin: $PATH rm -rf go1.14.4.linux-amd64.tar.gz","title":"Pre-requisites"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/3Build%20Services%20and%20packages/#building","text":"","title":"Building"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/3Build%20Services%20and%20packages/#foundational-security-usecase","text":"Sync the repo mkdir -p /root/intel-secl/build/fs && cd /root/intel-secl/build/fs repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/fs.xml repo sync Run the pre-requisites setup script cd utils/build/foundational-security/ chmod +x fs-prereq.sh ./fs-prereq.sh -s Build all repos cd /root/intel-secl/build/fs/ make binaries Built Binaries /root/intel-secl/build/fs/binaries","title":"Foundational Security Usecase"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/3Build%20Services%20and%20packages/#workload-security-usecase","text":"","title":"Workload Security Usecase"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/3Build%20Services%20and%20packages/#vm-confidentiality","text":"Sync the repo mkdir -p /root/intel-secl/build/vmc && cd /root/intel-secl/build/vmc repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/vmc.xml repo sync Run the pre-req script cd utils/build/workload-security chmod +x ws-prereq.sh ./ws-prereq.sh -v Build repo cd /root/intel-secl/build/vmc/ make all Built Binaries /root/intel-secl/build/vmc/binaries/","title":"VM Confidentiality"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/3Build%20Services%20and%20packages/#container-confidentiality-with-crio-runtime","text":"Sync the repo mkdir -p /root/intel-secl/build/cc-crio && cd /root/intel-secl/build/cc-crio repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/cc-crio.xml repo sync Run the pre-requisites script cd utils/build/workload-security chmod +x ws-prereq.sh ./ws-prereq.sh -c Enable and start the Docker daemon systemctl enable docker systemctl start docker Ignore the below steps if not running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" #Reload docker systemctl daemon-reload systemctl restart docker Download go dependencies cd /root/ go get github.com/cpuguy83/go-md2man mv /root/go/bin/go-md2man /usr/bin/ Build the repos cd /root/intel-secl/build/cc-crio make binaries Note The crio use case uses containerd that is bundled with docker-ce-19.03.13 during build time. As of this release , the version being used is containerd-1.4.4 . If the remote docker-ce repo gets updated for newer containerd version, then the version of containerd might be incompatible for building crio use case. It is recommended to use the version 1.4.4 in that case. Built binaries /root/intel-secl/build/cc-crio/binaries/","title":"Container Confidentiality with CRIO Runtime"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/","text":"Deployment The below details would enable the deployment through Ansible Role for Intel\u00ae SecL-DC Foundational & Workload Security Usecases. However the services can still be installed manually using the Product Guide. More details on Ansible Role for Intel\u00ae SecL-DC in Ansible-Role repository. Pre-requisites The Ansible Server is required to use this role to deploy Intel\u00ae SecL-DC services based on the supported deployment model. The Ansible server is recommended to be installed on the Build machine itself. The role has been tested with Ansible Version 2.9.10 Installing Ansible Install Ansible on Build Machine pip3 install ansible == 2 .9.10 Install epel-release repository and install sshpass for ansible to connect to remote hosts using SSH dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm dnf install sshpass Create directory for ansible default configuration and hosts file mkdir -p /etc/ansible/ touch /etc/ansible/ansible.cfg Copy the default ansible.cfg contents from https://raw.githubusercontent.com/ansible/ansible/v2.9.10/examples/ansible.cfg and paste it under /etc/ansible/ansible.cfg Download the Ansible Role The role can be cloned locally from git and the contents can be copied to the roles folder used by your ansible server #Create directory for using ansible deployment mkdir -p /root/intel-secl/deploy/ #Clone the repository cd /root/intel-secl/deploy/ && git clone https://github.com/intel-secl/utils.git #Checkout to specific release-version cd utils/ git checkout <release-version of choice> cd tools/ansible-role #Update ansible.cfg roles_path to point to path(/root/intel-secl/deploy/utils/tools/) Usecase Setup Options Usecase Variable Host Attestation setup: host-attestation in playbook or via --extra-vars as setup=host-attestation in CLI Application Integrity setup: application-integrity in playbook or via --extra-vars as setup=application-integrity in CLI Data Fencing & Asset Tags setup: data-fencing in playbook or via --extra-vars as setup=data-fencing in CLI Trusted Workload Placement - VM setup: trusted-workload-placement-vm in playbook or via --extra-vars as setup=trusted-workload-placement-vm in CLI Trusted Workload Placement - Containers setup: trusted-workload-placement-containers in playbook or via --extra-vars as setup=trusted-workload-placement-containers in CLI Launch Time Protection - VM Confidentiality setup: workload-conf-vm in playbook or via --extra-vars as setup=workload-conf-vm in CLI Launch Time Protection - Container Confidentiality with CRIO Runtime setup: workload-conf-containers-crio in playbook or via --extra-vars as setup=workload-conf-containers-crio in CLI Note Orchestrator installation is not bundled with the role and need to be done independently. Also, components dependent on the orchestrator like isecl-k8s-extensions and integration-hub are installed either partially or not installed Note Key Broker Service is not configured with KMIP compliant KMS when installing through ansible role Update Ansible Inventory In order to deploy Intel\u00ae SecL-DC binaries, the following inventory can be used and the required inventory vars as below need to be set. The below example inventory can be created under /etc/ansible/hosts [CSP] <machine1_ip/hostname> [Enterprise] <machine2_ip/hostname> [Node] <machine3_ip/hostname> [CSP:vars] isecl_role=csp ansible_user=root ansible_password=<password> [Enterprise:vars] isecl_role=enterprise ansible_user=root ansible_password=<password> [Node:vars] isecl_role=node ansible_user=root ansible_password=<password> Note Ansible requires ssh and root user access to remote machines. The following command can be used to ensure ansible can connect to remote machines with host key check. Ensure the existing keys of the machines are cleared to enable fresh keyscan. ssh-keyscan -H <ip_address/hostname> >> /root/.ssh/known_hosts Create and Run Playbook The following are playbook and CLI example for deploying Intel\u00ae SecL-DC binaries based on the supported deployment models and usecases. The below example playbooks can be created as site-bin-isecl.yml Note If running behind a proxy, update the proxy variables under vars/main.yml and run as below Note Go through the Additional Examples and Tips section for specific workflow samples Option 1 - hosts : all gather_facts : yes any_errors_fatal : true vars : setup : <setup var from supported usecases> binaries_path : <path where built binaries are copied to> roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> OR Option 2: - hosts : all gather_facts : yes any_errors_fatal : true roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> \\ --extra-vars setup = <setup var from supported usecases> \\ --extra-vars binaries_path = <path where built binaries are copied to> Additional Examples & Tips TBoot Installation Tboot needs to be built by the user from tboot source and the tboot.gz & tboot-syms files needs to be copied under the binaries folder. The supported version of Tboot as of 4.0 release is tboot-1.10.1 .The options must then be provided during runtime in the playbook: ansible-playbook <playbook-name> \\ --extra-vars setup = <setup var from supported usecases> \\ --extra-vars binaries_path = <path where built binaries are copied to> \\ --extra-vars tboot_gz_file = <path where built binaries are copied to>/tboot.gz --extra-vars tboot_syms_file = <path where built binaries are copied to>/tboot-syms or Update the following in vars/main.yml # The TPM Storage Root Key(SRK) Password to be used if TPM is already owned tboot_gz_file : \"<binaries_path>/tboot.gz\" tboot_syms_file : \"<binaries_path>/tboot-syms\" TPM is already owned If the Trusted Platform Module(TPM) is already owned, the owner secret(SRK) can be provided directly during runtime in the playbook: ansible-playbook <playbook-name> \\ --extra-vars setup = <setup var from supported usecases> \\ --extra-vars binaries_path = <path where built binaries are copied to> \\ --extra-vars tpm_secret = <tpm owner secret> or Update the following vars in vars/main.yml # The TPM Storage Root Key(SRK) Password to be used if TPM is already owned tpm_owner_secret : <tpm_secret> UEFI SecureBoot enabled If UEFI mode and UEFI SecureBoot feature is enabled, the following option can be used to during runtime in the playbook ansible-playbook <playbook-name> \\ --extra-vars setup = <setup var from supported usecases> \\ --extra-vars binaries_path = <path where built binaries are copied to> \\ --extra-vars uefi_secureboot = yes \\ --extra-vars grub_file_path = <uefi mode grub file path> or Update the following vars in vars/main.yml # UEFI mode or UEFI SecureBoot mode # ['no' - UEFI mode, 'yes' - UEFI SecureBoot mode] uefi_secureboot : 'yes' # The grub file path for UEFI Mode systems # [/boot/efi/EFI/redhat/grub.cfg - UEFI Mode] grub_file_path : /boot/efi/EFI/redhat/grub.cfg In case of Misconfigurations If any service installation fails due to any misconfiguration, just uninstall the specific service manually , fix the misconfiguration in ansible and rerun the playbook. The successfully installed services wont be reinstalled.","title":"Deployment"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#deployment","text":"The below details would enable the deployment through Ansible Role for Intel\u00ae SecL-DC Foundational & Workload Security Usecases. However the services can still be installed manually using the Product Guide. More details on Ansible Role for Intel\u00ae SecL-DC in Ansible-Role repository.","title":"Deployment"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#pre-requisites","text":"The Ansible Server is required to use this role to deploy Intel\u00ae SecL-DC services based on the supported deployment model. The Ansible server is recommended to be installed on the Build machine itself. The role has been tested with Ansible Version 2.9.10","title":"Pre-requisites"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#installing-ansible","text":"Install Ansible on Build Machine pip3 install ansible == 2 .9.10 Install epel-release repository and install sshpass for ansible to connect to remote hosts using SSH dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm dnf install sshpass Create directory for ansible default configuration and hosts file mkdir -p /etc/ansible/ touch /etc/ansible/ansible.cfg Copy the default ansible.cfg contents from https://raw.githubusercontent.com/ansible/ansible/v2.9.10/examples/ansible.cfg and paste it under /etc/ansible/ansible.cfg","title":"Installing Ansible"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#download-the-ansible-role","text":"The role can be cloned locally from git and the contents can be copied to the roles folder used by your ansible server #Create directory for using ansible deployment mkdir -p /root/intel-secl/deploy/ #Clone the repository cd /root/intel-secl/deploy/ && git clone https://github.com/intel-secl/utils.git #Checkout to specific release-version cd utils/ git checkout <release-version of choice> cd tools/ansible-role #Update ansible.cfg roles_path to point to path(/root/intel-secl/deploy/utils/tools/)","title":"Download the Ansible Role"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#usecase-setup-options","text":"Usecase Variable Host Attestation setup: host-attestation in playbook or via --extra-vars as setup=host-attestation in CLI Application Integrity setup: application-integrity in playbook or via --extra-vars as setup=application-integrity in CLI Data Fencing & Asset Tags setup: data-fencing in playbook or via --extra-vars as setup=data-fencing in CLI Trusted Workload Placement - VM setup: trusted-workload-placement-vm in playbook or via --extra-vars as setup=trusted-workload-placement-vm in CLI Trusted Workload Placement - Containers setup: trusted-workload-placement-containers in playbook or via --extra-vars as setup=trusted-workload-placement-containers in CLI Launch Time Protection - VM Confidentiality setup: workload-conf-vm in playbook or via --extra-vars as setup=workload-conf-vm in CLI Launch Time Protection - Container Confidentiality with CRIO Runtime setup: workload-conf-containers-crio in playbook or via --extra-vars as setup=workload-conf-containers-crio in CLI Note Orchestrator installation is not bundled with the role and need to be done independently. Also, components dependent on the orchestrator like isecl-k8s-extensions and integration-hub are installed either partially or not installed Note Key Broker Service is not configured with KMIP compliant KMS when installing through ansible role","title":"Usecase Setup Options"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#update-ansible-inventory","text":"In order to deploy Intel\u00ae SecL-DC binaries, the following inventory can be used and the required inventory vars as below need to be set. The below example inventory can be created under /etc/ansible/hosts [CSP] <machine1_ip/hostname> [Enterprise] <machine2_ip/hostname> [Node] <machine3_ip/hostname> [CSP:vars] isecl_role=csp ansible_user=root ansible_password=<password> [Enterprise:vars] isecl_role=enterprise ansible_user=root ansible_password=<password> [Node:vars] isecl_role=node ansible_user=root ansible_password=<password> Note Ansible requires ssh and root user access to remote machines. The following command can be used to ensure ansible can connect to remote machines with host key check. Ensure the existing keys of the machines are cleared to enable fresh keyscan. ssh-keyscan -H <ip_address/hostname> >> /root/.ssh/known_hosts","title":"Update Ansible Inventory"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#create-and-run-playbook","text":"The following are playbook and CLI example for deploying Intel\u00ae SecL-DC binaries based on the supported deployment models and usecases. The below example playbooks can be created as site-bin-isecl.yml Note If running behind a proxy, update the proxy variables under vars/main.yml and run as below Note Go through the Additional Examples and Tips section for specific workflow samples Option 1 - hosts : all gather_facts : yes any_errors_fatal : true vars : setup : <setup var from supported usecases> binaries_path : <path where built binaries are copied to> roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> OR Option 2: - hosts : all gather_facts : yes any_errors_fatal : true roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> \\ --extra-vars setup = <setup var from supported usecases> \\ --extra-vars binaries_path = <path where built binaries are copied to>","title":"Create and Run Playbook"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#additional-examples-tips","text":"","title":"Additional Examples &amp; Tips"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#tboot-installation","text":"Tboot needs to be built by the user from tboot source and the tboot.gz & tboot-syms files needs to be copied under the binaries folder. The supported version of Tboot as of 4.0 release is tboot-1.10.1 .The options must then be provided during runtime in the playbook: ansible-playbook <playbook-name> \\ --extra-vars setup = <setup var from supported usecases> \\ --extra-vars binaries_path = <path where built binaries are copied to> \\ --extra-vars tboot_gz_file = <path where built binaries are copied to>/tboot.gz --extra-vars tboot_syms_file = <path where built binaries are copied to>/tboot-syms or Update the following in vars/main.yml # The TPM Storage Root Key(SRK) Password to be used if TPM is already owned tboot_gz_file : \"<binaries_path>/tboot.gz\" tboot_syms_file : \"<binaries_path>/tboot-syms\"","title":"TBoot Installation"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#tpm-is-already-owned","text":"If the Trusted Platform Module(TPM) is already owned, the owner secret(SRK) can be provided directly during runtime in the playbook: ansible-playbook <playbook-name> \\ --extra-vars setup = <setup var from supported usecases> \\ --extra-vars binaries_path = <path where built binaries are copied to> \\ --extra-vars tpm_secret = <tpm owner secret> or Update the following vars in vars/main.yml # The TPM Storage Root Key(SRK) Password to be used if TPM is already owned tpm_owner_secret : <tpm_secret>","title":"TPM is already owned"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#uefi-secureboot-enabled","text":"If UEFI mode and UEFI SecureBoot feature is enabled, the following option can be used to during runtime in the playbook ansible-playbook <playbook-name> \\ --extra-vars setup = <setup var from supported usecases> \\ --extra-vars binaries_path = <path where built binaries are copied to> \\ --extra-vars uefi_secureboot = yes \\ --extra-vars grub_file_path = <uefi mode grub file path> or Update the following vars in vars/main.yml # UEFI mode or UEFI SecureBoot mode # ['no' - UEFI mode, 'yes' - UEFI SecureBoot mode] uefi_secureboot : 'yes' # The grub file path for UEFI Mode systems # [/boot/efi/EFI/redhat/grub.cfg - UEFI Mode] grub_file_path : /boot/efi/EFI/redhat/grub.cfg","title":"UEFI SecureBoot enabled"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/4Deployment/#in-case-of-misconfigurations","text":"If any service installation fails due to any misconfiguration, just uninstall the specific service manually , fix the misconfiguration in ansible and rerun the playbook. The successfully installed services wont be reinstalled.","title":"In case of Misconfigurations"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/5Usecase%20Workflows%20API%20Collections/","text":"Usecase Workflows API Collections The below allow to get started with workflows within Intel\u00ae SecL-DC for Foundational and Workload Security Usecases. More details available in API Collections repository Pre-requisites Postman client should be downloaded on supported platforms or on the web to get started with the usecase collections. Note The Postman API Network will always have the latest released version of the API Collections. For all releases, refer the github repository for API Collections Use Case Collections Use case Sub-Usecase API Collection Foundational Security Host Attestation(RHEL & VMWARE) \u2714\ufe0f Data Fencing with Asset Tags(RHEL & VMWARE) \u2714\ufe0f Trusted Workload Placement (VM & Containers) \u2714\ufe0f Application Integrity \u2714\ufe0f Launch Time Protection VM Confidentiality \u2714\ufe0f Container Confidentiality with CRIO Runtime \u2714\ufe0f Note Foundational Security - Host Attestation is a pre-requisite for all usecases beyond Host Attestation. E.g: For working with Launch Time Protection - VM Confidentiality , Host Attestation flow must be run as a pre-req before trying VM Confidentiality Downloading API Collections Postman API Network for latest released: https://explore.postman.com/intelsecldc or Github repo for all releases #Clone the github repo for api-collections git clone https://github.com/intel-secl/utils.git #Switch to specific release-version of choice cd utils/ git checkout <release-version of choice> #Import Collections from cd tools/api-collections Note The postman-collections are also available when cloning the repos via build manifest under utils/tools/api-collections Running API Collections Import the collection into Postman API Client Note This step is required only when not using Postman API Network and downloading from Github Update env as per the deployment details for specific usecase View Documentation Run the workflow","title":"Usecase Workflows API Collections"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/5Usecase%20Workflows%20API%20Collections/#usecase-workflows-api-collections","text":"The below allow to get started with workflows within Intel\u00ae SecL-DC for Foundational and Workload Security Usecases. More details available in API Collections repository","title":"Usecase Workflows API Collections"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/5Usecase%20Workflows%20API%20Collections/#pre-requisites","text":"Postman client should be downloaded on supported platforms or on the web to get started with the usecase collections. Note The Postman API Network will always have the latest released version of the API Collections. For all releases, refer the github repository for API Collections","title":"Pre-requisites"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/5Usecase%20Workflows%20API%20Collections/#use-case-collections","text":"Use case Sub-Usecase API Collection Foundational Security Host Attestation(RHEL & VMWARE) \u2714\ufe0f Data Fencing with Asset Tags(RHEL & VMWARE) \u2714\ufe0f Trusted Workload Placement (VM & Containers) \u2714\ufe0f Application Integrity \u2714\ufe0f Launch Time Protection VM Confidentiality \u2714\ufe0f Container Confidentiality with CRIO Runtime \u2714\ufe0f Note Foundational Security - Host Attestation is a pre-requisite for all usecases beyond Host Attestation. E.g: For working with Launch Time Protection - VM Confidentiality , Host Attestation flow must be run as a pre-req before trying VM Confidentiality","title":"Use Case Collections"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/5Usecase%20Workflows%20API%20Collections/#downloading-api-collections","text":"Postman API Network for latest released: https://explore.postman.com/intelsecldc or Github repo for all releases #Clone the github repo for api-collections git clone https://github.com/intel-secl/utils.git #Switch to specific release-version of choice cd utils/ git checkout <release-version of choice> #Import Collections from cd tools/api-collections Note The postman-collections are also available when cloning the repos via build manifest under utils/tools/api-collections","title":"Downloading API Collections"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/5Usecase%20Workflows%20API%20Collections/#running-api-collections","text":"Import the collection into Postman API Client Note This step is required only when not using Postman API Network and downloading from Github Update env as per the deployment details for specific usecase View Documentation Run the workflow","title":"Running API Collections"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/6Appendix/","text":"Appendix Running behind Proxy #Set proxy in ~/.bash_profile export http_proxy = <proxy-url> export https_proxy = <proxy-url> export no_proxy = <ip_address/hostname> Git Config Sample (~/.gitconfig) [user] name = <username> email = <email-id> [color] ui = auto [push] default = matching Rebuilding Repos In order to rebuild repos, ensure the following steps are followed as a pre-requisite # Clean all go-mod packages rm -rf ~/go/pkg/mod/* #Navigate to specific folder where repos are built, example below cd /root/isec/fs rm -rf * .repo Installing the Intel\u00ae SecL Kubernetes Extensions and Integration Hub Intel\u00ae SecL uses Custom Resource Definitions to add the ability to base orchestration decisions on Intel\u00ae SecL security attributes to Kubernetes. These CRDs allow Kubernetes administrators to configure pods to require specific security attributes so that the Kubernetes Control Plane Node will schedule those pods only on Worker Nodes that match the specified attributes. Two CRDs are required for integration with Intel\u00ae SecL \u2013 an extension for the Control Plane nodes, and a scheduler extension. The extensions are deployed as a Kubernetes deployment in the isecl namespace. Deploy Intel\u00ae SecL Custom Controller Copy isecl-k8s-extensions-*.tar.gz to Kubernetes Control plane machine and extract the contents #Copy scp /<build_path>/binaries/isecl-k8s-extensions-*.tar.gz <user>@<k8s_master_machine>:/<path>/ #Extract tar -xvzf /<path>/isecl-k8s-extensions-*.tar.gz cd /<path>/isecl-k8s-extensions/ Create hostattributes.crd.isecl.intel.com CRD #1.14<=k8s_version<=1.16 kubectl apply -f yamls/crd-1.14.yaml #1.16<=k8s_version<=1.18 kubectl apply -f yamls/crd-1.17.yaml Check whether the CRD is created kubectl get crds Load the isecl-controller docker image #Install skopeo to load docker image for controller and scheduler from archive dnf install -y skopeo #Push image to registry cd /<path>/isecl-k8s-extensions/ skopeo copy oci-archive:<isecl-k8s-controller-*.tar> docker://<docker_private_registry_server>:5000/<imageName>:<tagName> Update image name as above in /opt/isecl-k8s-extensions/yamls/isecl-controller.yaml containers: - name: isecl-controller image: <docker_private_registry_server>:5000/<imageName>:<tagName> Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterRoleBinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole = system:node --user = system:serviceaccount:isecl:isecl Fetch token required for ihub installation kubectl get secrets -n isecl #The below token will be used for ihub installation when configured with Kubernetes Tenant kubectl describe secret default-token-<name> -n isecl Additional Optional Configurable fields for isecl-controller configuration in isecl-controller.yaml Field Required Type Default Description LOG_LEVEL Optional string INFO Determines the log level LOG_MAX_LENGTH Optional int 1500 Determines the maximum length of characters in a line in log file TAG_PREFIX Optional string isecl A custom prefix which can be applied to isecl attributes that are pushed from IH. For example, if the tag-prefix is isecl. and trusted attribute in CRD becomes isecl.trusted . TAINT_UNTRUSTED_NODES Optional string false If set to true. NoExec taint applied to the nodes for which trust status is set to false, Applicable only for HVS based attestation Installing the Intel\u00ae SecL Integration Hub Copy the API Server certificate of K8s Master to machine where Integration Hub will be installed to /root/ directory Note In most Kubernetes distributions the Kubernetes certificate and key is normally present under /etc/kubernetes/pki . However this might differ in case of some specific Kubernetes distributions. Update the token obtained in Step 8 of Deploy Intel\u00ae SecL Custom Controller along with other relevant tenant configuration options in ihub.env Install Integration Hub Copy the /etc/ihub/ihub_public_key.pem to Kubernetes Master machine to /<path>/secrets/ directory #On K8s-Master machine mkdir -p /<path>/secrets #On IHUB machine, copy scp /etc/ihub/ihub_public_key.pem <user>@<k8s_master_machine>:/<path>/secrets/hvs_ihub_public_key.pem Deploy Intel\u00ae SecL Extended Scheduler Install cfssl and cfssljson on Kubernetes Control Plane #Install wget dnf install wget -y #Download cfssl to /usr/local/bin/ wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl #Download cfssljson to /usr/local/bin wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create TLS key-pair for isecl-scheduler service which is signed by Kubernetes apiserver.crt cd /<path>/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh #Set K8s_MASTER_IP,HOSTNAME export MASTER_IP = <k8s_machine_ip> export HOSTNAME = <k8s_machine_hostname> #Create TLS key-pair ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \" $MASTER_IP \" , \" $HOSTNAME \" -c <k8s_ca_authority_cert> -k <k8s_ca_authority_key> Note In most Kubernetes distributions the Kubernetes certificate and key is normally present under /etc/kubernetes/pki . However this might differ in case of some specific Kubernetes distributions. Copy the TLS key-pair generated to /<path>/secrets/ directory cp /<path>/isecl-k8s-extensions/server.key /<path>/secrets/ cp /<path>/isecl-k8s-extensions/server.crt /<path>/secrets/ Load the isecl-scheduler docker image #Push image to registry cd /<path>/isecl-k8s-extensions/ skopeo copy oci-archive:<isecl-k8s-scheduler-*.tar> docker://<docker_private_registry_server>:5000/<imageName>:<tagName> Update image name as above in /opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml containers: - name: isecl-scheduler image: <docker_private_registry_server>:5000/<imageName>:<tagName> Create scheduler-secret for isecl-scheduler cd /<path>/ kubectl create secret generic scheduler-certs --namespace isecl --from-file = secrets The isecl-scheduler.yaml file includes support for both SGX and Workload Security put together. For only working with Workload Security scenarios , the following line needs to be made empty in the yaml file. The scheduler and controller yaml files are located under /<path>/isecl-k8s-extensions/yamls - name : SGX_IHUB_PUBLIC_KEY_PATH value : \"\" Deploy isecl-scheduler cd /<path>/isecl-k8s-extensions/ kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl Additional optional fields for isecl-scheduler configuration in isecl-scheduler.yaml Field Required Type Default Description LOG_LEVEL Optional string INFO Determines the log level LOG_MAX_LENGTH Optional int 1500 Determines the maximum length of characters in a line in log file TAG_PREFIX Optional string isecl. A custom prefix which can be applied to isecl attributes that are pushed from IH. For example, if the tag-prefix is *isecl.* and *trusted* attribute in CRD becomes *isecl.trusted* . PORT Optional int 8888 ISecl scheduler service port HVS_IHUB_PUBLIC_KEY_PATH Required string Required for IHub with HVS Attestation SGX_IHUB_PUBLIC_KEY_PATH Required string Required for IHub with SGX Attestation TLS_CERT_PATH Required string Path of tls certificate signed by kubernetes CA TLS_KEY_PATH Required string Path of tls key Configuring kube-scheduler to establish communication with isecl-scheduler Note The below is a sample when using kubeadm as the Kubernetes distribution, the scheduler configuration files would be different for any other Kubernetes distributions being used. Add a mount path to the /etc/kubernetes/manifests/kube-scheduler.yaml file for the Intel SecL scheduler extension: - mountPath : /<path>/isecl-k8s-extensions/ name : extendedsched readOnly : true Add a volume path to the /etc/kubernetes/manifests/kube-scheduler.yaml file for the Intel SecL scheduler extension: - hostPath : path : /<path>/isecl-k8s-extensions/ type : \"\" name : extendedsched Add policy-config-file path in the /etc/kubernetes/manifests/kube-scheduler.yaml file under command section: - command : - kube-scheduler - --policy-config-file=/<path>/isecl-k8s-extensions/scheduler-policy.json - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=true Restart kubelet systemctl restart kubelet","title":"Appendix"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/6Appendix/#appendix","text":"","title":"Appendix"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/6Appendix/#running-behind-proxy","text":"#Set proxy in ~/.bash_profile export http_proxy = <proxy-url> export https_proxy = <proxy-url> export no_proxy = <ip_address/hostname>","title":"Running behind Proxy"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/6Appendix/#git-config-sample-gitconfig","text":"[user] name = <username> email = <email-id> [color] ui = auto [push] default = matching","title":"Git Config Sample (~/.gitconfig)"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/6Appendix/#rebuilding-repos","text":"In order to rebuild repos, ensure the following steps are followed as a pre-requisite # Clean all go-mod packages rm -rf ~/go/pkg/mod/* #Navigate to specific folder where repos are built, example below cd /root/isec/fs rm -rf * .repo","title":"Rebuilding Repos"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/6Appendix/#installing-the-intel-secl-kubernetes-extensions-and-integration-hub","text":"Intel\u00ae SecL uses Custom Resource Definitions to add the ability to base orchestration decisions on Intel\u00ae SecL security attributes to Kubernetes. These CRDs allow Kubernetes administrators to configure pods to require specific security attributes so that the Kubernetes Control Plane Node will schedule those pods only on Worker Nodes that match the specified attributes. Two CRDs are required for integration with Intel\u00ae SecL \u2013 an extension for the Control Plane nodes, and a scheduler extension. The extensions are deployed as a Kubernetes deployment in the isecl namespace.","title":"Installing the Intel\u00ae SecL Kubernetes Extensions and Integration Hub"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/6Appendix/#deploy-intel-secl-custom-controller","text":"Copy isecl-k8s-extensions-*.tar.gz to Kubernetes Control plane machine and extract the contents #Copy scp /<build_path>/binaries/isecl-k8s-extensions-*.tar.gz <user>@<k8s_master_machine>:/<path>/ #Extract tar -xvzf /<path>/isecl-k8s-extensions-*.tar.gz cd /<path>/isecl-k8s-extensions/ Create hostattributes.crd.isecl.intel.com CRD #1.14<=k8s_version<=1.16 kubectl apply -f yamls/crd-1.14.yaml #1.16<=k8s_version<=1.18 kubectl apply -f yamls/crd-1.17.yaml Check whether the CRD is created kubectl get crds Load the isecl-controller docker image #Install skopeo to load docker image for controller and scheduler from archive dnf install -y skopeo #Push image to registry cd /<path>/isecl-k8s-extensions/ skopeo copy oci-archive:<isecl-k8s-controller-*.tar> docker://<docker_private_registry_server>:5000/<imageName>:<tagName> Update image name as above in /opt/isecl-k8s-extensions/yamls/isecl-controller.yaml containers: - name: isecl-controller image: <docker_private_registry_server>:5000/<imageName>:<tagName> Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterRoleBinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole = system:node --user = system:serviceaccount:isecl:isecl Fetch token required for ihub installation kubectl get secrets -n isecl #The below token will be used for ihub installation when configured with Kubernetes Tenant kubectl describe secret default-token-<name> -n isecl Additional Optional Configurable fields for isecl-controller configuration in isecl-controller.yaml Field Required Type Default Description LOG_LEVEL Optional string INFO Determines the log level LOG_MAX_LENGTH Optional int 1500 Determines the maximum length of characters in a line in log file TAG_PREFIX Optional string isecl A custom prefix which can be applied to isecl attributes that are pushed from IH. For example, if the tag-prefix is isecl. and trusted attribute in CRD becomes isecl.trusted . TAINT_UNTRUSTED_NODES Optional string false If set to true. NoExec taint applied to the nodes for which trust status is set to false, Applicable only for HVS based attestation","title":"Deploy Intel\u00ae SecL Custom Controller"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/6Appendix/#installing-the-intel-secl-integration-hub","text":"Copy the API Server certificate of K8s Master to machine where Integration Hub will be installed to /root/ directory Note In most Kubernetes distributions the Kubernetes certificate and key is normally present under /etc/kubernetes/pki . However this might differ in case of some specific Kubernetes distributions. Update the token obtained in Step 8 of Deploy Intel\u00ae SecL Custom Controller along with other relevant tenant configuration options in ihub.env Install Integration Hub Copy the /etc/ihub/ihub_public_key.pem to Kubernetes Master machine to /<path>/secrets/ directory #On K8s-Master machine mkdir -p /<path>/secrets #On IHUB machine, copy scp /etc/ihub/ihub_public_key.pem <user>@<k8s_master_machine>:/<path>/secrets/hvs_ihub_public_key.pem","title":"Installing the Intel\u00ae SecL Integration Hub"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/6Appendix/#deploy-intel-secl-extended-scheduler","text":"Install cfssl and cfssljson on Kubernetes Control Plane #Install wget dnf install wget -y #Download cfssl to /usr/local/bin/ wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl #Download cfssljson to /usr/local/bin wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create TLS key-pair for isecl-scheduler service which is signed by Kubernetes apiserver.crt cd /<path>/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh #Set K8s_MASTER_IP,HOSTNAME export MASTER_IP = <k8s_machine_ip> export HOSTNAME = <k8s_machine_hostname> #Create TLS key-pair ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \" $MASTER_IP \" , \" $HOSTNAME \" -c <k8s_ca_authority_cert> -k <k8s_ca_authority_key> Note In most Kubernetes distributions the Kubernetes certificate and key is normally present under /etc/kubernetes/pki . However this might differ in case of some specific Kubernetes distributions. Copy the TLS key-pair generated to /<path>/secrets/ directory cp /<path>/isecl-k8s-extensions/server.key /<path>/secrets/ cp /<path>/isecl-k8s-extensions/server.crt /<path>/secrets/ Load the isecl-scheduler docker image #Push image to registry cd /<path>/isecl-k8s-extensions/ skopeo copy oci-archive:<isecl-k8s-scheduler-*.tar> docker://<docker_private_registry_server>:5000/<imageName>:<tagName> Update image name as above in /opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml containers: - name: isecl-scheduler image: <docker_private_registry_server>:5000/<imageName>:<tagName> Create scheduler-secret for isecl-scheduler cd /<path>/ kubectl create secret generic scheduler-certs --namespace isecl --from-file = secrets The isecl-scheduler.yaml file includes support for both SGX and Workload Security put together. For only working with Workload Security scenarios , the following line needs to be made empty in the yaml file. The scheduler and controller yaml files are located under /<path>/isecl-k8s-extensions/yamls - name : SGX_IHUB_PUBLIC_KEY_PATH value : \"\" Deploy isecl-scheduler cd /<path>/isecl-k8s-extensions/ kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl Additional optional fields for isecl-scheduler configuration in isecl-scheduler.yaml Field Required Type Default Description LOG_LEVEL Optional string INFO Determines the log level LOG_MAX_LENGTH Optional int 1500 Determines the maximum length of characters in a line in log file TAG_PREFIX Optional string isecl. A custom prefix which can be applied to isecl attributes that are pushed from IH. For example, if the tag-prefix is *isecl.* and *trusted* attribute in CRD becomes *isecl.trusted* . PORT Optional int 8888 ISecl scheduler service port HVS_IHUB_PUBLIC_KEY_PATH Required string Required for IHub with HVS Attestation SGX_IHUB_PUBLIC_KEY_PATH Required string Required for IHub with SGX Attestation TLS_CERT_PATH Required string Path of tls certificate signed by kubernetes CA TLS_KEY_PATH Required string Path of tls key","title":"Deploy Intel\u00ae SecL Extended Scheduler"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security/6Appendix/#configuring-kube-scheduler-to-establish-communication-with-isecl-scheduler","text":"Note The below is a sample when using kubeadm as the Kubernetes distribution, the scheduler configuration files would be different for any other Kubernetes distributions being used. Add a mount path to the /etc/kubernetes/manifests/kube-scheduler.yaml file for the Intel SecL scheduler extension: - mountPath : /<path>/isecl-k8s-extensions/ name : extendedsched readOnly : true Add a volume path to the /etc/kubernetes/manifests/kube-scheduler.yaml file for the Intel SecL scheduler extension: - hostPath : path : /<path>/isecl-k8s-extensions/ type : \"\" name : extendedsched Add policy-config-file path in the /etc/kubernetes/manifests/kube-scheduler.yaml file under command section: - command : - kube-scheduler - --policy-config-file=/<path>/isecl-k8s-extensions/scheduler-policy.json - --bind-address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=true Restart kubelet systemctl restart kubelet","title":"Configuring kube-scheduler to establish communication with isecl-scheduler"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/10Appendix/","text":"Appendix Hardware feature detection For checking whether a system is SUEFI enabled bootctl | grep \"Secure Boot: enabled\" For checking system installed with tboot txt-stat | grep \"TXT measured launch: TRUE\" Running behind Proxy #Set proxy in ~/.bash_profile export http_proxy = <proxy-url> export https_proxy = <proxy-url> export no_proxy = <ip_address/hostname> Git Config Sample (~/.gitconfig) [user] name = <username> email = <email-id> [color] ui = auto [push] default = matching Rebuilding Repos In order to rebuild repos, ensure the following steps are followed as a pre-requisite # Clean all go-mod packages rm -rf ~/go/pkg/mod/* #Navigate to specific folder where repos are built, example below cd /root/isecl/build/fs rm -rf .repo * #Rebuild as before repo init ... repo sync make ... Setup Task Flow Setup tasks flows have been updated to have K8s native flow to be more agnostic to K8s workflows. Following would be the flow for setup task User would create a new configMap object with the environment variables specific to the setup task. The Setup task variables would be documented in the Product Guide Users can provide variables for more than one setup task Users need to add SETUP_TASK: \"<setup task name>/<comma separated setup task name>\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f - Configuration Update Flow Configuration Update flows have been updated to have K8s native flow to be more agnostic to K8s workflows using configMap only. Following would be the flow for configuration update User would create a new configMap object using existing one and update the new values. The list of config variables would be documented in the Product Guide Users can update variables for more than one Users need to add SETUP_TASK: \"update-service-config\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f - Note Incase of agents, setup tasks or configuration updates done through above flows will be applied for all the agents running on different BMs. In order to run setup task or update configuration for individual agents, then user need to perform kubectl exec -it <pod_name> /bin/bash into a particular agent pod and run the specific setup task. Cleanup workflows Single-node In order to cleanup and setup fresh again on single node without data, config from previous deployment #Purge all data and pods,deploy,cm,secrets ./isecl-bootstrap.sh purge <all/usecase of choice> #Purge all db data,pods,deploy,cm,secrets ./isecl-bootstrap-db-services.sh purge #Comment/Remove the following lines from /var/snap/microk8s/current/args/kube-scheduler --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service #Setup fresh ./isecl-bootstrap-db-services.sh up ./isecl-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service In order to cleanup and setup fresh again on single node with data, config from previous deployment #Down all data and pods,deploy,cm,secrets with deleting config,data,logs ./isecl-bootstrap.sh down <all/usecase of choice> #Comment/Remove the following lines from /var/snap/microk8s/current/args/kube-scheduler --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service #Setup fresh ./isecl-bootstrap-db-services.sh up ./isecl-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service In order to clean single service and bring up again on single node without data, config from previous deployment ./isecl-bootstrap.sh down <service-name> rm -rf /etc/<service-name> rm -rf /var/log/<service-name> #Only in case of KBS, perform one more step along with above 2 steps rm -rf /opt/kbs ./isecl-bootstrap.sh up <service-name> In order to redeploy again on single node with data, config from previous deployment ./isecl-bootstrap.sh down <service-name> ./isecl-bootstrap.sh up <service-name> Multi-node In order to cleanup and setup fresh again on multi-node with data,config from previous deployment #Purge all data and pods,deploy,cm,secrets ./isecl-bootstrap.sh down <all/usecase of choice> #Delete 'scheduler-policy.json' rm -rf /opt/isecl-k8s-extensions #Comment/Remove '--policy-config-file=...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Comment/Remove '- mountPath: ...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true #Comment/Remove '-hostPath:...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet #Purge all db data,pods,deploy,cm,secrets ./isecl-bootstrap-db-services.sh purge #Cleanup all data from NFS share --> User controlled #Cleanup data from each worker node --> User controlled rm -rf /etc/workload-agent rm -rf /var/log/workload-agent rm -rf /opt/trustagent rm -rf /var/log/trustagent #Setup fresh ./isecl-bootstrap-db-services.sh up ./isecl-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet In order to cleanup and setup fresh again on multi-node without removing data,config from previous deployment #Down all pods,deploy,cm,secrets with removing persistent data ./isecl-bootstrap.sh down <all/usecase of choice> #Delete 'scheduler-policy.json' rm -rf /opt/isecl-k8s-extensions #Comment/Remove '--policy-config-file=...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Comment/Remove '- mountPath: ...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true #Comment/Remove '-hostPath:...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet #Setup fresh ./isecl-bootstrap-db-services.sh up ./isecl-bootstrap.sh up <all/usecase of choice> #Setup fresh ./isecl-bootstrap-db-services.sh up ./isecl-bootstrap.sh up <all/usecase of choice> #Copy ihub_public_key.pem from <NFS-PATH>/ihub/config/ to K8s control-plane and update IHUB_PUB_KEY_PATH in isecl-isecl-k8s.env #Bootstrap isecl-scheduler ./isecl-bootstrap.sh up isecl-scheduler #Reconfigure K8s-scheduler containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet In order to clean single service and bring up again on multi node without data, config from previous deployment ./isecl-bootstrap.sh down <service-name> log into nfs system rm -rf /<nfs-mount-path>/isecl/<service-name>/config rm -rf /<nfs-mount-path>/isecl/<service-name>/logs #Only in case of KBS, perform one more step along with above 2 steps rm -rf /<nfs-mount-path>/isecl/<service-name>/opt log into K8s control-plane ./isecl-bootstrap.sh up <service-name> In order to redeploy again on multi node with data, config from previous deployment ./isecl-bootstrap.sh down <service-name> ./isecl-bootstrap.sh up <service-name>","title":"Appendix"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/10Appendix/#appendix","text":"","title":"Appendix"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/10Appendix/#hardware-feature-detection","text":"For checking whether a system is SUEFI enabled bootctl | grep \"Secure Boot: enabled\" For checking system installed with tboot txt-stat | grep \"TXT measured launch: TRUE\"","title":"Hardware feature detection"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/10Appendix/#running-behind-proxy","text":"#Set proxy in ~/.bash_profile export http_proxy = <proxy-url> export https_proxy = <proxy-url> export no_proxy = <ip_address/hostname>","title":"Running behind Proxy"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/10Appendix/#git-config-sample-gitconfig","text":"[user] name = <username> email = <email-id> [color] ui = auto [push] default = matching","title":"Git Config Sample (~/.gitconfig)"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/10Appendix/#rebuilding-repos","text":"In order to rebuild repos, ensure the following steps are followed as a pre-requisite # Clean all go-mod packages rm -rf ~/go/pkg/mod/* #Navigate to specific folder where repos are built, example below cd /root/isecl/build/fs rm -rf .repo * #Rebuild as before repo init ... repo sync make ...","title":"Rebuilding Repos"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/10Appendix/#setup-task-flow","text":"Setup tasks flows have been updated to have K8s native flow to be more agnostic to K8s workflows. Following would be the flow for setup task User would create a new configMap object with the environment variables specific to the setup task. The Setup task variables would be documented in the Product Guide Users can provide variables for more than one setup task Users need to add SETUP_TASK: \"<setup task name>/<comma separated setup task name>\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f -","title":"Setup Task Flow"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/10Appendix/#configuration-update-flow","text":"Configuration Update flows have been updated to have K8s native flow to be more agnostic to K8s workflows using configMap only. Following would be the flow for configuration update User would create a new configMap object using existing one and update the new values. The list of config variables would be documented in the Product Guide Users can update variables for more than one Users need to add SETUP_TASK: \"update-service-config\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f - Note Incase of agents, setup tasks or configuration updates done through above flows will be applied for all the agents running on different BMs. In order to run setup task or update configuration for individual agents, then user need to perform kubectl exec -it <pod_name> /bin/bash into a particular agent pod and run the specific setup task.","title":"Configuration Update Flow"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/10Appendix/#cleanup-workflows","text":"","title":"Cleanup workflows"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/10Appendix/#single-node","text":"In order to cleanup and setup fresh again on single node without data, config from previous deployment #Purge all data and pods,deploy,cm,secrets ./isecl-bootstrap.sh purge <all/usecase of choice> #Purge all db data,pods,deploy,cm,secrets ./isecl-bootstrap-db-services.sh purge #Comment/Remove the following lines from /var/snap/microk8s/current/args/kube-scheduler --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service #Setup fresh ./isecl-bootstrap-db-services.sh up ./isecl-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service In order to cleanup and setup fresh again on single node with data, config from previous deployment #Down all data and pods,deploy,cm,secrets with deleting config,data,logs ./isecl-bootstrap.sh down <all/usecase of choice> #Comment/Remove the following lines from /var/snap/microk8s/current/args/kube-scheduler --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service #Setup fresh ./isecl-bootstrap-db-services.sh up ./isecl-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service In order to clean single service and bring up again on single node without data, config from previous deployment ./isecl-bootstrap.sh down <service-name> rm -rf /etc/<service-name> rm -rf /var/log/<service-name> #Only in case of KBS, perform one more step along with above 2 steps rm -rf /opt/kbs ./isecl-bootstrap.sh up <service-name> In order to redeploy again on single node with data, config from previous deployment ./isecl-bootstrap.sh down <service-name> ./isecl-bootstrap.sh up <service-name>","title":"Single-node"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/10Appendix/#multi-node","text":"In order to cleanup and setup fresh again on multi-node with data,config from previous deployment #Purge all data and pods,deploy,cm,secrets ./isecl-bootstrap.sh down <all/usecase of choice> #Delete 'scheduler-policy.json' rm -rf /opt/isecl-k8s-extensions #Comment/Remove '--policy-config-file=...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Comment/Remove '- mountPath: ...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true #Comment/Remove '-hostPath:...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet #Purge all db data,pods,deploy,cm,secrets ./isecl-bootstrap-db-services.sh purge #Cleanup all data from NFS share --> User controlled #Cleanup data from each worker node --> User controlled rm -rf /etc/workload-agent rm -rf /var/log/workload-agent rm -rf /opt/trustagent rm -rf /var/log/trustagent #Setup fresh ./isecl-bootstrap-db-services.sh up ./isecl-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet In order to cleanup and setup fresh again on multi-node without removing data,config from previous deployment #Down all pods,deploy,cm,secrets with removing persistent data ./isecl-bootstrap.sh down <all/usecase of choice> #Delete 'scheduler-policy.json' rm -rf /opt/isecl-k8s-extensions #Comment/Remove '--policy-config-file=...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Comment/Remove '- mountPath: ...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true #Comment/Remove '-hostPath:...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet #Setup fresh ./isecl-bootstrap-db-services.sh up ./isecl-bootstrap.sh up <all/usecase of choice> #Setup fresh ./isecl-bootstrap-db-services.sh up ./isecl-bootstrap.sh up <all/usecase of choice> #Copy ihub_public_key.pem from <NFS-PATH>/ihub/config/ to K8s control-plane and update IHUB_PUB_KEY_PATH in isecl-isecl-k8s.env #Bootstrap isecl-scheduler ./isecl-bootstrap.sh up isecl-scheduler #Reconfigure K8s-scheduler containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet In order to clean single service and bring up again on multi node without data, config from previous deployment ./isecl-bootstrap.sh down <service-name> log into nfs system rm -rf /<nfs-mount-path>/isecl/<service-name>/config rm -rf /<nfs-mount-path>/isecl/<service-name>/logs #Only in case of KBS, perform one more step along with above 2 steps rm -rf /<nfs-mount-path>/isecl/<service-name>/opt log into K8s control-plane ./isecl-bootstrap.sh up <service-name> In order to redeploy again on multi node with data, config from previous deployment ./isecl-bootstrap.sh down <service-name> ./isecl-bootstrap.sh up <service-name>","title":"Multi-node"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/","text":"Hardware & OS Requirements Physical Server requirements Intel\u00ae SecL-DC supports and uses a variety of Intel security features, but there are some key requirements to consider before beginning an installation. Most important among these is the Root of Trust configuration. This involves deciding what combination of TXT, Boot Guard, tboot, and UEFI Secure Boot to enable on platforms that will be attested using Intel\u00ae SecL. Note At least one \"Static Root of Trust\" mechanism must be used (TXT and/or BtG). For Legacy BIOS systems, tboot must be used. For UEFI mode systems, UEFI SecureBoot must be used* Use the chart below for a guide to acceptable configuration options. Only dTPM is supported on Intel\u00ae SecL-DC platform hardware. Machines Build Machine K8s control-plane Node Setup on CSP (VMs/Physical Nodes + TXT/SUEFI enabled Physical Nodes) K8s control-plane Node Setup on Enterprise (VMs/Physical Nodes) OS Requirements RHEL 8.3 / Ubuntu 18.04 for build RHEL 8.3 / Ubuntu 18.04 for K8s cluster deployments Note Foundational & Workload Security solution is built, installed and tested with root privileges. Please ensure that all the following instructions are executed with root privileges Container Runtime Docker-19.03.13 CRIO-1.17.5 K8s Distributions Single Node: A single-node deployment uses microk8s to deploy the entire control plane in a pod on a single hardware machine. This is best for POC or demo environments, but can also be used when integrating Intel SecL with another application that runs on a virtual machine Multi Node: A multi-node deployment is a more typical Kubernetes architecture, where the Intel SecL management plane is simply deployed as a Pod, with the Intel SecL agents (the WLA and the TA, depending on use case) deployed as a DaemonSet. kubeadm is the supported multi node distribution as of today. Storage hostPath in case of single-node microk8s for all services and agents NFS in case of multi-node kubeadm for services and hostPath for Trust Agent and Workload Agent","title":"Hardware & OS Requirements"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#hardware-os-requirements","text":"","title":"Hardware &amp; OS Requirements"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#physical-server-requirements","text":"Intel\u00ae SecL-DC supports and uses a variety of Intel security features, but there are some key requirements to consider before beginning an installation. Most important among these is the Root of Trust configuration. This involves deciding what combination of TXT, Boot Guard, tboot, and UEFI Secure Boot to enable on platforms that will be attested using Intel\u00ae SecL. Note At least one \"Static Root of Trust\" mechanism must be used (TXT and/or BtG). For Legacy BIOS systems, tboot must be used. For UEFI mode systems, UEFI SecureBoot must be used* Use the chart below for a guide to acceptable configuration options. Only dTPM is supported on Intel\u00ae SecL-DC platform hardware.","title":"Physical Server requirements"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#machines","text":"Build Machine K8s control-plane Node Setup on CSP (VMs/Physical Nodes + TXT/SUEFI enabled Physical Nodes) K8s control-plane Node Setup on Enterprise (VMs/Physical Nodes)","title":"Machines"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#os-requirements","text":"RHEL 8.3 / Ubuntu 18.04 for build RHEL 8.3 / Ubuntu 18.04 for K8s cluster deployments Note Foundational & Workload Security solution is built, installed and tested with root privileges. Please ensure that all the following instructions are executed with root privileges","title":"OS Requirements"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#container-runtime","text":"Docker-19.03.13 CRIO-1.17.5","title":"Container Runtime"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#k8s-distributions","text":"Single Node: A single-node deployment uses microk8s to deploy the entire control plane in a pod on a single hardware machine. This is best for POC or demo environments, but can also be used when integrating Intel SecL with another application that runs on a virtual machine Multi Node: A multi-node deployment is a more typical Kubernetes architecture, where the Intel SecL management plane is simply deployed as a Pod, with the Intel SecL agents (the WLA and the TA, depending on use case) deployed as a DaemonSet. kubeadm is the supported multi node distribution as of today.","title":"K8s Distributions"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#storage","text":"hostPath in case of single-node microk8s for all services and agents NFS in case of multi-node kubeadm for services and hostPath for Trust Agent and Workload Agent","title":"Storage"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/2Network%20Requirements/","text":"Network Requirements Build System Internet access required CSP Managed Services Internet access required for Foundation & Workload Security services deployed on CSP system/Compute Node; Enterprise Managed Services Internet access required for Foundation & Workload Security services on Enterprise system; TXT/SUEFI Enabled Host Internet access required to access KBS running on Enterprise environment Firewall Settings Ensure that all the FS,WS service ports are accessible with firewall","title":"Network Requirements"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/2Network%20Requirements/#network-requirements","text":"","title":"Network Requirements"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/2Network%20Requirements/#build-system","text":"Internet access required","title":"Build System"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/2Network%20Requirements/#csp-managed-services","text":"Internet access required for Foundation & Workload Security services deployed on CSP system/Compute Node;","title":"CSP Managed Services"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/2Network%20Requirements/#enterprise-managed-services","text":"Internet access required for Foundation & Workload Security services on Enterprise system;","title":"Enterprise Managed Services"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/2Network%20Requirements/#txtsuefi-enabled-host","text":"Internet access required to access KBS running on Enterprise environment","title":"TXT/SUEFI Enabled Host"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/2Network%20Requirements/#firewall-settings","text":"Ensure that all the FS,WS service ports are accessible with firewall","title":"Firewall Settings"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/3Rpms%20%26%20Debs%20Requirement/","text":"Rpms & Debs Requirement RHEL rhel-8-for-x86_64-appstream-rpms rhel-8-for-x86_64-baseos-rpms codeready-builder-for-rhel-8-x86_64-rpms epel-release-latest-8.noarch.rpm Ubuntu None","title":"Rpms & Debs Requirement"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/3Rpms%20%26%20Debs%20Requirement/#rpms-debs-requirement","text":"","title":"Rpms &amp; Debs Requirement"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/3Rpms%20%26%20Debs%20Requirement/#rhel","text":"rhel-8-for-x86_64-appstream-rpms rhel-8-for-x86_64-baseos-rpms codeready-builder-for-rhel-8-x86_64-rpms epel-release-latest-8.noarch.rpm","title":"RHEL"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/3Rpms%20%26%20Debs%20Requirement/#ubuntu","text":"None","title":"Ubuntu"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/4Deployment%20Model/","text":"Deployment Model Single Node The single Node uses microk8s as a supported K8s distribution Multi Node The multi node supports kubeadm as a supported K8s distribution","title":"Deployment Model"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/4Deployment%20Model/#deployment-model","text":"","title":"Deployment Model"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/4Deployment%20Model/#single-node","text":"The single Node uses microk8s as a supported K8s distribution","title":"Single Node"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/4Deployment%20Model/#multi-node","text":"The multi node supports kubeadm as a supported K8s distribution","title":"Multi Node"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/5Build/","text":"Build Pre-requisites The below steps need to be done on RHEL 8.3 / Ubuntu-18.04 Build machine (VM/Physical Node) Development Tools and Utilities # RedHat Enterprise Linux 8.3 dnf install -y git wget tar python3 gcc gcc-c++ zip make yum-utils openssl-devel dnf install -y https://dl.fedoraproject.org/pub/fedora/linux/releases/32/Everything/x86_64/os/Packages/m/makeself-2.4.0-5.fc32.noarch.rpm ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip # Ubuntu-18.04 apt update apt remove -y gcc gcc-7 apt install -y python3-problem-report git wget tar python3 gcc-8 make makeself openssl libssl-dev libgpg-error-dev cp /usr/bin/gcc-8 /usr/bin/gcc ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip Repo tool tmpdir = $( mktemp -d ) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir /repo /usr/local/bin rm -rf $tmpdir Golang wget https://dl.google.com/go/go1.14.4.linux-amd64.tar.gz tar -xzf go1.14.4.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT = /usr/local/go export PATH = $GOROOT /bin: $PATH rm -rf go1.14.4.linux-amd64.tar.gz Docker # RedHat Enterprise Linux-8.3 dnf module enable -y container-tools dnf install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo dnf install -y docker-ce-19.03.13 docker-ce-cli-19.03.13 systemctl enable docker systemctl start docker # Ubuntu-18.04 apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null apt-get update apt-get install docker-ce = 5 :19.03.13~3-0~ubuntu-bionic docker-ce-cli = 5 :19.03.13~3-0~ubuntu-bionic containerd.io systemctl enable docker systemctl start docker Apply the below steps only if running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" systemctl daemon-reload systemctl restart docker Build OCI Container images and K8s Manifests Foundational Security Sync the repos mkdir -p /root/intel-secl/build/fs && cd /root/intel-secl/build/fs repo init -u https://github.com/intel-secl/build-manifest.git -m manifest/fs.xml -b refs/tags/v4.0.0 repo sync Run the pre-requisites setup script cd utils/build/foundational-security/ chmod +x fs-prereq.sh ./fs-prereq.sh -s Install skopeo # RHEL 8.x dnf install -y skopeo # Ubuntu 18.04 add-apt-repository ppa:projectatomic/ppa apt-get update apt-get install skopeo Build cd /root/intel-secl/build/fs/ #Single node cluster with microk8s make k8s-aio #Multi node cluster with kubeadm make k8s Built Container images,K8s manifests and deployment scripts /root/intel-secl/build/fs/k8s/ Workload Security Container Confidentiality with CRIO Runtime Sync the repos mkdir -p /root/intel-secl/build/cc-crio && cd /root/intel-secl/build/cc-crio repo init -u https://github.com/intel-secl/build-manifest.git -m manifest/cc-crio.xml -b refs/tags/v4.0.0 repo sync Run the pre-requisites script cd utils/build/workload-security chmod +x ws-prereq.sh ./ws-prereq.sh -c Build cd /root/intel-secl/build/cc-crio #Single node cluster with microk8s make k8s-aio #Multi node cluster with kubeadm make k8s Built Container images,K8s manifests and deployment scripts /root/intel-secl/build/cc-crio/k8s/","title":"Build"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/5Build/#build","text":"","title":"Build"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/5Build/#pre-requisites","text":"The below steps need to be done on RHEL 8.3 / Ubuntu-18.04 Build machine (VM/Physical Node)","title":"Pre-requisites"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/5Build/#development-tools-and-utilities","text":"# RedHat Enterprise Linux 8.3 dnf install -y git wget tar python3 gcc gcc-c++ zip make yum-utils openssl-devel dnf install -y https://dl.fedoraproject.org/pub/fedora/linux/releases/32/Everything/x86_64/os/Packages/m/makeself-2.4.0-5.fc32.noarch.rpm ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip # Ubuntu-18.04 apt update apt remove -y gcc gcc-7 apt install -y python3-problem-report git wget tar python3 gcc-8 make makeself openssl libssl-dev libgpg-error-dev cp /usr/bin/gcc-8 /usr/bin/gcc ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip","title":"Development Tools and Utilities"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/5Build/#repo-tool","text":"tmpdir = $( mktemp -d ) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir /repo /usr/local/bin rm -rf $tmpdir","title":"Repo tool"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/5Build/#golang","text":"wget https://dl.google.com/go/go1.14.4.linux-amd64.tar.gz tar -xzf go1.14.4.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT = /usr/local/go export PATH = $GOROOT /bin: $PATH rm -rf go1.14.4.linux-amd64.tar.gz","title":"Golang"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/5Build/#docker","text":"# RedHat Enterprise Linux-8.3 dnf module enable -y container-tools dnf install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo dnf install -y docker-ce-19.03.13 docker-ce-cli-19.03.13 systemctl enable docker systemctl start docker # Ubuntu-18.04 apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null apt-get update apt-get install docker-ce = 5 :19.03.13~3-0~ubuntu-bionic docker-ce-cli = 5 :19.03.13~3-0~ubuntu-bionic containerd.io systemctl enable docker systemctl start docker Apply the below steps only if running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" systemctl daemon-reload systemctl restart docker","title":"Docker"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/5Build/#build-oci-container-images-and-k8s-manifests","text":"","title":"Build OCI Container images and K8s Manifests"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/5Build/#foundational-security","text":"Sync the repos mkdir -p /root/intel-secl/build/fs && cd /root/intel-secl/build/fs repo init -u https://github.com/intel-secl/build-manifest.git -m manifest/fs.xml -b refs/tags/v4.0.0 repo sync Run the pre-requisites setup script cd utils/build/foundational-security/ chmod +x fs-prereq.sh ./fs-prereq.sh -s Install skopeo # RHEL 8.x dnf install -y skopeo # Ubuntu 18.04 add-apt-repository ppa:projectatomic/ppa apt-get update apt-get install skopeo Build cd /root/intel-secl/build/fs/ #Single node cluster with microk8s make k8s-aio #Multi node cluster with kubeadm make k8s Built Container images,K8s manifests and deployment scripts /root/intel-secl/build/fs/k8s/","title":"Foundational Security"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/5Build/#workload-security","text":"","title":"Workload Security"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/5Build/#container-confidentiality-with-crio-runtime","text":"Sync the repos mkdir -p /root/intel-secl/build/cc-crio && cd /root/intel-secl/build/cc-crio repo init -u https://github.com/intel-secl/build-manifest.git -m manifest/cc-crio.xml -b refs/tags/v4.0.0 repo sync Run the pre-requisites script cd utils/build/workload-security chmod +x ws-prereq.sh ./ws-prereq.sh -c Build cd /root/intel-secl/build/cc-crio #Single node cluster with microk8s make k8s-aio #Multi node cluster with kubeadm make k8s Built Container images,K8s manifests and deployment scripts /root/intel-secl/build/cc-crio/k8s/","title":"Container Confidentiality with CRIO Runtime"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/","text":"Deployment Pre-requisites Install openssl on K8s control-plane Ensure a docker registry is running locally or remotely. Note For single node microk8s deployment, a registry can be brought up by using microk8s add-ons. More details present in Microk8s documentation. This is not mandatory, if a remote registry already exists, the same can be used as well for single-node Note For multi-node kubeadm deployment, a docker registry needs to be setup by the user Push all container images to docker registry. Example below # Without TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/<image-name>:<image-tag> --dest-tls-verify = false # With TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/image-name>:<image-tag> Note In case of microk8s deployment, when docker registry is enabled locally, the OCI container images need to be copied to the node where registry is enabled and then the above example command can be run. The same would not be required when registry is remotely installed On each worker node with TXT/BTG enabled and registered to K8s control-plane, the following pre-req needs to be done on RHEL-8.3 / Ubuntu-18.04 systems Foundational Security Tboot-1.10.1 or later to be installed for non SUEFI servers. Tboot installation Details Only for Ubuntu-18.04 , run the following commands $ modprobe msr Workload Security Container Confidentiality with CRIO runtime Tboot-1.10.1 or later to be installed for non SUEFI servers. Tboot installation Details Copy container-runtime directory to each of the physical servers Run the install-prereqs-crio.sh script on the physical servers from container-runtime Note container-runtime scripts need to be run on TXT/BTG/SUEFI enabled services Reboot the server Only for Ubuntu-18.04 , run the following command $ modprobe msr Deploy Single-Node Pre-requisites Setup microk8s being the default supported single node K8s distribution, users would need to install microk8s on a Physical server Copy all manifests and OCI container images as required to KK8s control-plane Ensure docker registry is running locally or remotely The K8s cluster admin should configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: TXT-ENABLED or node.type: SUEFI-ENABLED respectively for TXT/SUEFI enabled servers can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the K8s manifests. This can be edited in k8s/manifests/ta/daemonset.yml , k8s/manifests/wla/daemonset.yml Refer Section in appendix for Feature Detection - node.type: TXT-ENABLED should be labeled for nodes installed with tboot, where event logs will be collected from tboot measurements. - node.type: SUEFI-ENABLED should be labeled for nodes with SUEFI enabled, where event logs will be efi logs. #Label node for TXT kubectl label node <node-name> node.type = TXT-ENABLED #Label node for SUEFI kubectl label node <node-name> node.type = SUEFI-ENABLED In case of microk8s cluster, the --allow-privileged=true flag needs to be added to the kube-apiserver under /var/snap/microk8s/current/args/kube-apiserver and restart kube-apiserver with systemctl restart snap.microk8s.daemon-apiserver to allow running of privileged containers like TRUST-AGENT and WORKLOAD-AGENT Ensure a backend KMIP-2.0 compliant server like pykmip is up and running. Manifests Update all the K8s manifests with the image names to be pulled from the registry The tolerations and node-affinity in case of isecl-scheduler and isecl-controller needs to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to microk8s.io/cluster based on k8s distributions of kubeadm and microk8s respectively Deploy steps The bootstrap script would facilitate the deployment of all FS,WS components at a use case level. Sample one given below. Update isecl-k8s.env file #Kubernetes Distribution - microk8s K8S_DISTRIBUTION = microk8s K8S_CONTROL_PLANE_IP = K8S_CONTROL_PLANE_HOSTNAME = # cms CMS_BASE_URL = https://cms-svc.isecl.svc.cluster.local:8445/cms/v1 CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> CMS_K8S_ENDPOINT_URL = https://<k8s control-plane IP>:30445/cms/v1 # authservice AAS_API_URL = https://aas-svc.isecl.svc.cluster.local:8444/aas/v1 AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_DB_HOSTNAME = aasdb-svc.isecl.svc.cluster.local AAS_DB_PORT = \"5432\" AAS_DB_NAME = aasdb AAS_DB_SSLMODE = verify-full AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> #NATS_ACCOUNT_NAME=ISecL-account # Workload Service WLS_SERVICE_USERNAME = admin@wls WLS_SERVICE_PASSWORD = wlsAdminPass WLS_DB_USERNAME = wlsdbuser WLS_DB_PASSWORD = wlsdbpassword WLS_DB_HOSTNAME = wlsdb-svc.isecl.svc.cluster.local WLS_DB_NAME = wlsdb WLS_DB_PORT = \"5432\" WLS_API_URL = https://wls-svc.isecl.svc.cluster.local:5000/wls/v1 WLS_CERT_SAN_LIST = wls-svc.isecl.svc.cluster.local # Host Verification Service HVS_SERVICE_USERNAME = admin@hvs HVS_SERVICE_PASSWORD = hvsAdminPass HVS_DB_USERNAME = hvsdbuser HVS_DB_PASSWORD = hvsdbpassword HVS_DB_HOSTNAME = hvsdb-svc.isecl.svc.cluster.local HVS_DB_NAME = hvsdb HVS_CERT_SAN_LIST = hvs-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> HVS_DB_PORT = \"5432\" HVS_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2/ #Nats Servers configuration for TA and HVS #NATS_SERVERS=nats://<K8s control-plane IP/Hostname>:30222 # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> # For microk8s # K8S_API_SERVER_CERT=/var/snap/microk8s/current/certs/server.crt K8S_API_SERVER_CERT = /var/snap/microk8s/current/certs/server.crt # This is valid for multinode deployment, should be populated once ihub is deployed successfully IHUB_PUB_KEY_PATH = HVS_BASE_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2 # TrustAgent # e.g TA_CERT_SAN_LIST=*.example.com,192.168.1.* TA_CERT_SAN_LIST = TPM_OWNER_SECRET = # Workload Agent WLA_SERVICE_USERNAME = wlauser@wls WLA_SERVICE_PASSWORD = wlaAdminPass # KBS ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_HOSTNAME = <KMIP IP/Hostname> KMIP_SERVER_IP = KMIP_SERVER_PORT = # Retrieve the following KMIP server\u2019s client certificate, client key and root ca certificate from the KMIP server. # This key and certificates will be available in KMIP server, /etc/pykmip is the default path copy them to this system manifests/kbs/kmip-secrets path KMIP_CLIENT_CERT_NAME = client_certificate.pem KMIP_CLIENT_KEY_NAME = client_key.pem KMIP_ROOT_CERT_NAME = root_certificate.pem # ISecl Scheduler # For microk8s # K8S_CA_KEY=/var/snap/microk8s/current/certs/ca.key # K8S_CA_CERT=/var/snap/microk8s/current/certs/ca.crt K8S_CA_KEY = /var/snap/microk8s/current/certs/ca.key K8S_CA_CERT = /var/snap/microk8s/current/certs/ca.crt # populate users.env ISECL_INSTALL_COMPONENTS = \"AAS,HVS,WLS,IHUB,KBS,WLA,TA,WPM\" #NATS_CERT_SAN_LIST= #NATS_TLS_COMMON_NAME= GLOBAL_ADMIN_USERNAME = GLOBAL_ADMIN_PASSWORD = INSTALL_ADMIN_USERNAME = INSTALL_ADMIN_PASSWORD = WPM_SERVICE_USERNAME = WPM_SERVICE_PASSWORD = CUSTOM_CLAIMS_COMPONENTS = CCC_ADMIN_USERNAME = CCC_ADMIN_PASSWORD = Note Ensure to update KMIP_CLIENT_CERT_NAME , KMIP_CLIENT_KEY_NAME , KMIP_ROOT_CERT_NAME in the env from /etc/pykmip of pykmip by copying the key and certs to this system under manifests/kbs/kmip-secrets path Run scripts on K8s control-plane The bootstrap scripts are sample scripts to allow for a quick start of FS,WS services and agents. Users are free to modify the script or directly use the K8s manifests as per their deployment model requirements #Pre-reqs.sh ./pre-requisites.sh #isecl-bootstrap-db-services #Reference #Usage: ./isecl-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, Workload Service and Host #verification Service # purge Delete Database Services for Authservice, Workload Service and Host #verification Service ./isecl-bootstrap-db-services.sh up #isecl-bootstrap #Reference #Usage: ./isecl-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap ISecL K8s environment for #specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete ISecL K8s environment for specified #agent/service/usecase [will not delete data, config, logs] # purge Delete ISecL K8s environment with data, #config, logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of tagent, wlagent # service Can be one of cms, authservice, hvs, ihub, wls, kbs, isecl-#controller, isecl-scheduler # usecase Can be one of foundational-security, workload-security, isecl-#orchestration-k8s, csp, enterprise ./isecl-bootstrap.sh up <all/usecase of choice> Note An error to create asymmetric key would mean the following line, RANDFILE = $ENV::HOME/.rnd needs to be commented under /etc/ssl/openssl.cnf Update the IHUB_PUB_KEY_PATH in isecl-k8s.env to /etc/ihub/ihub_public_key.pem Bring up isecl-scheduler ./isecl-bootstrap.sh up isecl-scheduler Copy scheduler-policy.json mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions/ Edit kube-scheduler and restart kubelet #Edit the kube-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service Multi-Node Pre-requisites Setup kubeadm being the default supported multi-node K8s distribution, users would need to install a kubeadm K8s control-plane node setup Copy all manifests and OCI container images as required to K8s control-plane Ensure images are pushed to registry locally or remotely The K8s cluster admin should configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: TXT-ENABLED or node.type: SUEFI-ENABLED respectively for TXT/SUEFI enabled servers can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the K8s manifests. This can be edited in k8s/manifests/ta/daemonset.yml , k8s/manifests/wla/daemonset.yml Refer Section in appendix for Feature Detection - node.type: TXT-ENABLED should be used for nodes with tboot installed, where event logs will be collected from tboot measurements. - node.type: SUEFI-ENABLED should be used for nodes with SUEFI enabled, where event logs will be EFI logs. #Label node for TXT kubectl label node <node-name> node.type = TXT-ENABLED #Label node for SUEFI kubectl label node <node-name> node.type = SUEFI-ENABLED NFS storage class is used in kubernetes environment for data persistence and supported in ISecL FS/WS usecases. User needs to setup NFS server and create directory structure along with granting permission for a given user id. From security point of view, its been recommended to create a separate user id and grant the permission for all isecl directories for this user id. Below are some samples for reference Snapshot showing directory structure for which user needs to create on NFS volumes manually or using custom scripts. Snapshot showing ownership and permissions for directories for which user needs to manually grant the ownership. Snapshot for configuring PV and PVC , user need to provide the NFS server IP or hostname and paths for each of the service directories. Sample manifest for creating config-pv for cms service --- apiVersion : v1 kind : PersistentVolume metadata : name : cms-config-pv spec : capacity : storage : 128Mi volumeMode : Filesystem accessModes : - ReadWriteMany persistentVolumeReclaimPolicy : Retain storageClassName : nfs nfs : path : /<NFS-vol-base-path>/isecl/cms/config server : <NFS Server IP/Hostname> Sample manifest for creating config-pvc for cms service --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : cms-config-pvc namespace : isecl spec : storageClassName : nfs accessModes : - ReadWriteMany resources : requests : storage : 128Mi Note The user id specified in security context in deployment.yml for a given service and owner of the service related directories in NFS must be same Ensure a backend KMIP-2.0 compliant server like pykmip is up and running. Manifests Update all the K8s manifests with the image names to be pulled from the registry The tolerations and node-affinity in case of isecl-scheduler and isecl-controller needs to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to node-role.kubernetes.io/master All NFS PV yaml files needs to be updated with the path: /<NFS-vol-path> and server: <NFS Server IP/Hostname> under each service manifest file for config , logs , db-data Deploy steps Update isecl-k8s.env file #Kubernetes Distribution - kubeadm K8S_DISTRIBUTION = kubeadm K8S_CONTROL_PLANE_IP = K8S_CONTROL_PLANE_HOSTNAME = # cms CMS_BASE_URL = https://cms-svc.isecl.svc.cluster.local:8445/cms/v1 CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> CMS_K8S_ENDPOINT_URL = https://<k8s control-plane IP>:30445/cms/v1 # authservice AAS_API_URL = https://aas-svc.isecl.svc.cluster.local:8444/aas/v1 AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_DB_HOSTNAME = aasdb-svc.isecl.svc.cluster.local AAS_DB_PORT = \"5432\" AAS_DB_NAME = aasdb AAS_DB_SSLMODE = verify-full AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> #NATS_ACCOUNT_NAME=ISecL-account # Workload Service WLS_SERVICE_USERNAME = admin@wls WLS_SERVICE_PASSWORD = wlsAdminPass WLS_DB_USERNAME = wlsdbuser WLS_DB_PASSWORD = wlsdbpassword WLS_DB_HOSTNAME = wlsdb-svc.isecl.svc.cluster.local WLS_DB_NAME = wlsdb WLS_DB_PORT = \"5432\" WLS_API_URL = https://wls-svc.isecl.svc.cluster.local:5000/wls/v1 WLS_CERT_SAN_LIST = wls-svc.isecl.svc.cluster.local # Host Verification Service HVS_SERVICE_USERNAME = admin@hvs HVS_SERVICE_PASSWORD = hvsAdminPass HVS_DB_USERNAME = hvsdbuser HVS_DB_PASSWORD = hvsdbpassword HVS_DB_HOSTNAME = hvsdb-svc.isecl.svc.cluster.local HVS_DB_NAME = hvsdb HVS_CERT_SAN_LIST = hvs-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> HVS_DB_PORT = \"5432\" HVS_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2/ #Nats Servers configuration for TA and HVS #NATS_SERVERS=nats://<K8s control-plane IP/Hostname>:30222 # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> # For Kubeadm # K8S_API_SERVER_CERT=/etc/kubernetes/pki/apiserver.crt K8S_API_SERVER_CERT = /etc/kubernetes/pki/apiserver.crt # This is valid for multinode deployment, should be populated once ihub is deployed successfully IHUB_PUB_KEY_PATH = HVS_BASE_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2 # TrustAgent # e.g TA_CERT_SAN_LIST=*.example.com,192.168.1.* TA_CERT_SAN_LIST = TPM_OWNER_SECRET = # Workload Agent WLA_SERVICE_USERNAME = wlauser@wls WLA_SERVICE_PASSWORD = wlaAdminPass # KBS ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_HOSTNAME = <KMIP IP/Hostname> KMIP_SERVER_IP = KMIP_SERVER_PORT = # Retrieve the following KMIP server\u2019s client certificate, client key and root ca certificate from the KMIP server. # This key and certificates will be available in KMIP server, /etc/pykmip is the default path copy them to this system manifests/kbs/kmip-secrets path KMIP_CLIENT_CERT_NAME = client_certificate.pem KMIP_CLIENT_KEY_NAME = client_key.pem KMIP_ROOT_CERT_NAME = root_certificate.pem # ISecl Scheduler # For Kubeadm # K8S_CA_KEY=/etc/kubernetes/pki/ca.key # K8S_CA_CERT=/etc/kubernetes/pki/ca.crt K8S_CA_KEY = /etc/kubernetes/pki/ca.key K8S_CA_CERT = /etc/kubernetes/pki/ca.crt # populate users.env ISECL_INSTALL_COMPONENTS = \"AAS,HVS,WLS,IHUB,KBS,WLA,TA,WPM\" #NATS_CERT_SAN_LIST= #NATS_TLS_COMMON_NAME= GLOBAL_ADMIN_USERNAME = GLOBAL_ADMIN_PASSWORD = INSTALL_ADMIN_USERNAME = INSTALL_ADMIN_PASSWORD = WPM_SERVICE_USERNAME = WPM_SERVICE_PASSWORD = CUSTOM_CLAIMS_COMPONENTS = CCC_ADMIN_USERNAME = CCC_ADMIN_PASSWORD = Note Ensure to update KMIP_CLIENT_CERT_NAME , KMIP_CLIENT_KEY_NAME , KMIP_ROOT_CERT_NAME in the env from /etc/pykmip of pykmip by copying the key and certs to this system under manifests/kbs/kmip-secrets path Run scripts on K8s control-plane The bootstrap scripts are sample scripts to allow for a quick start of FS,WS services and agents. Users are free to modify the script or directly use the K8s manifests as per their deployment model requirements #Pre-reqs.sh ./pre-requisites.sh #isecl-bootstrap-db-services #Reference #Usage: ./isecl-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, Workload Service and Host verification Service # purge Delete Database Services for Authservice, Workload Service and Host verification Service ./isecl-bootstrap-db-services.sh up #isecl-bootstrap #Reference #Usage: ./isecl-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap ISecL K8s environment for #specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete ISecL K8s environment for specified #agent/service/usecase [will not delete data, config, logs] # purge Delete ISecL K8s environment with data, config, logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of tagent, wlagent # service Can be one of cms, authservice, hvs, ihub, wls, kbs, isecl-#controller, isecl-scheduler # usecase Can be one of foundational-security, workload-security, isecl-#orchestration-k8s, csp, enterprise ./isecl-bootstrap.sh up <all/usecase of choice> Note An error to create asymmetric key would mean the following line, RANDFILE = $ENV::HOME/.rnd needs to be commented under /etc/ssl/openssl.cnf Copy the ihub_public_key.pem from NFS path - <mnt>/isecl/ihub/config/ihub_public_key.pem to K8s control-plane Update the isecl-k8s.env for IHUB_PUB_KEY_PATH Bring up the isecl-k8s-scheduler ./isecl-bootstrap.sh up isecl-scheduler Create and update scheduler-policy.json path mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions Configure kube-scheduler to establish communication with isecl-scheduler. Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec : containers : - command : - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers : volumeMounts : - mountPath : /opt/isecl-k8s-extensions/ name : extendedsched readOnly : true volumes : - hostPath : path : /opt/isecl-k8s-extensions/ type : name : extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml Restart kubelet which restart all the k8s services including kube-scheduler systemctl restart kubelet","title":"Deployment"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#deployment","text":"","title":"Deployment"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#pre-requisites","text":"Install openssl on K8s control-plane Ensure a docker registry is running locally or remotely. Note For single node microk8s deployment, a registry can be brought up by using microk8s add-ons. More details present in Microk8s documentation. This is not mandatory, if a remote registry already exists, the same can be used as well for single-node Note For multi-node kubeadm deployment, a docker registry needs to be setup by the user Push all container images to docker registry. Example below # Without TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/<image-name>:<image-tag> --dest-tls-verify = false # With TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/image-name>:<image-tag> Note In case of microk8s deployment, when docker registry is enabled locally, the OCI container images need to be copied to the node where registry is enabled and then the above example command can be run. The same would not be required when registry is remotely installed On each worker node with TXT/BTG enabled and registered to K8s control-plane, the following pre-req needs to be done on RHEL-8.3 / Ubuntu-18.04 systems Foundational Security Tboot-1.10.1 or later to be installed for non SUEFI servers. Tboot installation Details Only for Ubuntu-18.04 , run the following commands $ modprobe msr Workload Security Container Confidentiality with CRIO runtime Tboot-1.10.1 or later to be installed for non SUEFI servers. Tboot installation Details Copy container-runtime directory to each of the physical servers Run the install-prereqs-crio.sh script on the physical servers from container-runtime Note container-runtime scripts need to be run on TXT/BTG/SUEFI enabled services Reboot the server Only for Ubuntu-18.04 , run the following command $ modprobe msr","title":"Pre-requisites"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#deploy","text":"","title":"Deploy"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#single-node","text":"","title":"Single-Node"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#pre-requisites_1","text":"","title":"Pre-requisites"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#setup","text":"microk8s being the default supported single node K8s distribution, users would need to install microk8s on a Physical server Copy all manifests and OCI container images as required to KK8s control-plane Ensure docker registry is running locally or remotely The K8s cluster admin should configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: TXT-ENABLED or node.type: SUEFI-ENABLED respectively for TXT/SUEFI enabled servers can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the K8s manifests. This can be edited in k8s/manifests/ta/daemonset.yml , k8s/manifests/wla/daemonset.yml Refer Section in appendix for Feature Detection - node.type: TXT-ENABLED should be labeled for nodes installed with tboot, where event logs will be collected from tboot measurements. - node.type: SUEFI-ENABLED should be labeled for nodes with SUEFI enabled, where event logs will be efi logs. #Label node for TXT kubectl label node <node-name> node.type = TXT-ENABLED #Label node for SUEFI kubectl label node <node-name> node.type = SUEFI-ENABLED In case of microk8s cluster, the --allow-privileged=true flag needs to be added to the kube-apiserver under /var/snap/microk8s/current/args/kube-apiserver and restart kube-apiserver with systemctl restart snap.microk8s.daemon-apiserver to allow running of privileged containers like TRUST-AGENT and WORKLOAD-AGENT Ensure a backend KMIP-2.0 compliant server like pykmip is up and running.","title":"Setup"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#manifests","text":"Update all the K8s manifests with the image names to be pulled from the registry The tolerations and node-affinity in case of isecl-scheduler and isecl-controller needs to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to microk8s.io/cluster based on k8s distributions of kubeadm and microk8s respectively","title":"Manifests"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#deploy-steps","text":"The bootstrap script would facilitate the deployment of all FS,WS components at a use case level. Sample one given below.","title":"Deploy steps"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#update-isecl-k8senv-file","text":"#Kubernetes Distribution - microk8s K8S_DISTRIBUTION = microk8s K8S_CONTROL_PLANE_IP = K8S_CONTROL_PLANE_HOSTNAME = # cms CMS_BASE_URL = https://cms-svc.isecl.svc.cluster.local:8445/cms/v1 CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> CMS_K8S_ENDPOINT_URL = https://<k8s control-plane IP>:30445/cms/v1 # authservice AAS_API_URL = https://aas-svc.isecl.svc.cluster.local:8444/aas/v1 AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_DB_HOSTNAME = aasdb-svc.isecl.svc.cluster.local AAS_DB_PORT = \"5432\" AAS_DB_NAME = aasdb AAS_DB_SSLMODE = verify-full AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> #NATS_ACCOUNT_NAME=ISecL-account # Workload Service WLS_SERVICE_USERNAME = admin@wls WLS_SERVICE_PASSWORD = wlsAdminPass WLS_DB_USERNAME = wlsdbuser WLS_DB_PASSWORD = wlsdbpassword WLS_DB_HOSTNAME = wlsdb-svc.isecl.svc.cluster.local WLS_DB_NAME = wlsdb WLS_DB_PORT = \"5432\" WLS_API_URL = https://wls-svc.isecl.svc.cluster.local:5000/wls/v1 WLS_CERT_SAN_LIST = wls-svc.isecl.svc.cluster.local # Host Verification Service HVS_SERVICE_USERNAME = admin@hvs HVS_SERVICE_PASSWORD = hvsAdminPass HVS_DB_USERNAME = hvsdbuser HVS_DB_PASSWORD = hvsdbpassword HVS_DB_HOSTNAME = hvsdb-svc.isecl.svc.cluster.local HVS_DB_NAME = hvsdb HVS_CERT_SAN_LIST = hvs-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> HVS_DB_PORT = \"5432\" HVS_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2/ #Nats Servers configuration for TA and HVS #NATS_SERVERS=nats://<K8s control-plane IP/Hostname>:30222 # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> # For microk8s # K8S_API_SERVER_CERT=/var/snap/microk8s/current/certs/server.crt K8S_API_SERVER_CERT = /var/snap/microk8s/current/certs/server.crt # This is valid for multinode deployment, should be populated once ihub is deployed successfully IHUB_PUB_KEY_PATH = HVS_BASE_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2 # TrustAgent # e.g TA_CERT_SAN_LIST=*.example.com,192.168.1.* TA_CERT_SAN_LIST = TPM_OWNER_SECRET = # Workload Agent WLA_SERVICE_USERNAME = wlauser@wls WLA_SERVICE_PASSWORD = wlaAdminPass # KBS ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_HOSTNAME = <KMIP IP/Hostname> KMIP_SERVER_IP = KMIP_SERVER_PORT = # Retrieve the following KMIP server\u2019s client certificate, client key and root ca certificate from the KMIP server. # This key and certificates will be available in KMIP server, /etc/pykmip is the default path copy them to this system manifests/kbs/kmip-secrets path KMIP_CLIENT_CERT_NAME = client_certificate.pem KMIP_CLIENT_KEY_NAME = client_key.pem KMIP_ROOT_CERT_NAME = root_certificate.pem # ISecl Scheduler # For microk8s # K8S_CA_KEY=/var/snap/microk8s/current/certs/ca.key # K8S_CA_CERT=/var/snap/microk8s/current/certs/ca.crt K8S_CA_KEY = /var/snap/microk8s/current/certs/ca.key K8S_CA_CERT = /var/snap/microk8s/current/certs/ca.crt # populate users.env ISECL_INSTALL_COMPONENTS = \"AAS,HVS,WLS,IHUB,KBS,WLA,TA,WPM\" #NATS_CERT_SAN_LIST= #NATS_TLS_COMMON_NAME= GLOBAL_ADMIN_USERNAME = GLOBAL_ADMIN_PASSWORD = INSTALL_ADMIN_USERNAME = INSTALL_ADMIN_PASSWORD = WPM_SERVICE_USERNAME = WPM_SERVICE_PASSWORD = CUSTOM_CLAIMS_COMPONENTS = CCC_ADMIN_USERNAME = CCC_ADMIN_PASSWORD = Note Ensure to update KMIP_CLIENT_CERT_NAME , KMIP_CLIENT_KEY_NAME , KMIP_ROOT_CERT_NAME in the env from /etc/pykmip of pykmip by copying the key and certs to this system under manifests/kbs/kmip-secrets path","title":"Update isecl-k8s.env file"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#run-scripts-on-k8s-control-plane","text":"The bootstrap scripts are sample scripts to allow for a quick start of FS,WS services and agents. Users are free to modify the script or directly use the K8s manifests as per their deployment model requirements #Pre-reqs.sh ./pre-requisites.sh #isecl-bootstrap-db-services #Reference #Usage: ./isecl-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, Workload Service and Host #verification Service # purge Delete Database Services for Authservice, Workload Service and Host #verification Service ./isecl-bootstrap-db-services.sh up #isecl-bootstrap #Reference #Usage: ./isecl-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap ISecL K8s environment for #specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete ISecL K8s environment for specified #agent/service/usecase [will not delete data, config, logs] # purge Delete ISecL K8s environment with data, #config, logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of tagent, wlagent # service Can be one of cms, authservice, hvs, ihub, wls, kbs, isecl-#controller, isecl-scheduler # usecase Can be one of foundational-security, workload-security, isecl-#orchestration-k8s, csp, enterprise ./isecl-bootstrap.sh up <all/usecase of choice> Note An error to create asymmetric key would mean the following line, RANDFILE = $ENV::HOME/.rnd needs to be commented under /etc/ssl/openssl.cnf Update the IHUB_PUB_KEY_PATH in isecl-k8s.env to /etc/ihub/ihub_public_key.pem Bring up isecl-scheduler ./isecl-bootstrap.sh up isecl-scheduler Copy scheduler-policy.json mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions/ Edit kube-scheduler and restart kubelet #Edit the kube-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service","title":"Run scripts on K8s control-plane"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#multi-node","text":"","title":"Multi-Node"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#pre-requisites_2","text":"","title":"Pre-requisites"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#setup_1","text":"kubeadm being the default supported multi-node K8s distribution, users would need to install a kubeadm K8s control-plane node setup Copy all manifests and OCI container images as required to K8s control-plane Ensure images are pushed to registry locally or remotely The K8s cluster admin should configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: TXT-ENABLED or node.type: SUEFI-ENABLED respectively for TXT/SUEFI enabled servers can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the K8s manifests. This can be edited in k8s/manifests/ta/daemonset.yml , k8s/manifests/wla/daemonset.yml Refer Section in appendix for Feature Detection - node.type: TXT-ENABLED should be used for nodes with tboot installed, where event logs will be collected from tboot measurements. - node.type: SUEFI-ENABLED should be used for nodes with SUEFI enabled, where event logs will be EFI logs. #Label node for TXT kubectl label node <node-name> node.type = TXT-ENABLED #Label node for SUEFI kubectl label node <node-name> node.type = SUEFI-ENABLED NFS storage class is used in kubernetes environment for data persistence and supported in ISecL FS/WS usecases. User needs to setup NFS server and create directory structure along with granting permission for a given user id. From security point of view, its been recommended to create a separate user id and grant the permission for all isecl directories for this user id. Below are some samples for reference Snapshot showing directory structure for which user needs to create on NFS volumes manually or using custom scripts. Snapshot showing ownership and permissions for directories for which user needs to manually grant the ownership. Snapshot for configuring PV and PVC , user need to provide the NFS server IP or hostname and paths for each of the service directories. Sample manifest for creating config-pv for cms service --- apiVersion : v1 kind : PersistentVolume metadata : name : cms-config-pv spec : capacity : storage : 128Mi volumeMode : Filesystem accessModes : - ReadWriteMany persistentVolumeReclaimPolicy : Retain storageClassName : nfs nfs : path : /<NFS-vol-base-path>/isecl/cms/config server : <NFS Server IP/Hostname> Sample manifest for creating config-pvc for cms service --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : cms-config-pvc namespace : isecl spec : storageClassName : nfs accessModes : - ReadWriteMany resources : requests : storage : 128Mi Note The user id specified in security context in deployment.yml for a given service and owner of the service related directories in NFS must be same Ensure a backend KMIP-2.0 compliant server like pykmip is up and running.","title":"Setup"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#manifests_1","text":"Update all the K8s manifests with the image names to be pulled from the registry The tolerations and node-affinity in case of isecl-scheduler and isecl-controller needs to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to node-role.kubernetes.io/master All NFS PV yaml files needs to be updated with the path: /<NFS-vol-path> and server: <NFS Server IP/Hostname> under each service manifest file for config , logs , db-data","title":"Manifests"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#deploy-steps_1","text":"","title":"Deploy steps"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#update-isecl-k8senv-file_1","text":"#Kubernetes Distribution - kubeadm K8S_DISTRIBUTION = kubeadm K8S_CONTROL_PLANE_IP = K8S_CONTROL_PLANE_HOSTNAME = # cms CMS_BASE_URL = https://cms-svc.isecl.svc.cluster.local:8445/cms/v1 CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> CMS_K8S_ENDPOINT_URL = https://<k8s control-plane IP>:30445/cms/v1 # authservice AAS_API_URL = https://aas-svc.isecl.svc.cluster.local:8444/aas/v1 AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_DB_HOSTNAME = aasdb-svc.isecl.svc.cluster.local AAS_DB_PORT = \"5432\" AAS_DB_NAME = aasdb AAS_DB_SSLMODE = verify-full AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> #NATS_ACCOUNT_NAME=ISecL-account # Workload Service WLS_SERVICE_USERNAME = admin@wls WLS_SERVICE_PASSWORD = wlsAdminPass WLS_DB_USERNAME = wlsdbuser WLS_DB_PASSWORD = wlsdbpassword WLS_DB_HOSTNAME = wlsdb-svc.isecl.svc.cluster.local WLS_DB_NAME = wlsdb WLS_DB_PORT = \"5432\" WLS_API_URL = https://wls-svc.isecl.svc.cluster.local:5000/wls/v1 WLS_CERT_SAN_LIST = wls-svc.isecl.svc.cluster.local # Host Verification Service HVS_SERVICE_USERNAME = admin@hvs HVS_SERVICE_PASSWORD = hvsAdminPass HVS_DB_USERNAME = hvsdbuser HVS_DB_PASSWORD = hvsdbpassword HVS_DB_HOSTNAME = hvsdb-svc.isecl.svc.cluster.local HVS_DB_NAME = hvsdb HVS_CERT_SAN_LIST = hvs-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> HVS_DB_PORT = \"5432\" HVS_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2/ #Nats Servers configuration for TA and HVS #NATS_SERVERS=nats://<K8s control-plane IP/Hostname>:30222 # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP/K8s control-plane Hostname> # For Kubeadm # K8S_API_SERVER_CERT=/etc/kubernetes/pki/apiserver.crt K8S_API_SERVER_CERT = /etc/kubernetes/pki/apiserver.crt # This is valid for multinode deployment, should be populated once ihub is deployed successfully IHUB_PUB_KEY_PATH = HVS_BASE_URL = https://hvs-svc.isecl.svc.cluster.local:8443/hvs/v2 # TrustAgent # e.g TA_CERT_SAN_LIST=*.example.com,192.168.1.* TA_CERT_SAN_LIST = TPM_OWNER_SECRET = # Workload Agent WLA_SERVICE_USERNAME = wlauser@wls WLA_SERVICE_PASSWORD = wlaAdminPass # KBS ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_HOSTNAME = <KMIP IP/Hostname> KMIP_SERVER_IP = KMIP_SERVER_PORT = # Retrieve the following KMIP server\u2019s client certificate, client key and root ca certificate from the KMIP server. # This key and certificates will be available in KMIP server, /etc/pykmip is the default path copy them to this system manifests/kbs/kmip-secrets path KMIP_CLIENT_CERT_NAME = client_certificate.pem KMIP_CLIENT_KEY_NAME = client_key.pem KMIP_ROOT_CERT_NAME = root_certificate.pem # ISecl Scheduler # For Kubeadm # K8S_CA_KEY=/etc/kubernetes/pki/ca.key # K8S_CA_CERT=/etc/kubernetes/pki/ca.crt K8S_CA_KEY = /etc/kubernetes/pki/ca.key K8S_CA_CERT = /etc/kubernetes/pki/ca.crt # populate users.env ISECL_INSTALL_COMPONENTS = \"AAS,HVS,WLS,IHUB,KBS,WLA,TA,WPM\" #NATS_CERT_SAN_LIST= #NATS_TLS_COMMON_NAME= GLOBAL_ADMIN_USERNAME = GLOBAL_ADMIN_PASSWORD = INSTALL_ADMIN_USERNAME = INSTALL_ADMIN_PASSWORD = WPM_SERVICE_USERNAME = WPM_SERVICE_PASSWORD = CUSTOM_CLAIMS_COMPONENTS = CCC_ADMIN_USERNAME = CCC_ADMIN_PASSWORD = Note Ensure to update KMIP_CLIENT_CERT_NAME , KMIP_CLIENT_KEY_NAME , KMIP_ROOT_CERT_NAME in the env from /etc/pykmip of pykmip by copying the key and certs to this system under manifests/kbs/kmip-secrets path","title":"Update isecl-k8s.env file"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/6Deployment/#run-scripts-on-k8s-control-plane_1","text":"The bootstrap scripts are sample scripts to allow for a quick start of FS,WS services and agents. Users are free to modify the script or directly use the K8s manifests as per their deployment model requirements #Pre-reqs.sh ./pre-requisites.sh #isecl-bootstrap-db-services #Reference #Usage: ./isecl-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, Workload Service and Host verification Service # purge Delete Database Services for Authservice, Workload Service and Host verification Service ./isecl-bootstrap-db-services.sh up #isecl-bootstrap #Reference #Usage: ./isecl-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap ISecL K8s environment for #specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete ISecL K8s environment for specified #agent/service/usecase [will not delete data, config, logs] # purge Delete ISecL K8s environment with data, config, logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of tagent, wlagent # service Can be one of cms, authservice, hvs, ihub, wls, kbs, isecl-#controller, isecl-scheduler # usecase Can be one of foundational-security, workload-security, isecl-#orchestration-k8s, csp, enterprise ./isecl-bootstrap.sh up <all/usecase of choice> Note An error to create asymmetric key would mean the following line, RANDFILE = $ENV::HOME/.rnd needs to be commented under /etc/ssl/openssl.cnf Copy the ihub_public_key.pem from NFS path - <mnt>/isecl/ihub/config/ihub_public_key.pem to K8s control-plane Update the isecl-k8s.env for IHUB_PUB_KEY_PATH Bring up the isecl-k8s-scheduler ./isecl-bootstrap.sh up isecl-scheduler Create and update scheduler-policy.json path mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions Configure kube-scheduler to establish communication with isecl-scheduler. Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec : containers : - command : - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers : volumeMounts : - mountPath : /opt/isecl-k8s-extensions/ name : extendedsched readOnly : true volumes : - hostPath : path : /opt/isecl-k8s-extensions/ type : name : extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml Restart kubelet which restart all the k8s services including kube-scheduler systemctl restart kubelet","title":"Run scripts on K8s control-plane"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/7Default%20Service%20and%20Agent%20Mount%20Paths/","text":"Default Service and Agent Mount Paths Single Node Deployments Single node Deployments use hostPath mounting pod(container) files directly on host. Following is the complete list of the files being mounted on host #Certificate-Management-Service Config : /etc/cms Logs : /var/log/cms #Authentication Authorization Service Config : /etc/authservice Logs : /var/log/authservice Pg-data : /usr/local/kube/data/authservice/pgdata #Host Attestation Service Config : /etc/hvs Logs : /var/log/hvs Pg-data : /usr/local/kube/data/hvs #Integration-Hub Config : /etc/ihub Log : /var/log/ihub #Workload Service Config : /etc/workload-service Logs : /var/log/workload-service Pg-data : /usr/local/kube/data/workload-service #Key-Broker-Service Config : /etc/kbs Log : /var/log/kbs Opt : /opt/kbs #Trust Agent: Config : /opt/trustagent/configuration Logs : /var/log/trustagent/ tpmrm : /dev/tpmrm0 txt-stat : /usr/sbin/txt-stat ta-hostname-path : /etc/hostname ta-hosts-path : /etc/hosts #Workload Agent: Config : /etc/workload-agent/ Logs : /var/log/workload-agent TA Config : /opt/trustagent/configuration WLA-Socket : /var/run/workload-agent Multi Node Deployments Multi node Deployments use k8s persistent volume and persistent volume claims for mounting pod(container) files on NFS volumes for all services, agents will continue to use hostPath . Following is a sample list of the files being mounted on NFS base volumes #Certificate-Management-Service Config : <NFS-vol-base-path>/isecl/cms/config Logs : <NFS-vol-base-path>/isecl/cms/logs #Authentication Authorization Service Config : <NFS-vol-base-path>/isecl/aas/config Logs : <NFS-vol-base-path>/isecl/aas/logs Pg-data : <NFS-vol-base-path>/isecl/aas/db #Host Attestation Service Config : <NFS-vol-base-path>/isecl/hvs/config Logs : <NFS-vol-base-path>/isecl/hvs/logs Pg-data : <NFS-vol-base-path>/usr/local/kube/data/hvs #Integration-Hub Config : <NFS-vol-base-path>/isecl/ihub/config Log : <NFS-vol-base-path>/isecl/ihub/logs #Workload Service Config : <NFS-vol-base-path>/isecl/wls/config Logs : <NFS-vol-base-path>/isecl/wls/log Pg-data : <NFS-vol-base-path>/usr/local/kube/data/wls #Key-Broker-Service Config : <NFS-vol-base-path>/isecl/kbs/config Log : <NFS-vol-base-path>/isecl/kbs/logs Opt : <NFS-vol-base-path>/isecl/kbs/kbs/opt #Trust Agent: Config : /opt/trustagent/configuration Logs : /var/log/trustagent/ tpmrm : /dev/tpmrm0 txt-stat : /usr/sbin/txt-stat ta-hostname-path : /etc/hostname ta-hosts-path : /etc/hosts #Workload Agent: Config : /etc/workload-agent/ Logs : /var/log/workload-agent WLA-Socket : /var/run/workload-agent","title":"Default Service and Agent Mount Paths"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/7Default%20Service%20and%20Agent%20Mount%20Paths/#default-service-and-agent-mount-paths","text":"","title":"Default Service and Agent Mount Paths"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/7Default%20Service%20and%20Agent%20Mount%20Paths/#single-node-deployments","text":"Single node Deployments use hostPath mounting pod(container) files directly on host. Following is the complete list of the files being mounted on host #Certificate-Management-Service Config : /etc/cms Logs : /var/log/cms #Authentication Authorization Service Config : /etc/authservice Logs : /var/log/authservice Pg-data : /usr/local/kube/data/authservice/pgdata #Host Attestation Service Config : /etc/hvs Logs : /var/log/hvs Pg-data : /usr/local/kube/data/hvs #Integration-Hub Config : /etc/ihub Log : /var/log/ihub #Workload Service Config : /etc/workload-service Logs : /var/log/workload-service Pg-data : /usr/local/kube/data/workload-service #Key-Broker-Service Config : /etc/kbs Log : /var/log/kbs Opt : /opt/kbs #Trust Agent: Config : /opt/trustagent/configuration Logs : /var/log/trustagent/ tpmrm : /dev/tpmrm0 txt-stat : /usr/sbin/txt-stat ta-hostname-path : /etc/hostname ta-hosts-path : /etc/hosts #Workload Agent: Config : /etc/workload-agent/ Logs : /var/log/workload-agent TA Config : /opt/trustagent/configuration WLA-Socket : /var/run/workload-agent","title":"Single Node Deployments"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/7Default%20Service%20and%20Agent%20Mount%20Paths/#multi-node-deployments","text":"Multi node Deployments use k8s persistent volume and persistent volume claims for mounting pod(container) files on NFS volumes for all services, agents will continue to use hostPath . Following is a sample list of the files being mounted on NFS base volumes #Certificate-Management-Service Config : <NFS-vol-base-path>/isecl/cms/config Logs : <NFS-vol-base-path>/isecl/cms/logs #Authentication Authorization Service Config : <NFS-vol-base-path>/isecl/aas/config Logs : <NFS-vol-base-path>/isecl/aas/logs Pg-data : <NFS-vol-base-path>/isecl/aas/db #Host Attestation Service Config : <NFS-vol-base-path>/isecl/hvs/config Logs : <NFS-vol-base-path>/isecl/hvs/logs Pg-data : <NFS-vol-base-path>/usr/local/kube/data/hvs #Integration-Hub Config : <NFS-vol-base-path>/isecl/ihub/config Log : <NFS-vol-base-path>/isecl/ihub/logs #Workload Service Config : <NFS-vol-base-path>/isecl/wls/config Logs : <NFS-vol-base-path>/isecl/wls/log Pg-data : <NFS-vol-base-path>/usr/local/kube/data/wls #Key-Broker-Service Config : <NFS-vol-base-path>/isecl/kbs/config Log : <NFS-vol-base-path>/isecl/kbs/logs Opt : <NFS-vol-base-path>/isecl/kbs/kbs/opt #Trust Agent: Config : /opt/trustagent/configuration Logs : /var/log/trustagent/ tpmrm : /dev/tpmrm0 txt-stat : /usr/sbin/txt-stat ta-hostname-path : /etc/hostname ta-hosts-path : /etc/hosts #Workload Agent: Config : /etc/workload-agent/ Logs : /var/log/workload-agent WLA-Socket : /var/run/workload-agent","title":"Multi Node Deployments"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/8Default%20Service%20Ports/","text":"Default Service Ports For both Single node and multi-node deployments following ports are used. All services exposing APIs will use the below ports CMS : 30445 AAS : 30444 HVS : 30443 WLS : 30447 IHUB : None KBS : 30448 K8s-scheduler : 30888 K8s-controller : None TA : 31443 WLA : None","title":"Default Service Ports"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/8Default%20Service%20Ports/#default-service-ports","text":"For both Single node and multi-node deployments following ports are used. All services exposing APIs will use the below ports CMS : 30445 AAS : 30444 HVS : 30443 WLS : 30447 IHUB : None KBS : 30448 K8s-scheduler : 30888 K8s-controller : None TA : 31443 WLA : None","title":"Default Service Ports"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/","text":"Usecase Workflows API Collections The below allow to get started with workflows within Intel\u00ae SecL-DC for Foundational and Workload Security Usecases. More details available in API Collections repository Pre-requisites Postman client should be downloaded on supported platforms or on the web to get started with the usecase collections. Note The Postman API Network will always have the latest released version of the API Collections. For all releases, refer the github repository for API Collections Use Case Collections Use case Sub-Usecase API Collection Foundational Security Host Attestation(RHEL & VMWARE) \u2714\ufe0f Data Fencing with Asset Tags(RHEL & VMWARE) \u2714\ufe0f Trusted Workload Placement (Containers) \u2714\ufe0f Workload Security Container Confidentiality with Docker Runtime \u2714\ufe0f Container Confidentiality with CRIO Runtime \u2714\ufe0f Downloading API Collections Postman API Network for latest released: https://explore.postman.com/intelsecldc or Github repo for all releases #Clone the github repo for api-collections git clone https://github.com/intel-secl/utils.git #Switch to specific release-version of choice cd utils/ git checkout <release-version of choice> #Import Collections from cd tools/api-collections Note The postman-collections are also available when cloning the repos via build manifest under utils/tools/api-collections Running API Collections Import the collection into Postman API Client Note This step is required only when not using Postman API Network and downloading from Github Update env as per the deployment details for specific usecase View Documentation Run the workflow","title":"Usecase Workflows API Collections"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/#usecase-workflows-api-collections","text":"The below allow to get started with workflows within Intel\u00ae SecL-DC for Foundational and Workload Security Usecases. More details available in API Collections repository","title":"Usecase Workflows API Collections"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/#pre-requisites","text":"Postman client should be downloaded on supported platforms or on the web to get started with the usecase collections. Note The Postman API Network will always have the latest released version of the API Collections. For all releases, refer the github repository for API Collections","title":"Pre-requisites"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/#use-case-collections","text":"Use case Sub-Usecase API Collection Foundational Security Host Attestation(RHEL & VMWARE) \u2714\ufe0f Data Fencing with Asset Tags(RHEL & VMWARE) \u2714\ufe0f Trusted Workload Placement (Containers) \u2714\ufe0f Workload Security Container Confidentiality with Docker Runtime \u2714\ufe0f Container Confidentiality with CRIO Runtime \u2714\ufe0f","title":"Use Case Collections"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/#downloading-api-collections","text":"Postman API Network for latest released: https://explore.postman.com/intelsecldc or Github repo for all releases #Clone the github repo for api-collections git clone https://github.com/intel-secl/utils.git #Switch to specific release-version of choice cd utils/ git checkout <release-version of choice> #Import Collections from cd tools/api-collections Note The postman-collections are also available when cloning the repos via build manifest under utils/tools/api-collections","title":"Downloading API Collections"},{"location":"quick-start-guides/Foundational%20%26%20Workload%20Security%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/#running-api-collections","text":"Import the collection into Postman API Client Note This step is required only when not using Postman API Network and downloading from Github Update env as per the deployment details for specific usecase View Documentation Run the workflow","title":"Running API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/","text":"Appendix Running behind Proxy #Set proxy in ~/.bash_profile export http_proxy = <proxy-url> export https_proxy = <proxy-url> export no_proxy = <ip_address/hostname> Git Config Sample (~/.gitconfig) [user] name = <username> email = <email-id> [color] ui = auto [push] default = matching Rebuilding Repos In order to rebuild repos, ensure the following steps are followed as a pre-requisite # Clean all go-mod packages rm -rf ~/go/pkg/mod/* #Navigate to specific folder where repos are built, example below cd /root/isecl/build/skc-k8s-single-node rm -rf .repo * #Rebuild as before repo init ... repo sync #To rebuild single node make k8s-aio #To rebuild multi node make k8s SGX Attestation Flow To Build and obtain the sample_apps tar: cd into the repo folder (For single node 'cd /root/intel-secl/build/skc-k8s-single-node' and for multi node 'cd /root/intel-secl/build/skc-k8s-multi-node') make sample_apps mkdir -p binaries/ cp utils/build/skc-tools/sample_apps/build_scripts/sample_apps.* binaries/ cp utils/build/skc-tools/sample_apps/sampleapps_untar.sh binaries/ To Deploy SampleApp: Copy sample_apps.tar, sample_apps.sha2 and sampleapps_untar.sh from binaries directory to a directory in SGX compute node and untar it using './sample_apps_untar.sh' Install Intel\u00ae SGX SDK for Linux*OS into /opt/intel/sgxsdk using './install_sgxsdk.sh' Install SGX dependencies using './deploy_sgx_dependencies.sh' Note: Make sure to deploy SQVS with includetoken configuration as false. To Verify the SampleApp flow: Update sample_apps.conf with the following - IP address for SQVS services deployed on Enterprise system - IP address for SCS services deployed on CSP system - ENTERPRISE_CMS_IP should point to the IP of CMS service deployed on Enterprise system - Network Port numbers for SCS services deployed on CSP system - Network Port numbers for SQVS and CMS services deployed on Enterprise system - Set RUN_ATTESTING_APP to yes if user wants to run both apps in same machine Run SampleApp using './run_sample_apps.sh' Check the output of attestedApp and attestingApp under out/attested_app_console_out.log and out/attesting_app_console_out.log files SKC Key Transfer Flow Below steps to be followed post successful deployment with Single-Node/Multi-Node deployment Generating keys From cluster node, copy k8s/manifests/kbs/rsa_create.py to KMIP server, update KMIP server IP in rsa_create.py inside single quote and execute it using python3 rsa_create.py . It will generate the KMIP KEY ID and server.crt. Copy server.crt to cluster node. On cluster node, navigate to k8s/manifests/kbs/ Update kbs.conf with SYSTEM_IP , AAS_PORT , KBS_PORT , CMS_PORT , KMIP_KEY_ID and SERVER_CERT SYSTEM_IP : K8s control-plane IP AAS_PORT : k8s exposed port for AAS (default 30444) KBS_PORT : k8s exposed port for KBS (default 30448) CMS_PORT : k8s exposed port for CMS (default 30445) KMIP_KEY_ID : KMIP KEY ID obtained by running rsa_create.py on KMIP server. SERVER_CERT : Complete path of copied server.crt file obtained by running rsa_create.py on KMIP server. Generate the key using ./run.sh reg Note Before generating new key, every time we have to follow above 4 steps. rsa_create.py script will generate new KMIP_KEY_ID and SERVER_CERT everytime. Setup configurations Copy generated key to k8s/manifests/skc_library/resources Update create_roles.conf under k8s/manifests/skc_library/resources for AAS_IP to K8s control-plane & AAS_PORT to K8s pod exposed service port (Default: 30444 ) Execute skc_library_create_roles.sh to create skc token Update below files available under k8s/manifests/skc_library/resources with required values hosts Update K8s control-plane IP and K8s control-plane Host Name where we generate KBS key keys.txt Update KBS Key ID nginx.conf Update KBS Key ID in both ssl_certificate and ssl_certificate_key lines. sgx_default_qcnl.conf Update K8s control-plane IP and SCS K8s Service Port kms_npm.ini Update K8s control-plane IP and KBS K8s Service Port skc_library.conf Update KBS_HOSTNAME , KBS_IP and KBS_PORT with K8s control-plane Hostname, K8s control-plane IP and KBS K8s Service Port. Update CMS_IP and CMS_PORT with K8s control-plane IP and CMS K8s Service Port. Update CSP_SCS_IP and CSP_SCS_PORT with K8s control-plane IP and SCS K8s Service Port. Note Update skc token in the skc_library.conf generated in previous step Update mountPath and subPath with generated key id, along with image name in k8s/manifests/skc_library/deployment.yml Update isecl-skc-k8s.env by uncommenting KBS_PUBLIC_CERTIFICATE=<key id>.crt and update the <key id> value Initiate Key Transfer Flow Deploy the SKC Library using ./skc-bootstrap.sh up skclib It will initiate the key transfer Note To enable debug log, make debug=true in pkcs11-apimodule.ini, kms_npm.ini and sgx_stm.ini files available under k8s/manifests/skc_library/resources Establish tls session with the nginx using the key transferred inside the enclave wget https://<K8s control-plane IP>:30443 --no-check-certificate SGX Discovery Flow Create below sample pod.yml file for nginx workload and add SGX labels as node affinity to it. apiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: SGX-Enabled operator: In values: - \"true\" - key: EPC-Memory operator: In values: - \"2.0GB\" containers: - name: nginx image: nginx ports: - containerPort: 80 Launch the pod and validate if pod can be launched on the node. Run following commands: kubectl apply -f pod.yml kubectl get pods kubectl describe pod nginx Pod should be in running state and launched on the host as per values in yml. Validate by running below command on worker node: docker ps SKC Virtualization Flow Update the populate-users.env under aas/scripts directory for SCS_CERT_SAN_LIST and SHVS_CERT_SAN_LIST to add the SGX enabled node IP's Deploy all services on SGX Virtualization control plane Ensure to update agent.conf for deploying SGX agent service and run ./deploy_sgx_agent.sh to deploy SGX_AGENT as binary deployment Deploy SKC LIBRARY on SGX Host, update skc_library.conf and execute ./deploy_skc_library.sh Follow the Generating keys section to generate the keys Copy the generated Key to SGX Host Update keys.txt and nginx.conf Initiate key transfer Configuration for Key Transfer On SGX Virtualization Setup Note Below mentioned OpenSSL and NGINX configuration updates are provided as patches (nginx.patch and openssl.patch) as part of skc_library deployment script. OpenSSL Update openssl configuration file /etc/pki/tls/openssl.cnf with below changes [openssl_def] engines = engine_section [engine_section] pkcs11 = pkcs11_section [pkcs11_section] engine_id = pkcs11 dynamic_path = /usr/lib64/engines-1.1/pkcs11.so MODULE_PATH = /opt/skc/lib/libpkcs11-api.so init = 0 Nginx Update nginx configuration file /etc/nginx/nginx.conf with below changes ssl_engine pkcs11; Update the location of certificate with the loaction where it was copied into the skc_library machine. ssl_certificate \"add absolute path of crt file\"; Update the KeyID with the KeyID received when RSA key was generated in KBS ssl_certificate_key \"engine:pkcs11:pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234\"; SKC Configuration Create keys.txt in /root folder. This provides key preloading functionality in skc_library. Note Any number of keys can be added in keys.txt. Each PKCS11 URL should contain different Key ID which need to be transferred from KBS along with respective object tag for each key id specified Sample PKCS11 url is as below pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234; Note Last PKCS11 url entry in keys.txt should match with the one in nginx.conf The keyID should match the keyID of RSA key created in KBS. Other contents should match with nginx.conf . File location should match with preload_keys directive in pkcs11-apimodule.ini Sample /opt/skc/etc/pkcs11-apimodule.ini file [core] preload_keys = /root/keys.txt keyagent_conf = /opt/skc/etc/key-agent.ini mode = SGX debug = true [SW] module = /usr/lib64/pkcs11/libsofthsm2.so [SGX] module = /opt/intel/cryptoapitoolkit/lib/libp11sgx.so Key-transfer flow validation On SGX Compute node, Execute below commands for KBS key-transfer: pkill nginx Remove any existing pkcs11 token rm -rf /opt/intel/cryptoapitoolkit/tokens/* Initiate Key transfer from KBS systemctl restart nginx Changing group ownership and permissions of pkcs11 token chown -R root:intel /opt/intel/cryptoapitoolkit/tokens/ chmod -R 770 /opt/intel/cryptoapitoolkit/tokens/ Establish a tls session with the nginx using the key transferred inside the enclave wget https://localhost:2443 --no-check-certificate Note on Key Transfer Policy Key Transfer Policy is used to enforce a set of policies which need to be complied before the secret can be securely provisioned onto a sgx enclave Sample Key Transfer Policy: \"sgx_enclave_issuer_anyof\" :[\"cd171c56941c6ce49690b455f691d9c8a04c2e43e0a4d30f752fa5285c7ee57f\"], \"sgx_enclave_issuer_product_id_anyof\":[0], \"sgx_enclave_measurement_anyof\":[\"7df0b7e815bd4b4af41239038d04a740daccf0beb412a2056c8d900b45b621fd\"], \"tls_client_certificate_issuer_cn_anyof\":[\"CMSCA\", \"CMS TLS Client CA\"], \"client_permissions_allof\":[\"nginx\",\"USA\"], \"sgx_enforce_tcb_up_to_date\":false a. sgx_enclave_issuer_anyof - Establishes the signing identity provided by an authority who has signed the SGX enclave. In other words the owner of the enclave. b. sgx_enclave_measurement_anyof - Represents the cryptographic hash of the enclave log (enclave code, data) c. sgx_enforce_tcb_up_to_date - If set to true, Key Broker service will provision the key only of the platform generating the quote conforms to the latest Trusted Computing Base d. client_permissions_allof - Special permission embedded into the skc_library client TLS certificate which can enforce additional restrictions on who can get access to the key. In the above example, key is provisioned only to the nginx workload and platform which is tagged with value for ex: USA Note on SKC Library Deployment SKC Library Deployment needs to performed with root privilege Each container instance of workload gets its own private SKC Client Library config information The SKC Client Library TLS client certificate private key is stored in the configuration directories and can be read only with elevated root privileges keys.txt (set of PKCS11 URIs for the keys to be securely provisioned into an SGX enclave) can only be modified with elevated privileges Extracting SGX Enclave values for Key Transfer Policy Values that are specific to the enclave such as sgx_enclave_issuer_anyof , sgx_enclave_measurement_anyof and sgx_enclave_issuer_product_id_anyof can be retrieved using sgx_sign utility that is available as part of Intel SGX SDK. Run sgx_sign utility on the signed enclave (This command should be run on the build system). /opt/intel/sgxsdk/bin/x64/sgx_sign dump -enclave <path to the signed enclave> -dumpfile info.txt For sgx_enclave_issuer_anyof , in info.txt, search for mrsigner->value . E.g.. mrsigner->value : mrsigner->value: \"0x83 0xd7 0x19 0xe7 0x7d 0xea 0xca 0x14 0x70 0xf6 0xba 0xf6 0x2a 0x4d 0x77 0x43 0x03 0xc8 0x99 0xdb 0x69 0x02 0x0f 0x9c 0x70 0xee 0x1d 0xfc 0x08 0xc7 0xce 0x9e\" Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g.. : \"sgx_enclave_issuer_anyof\" :[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"] For sgx_enclave_measurement_anyof , in info.txt, search for metadata->enclave_css.body.enclave_hash.m . E.g. metadata->enclave_css.body.enclave_hash.m : metadata->enclave_css.body.enclave_hash.m: 0xad 0x46 0x74 0x9e 0xd4 0x1e 0xba 0xa2 0x32 0x72 0x52 0x04 0x1e 0xe7 0x46 0xd3 0x79 0x1a 0x9f 0x24 0x31 0x83 0x0f 0xee 0x08 0x83 0xf7 0x99 0x3c 0xaf 0x31 0x6a Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_measurement_anyof\" : [ \"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\" ] Please note that the SGX Enclave measurement value will depend on the toolchain used to build and link the SGX enclave. Hence the SGX Enclave measurement value would differ across OS flavours. For more details please refer https://github.com/intel/linux-sgx/tree/master/linux/reproducibility Setup Task Flow Setup tasks flows have been updated to have K8s native flow to be more agnostic to K8s workflows. Following would be the flow for setup task User would create a new configMap object with the environment variables specific to the setup task. The Setup task variables would be documented in the Product Guide Users can provide variables for more than one setup task If variables involve BEARER_TOKEN then user need to delete the corresponding secret using kubectl delete secret <service_name>-secret -n isecl , update the new token inside secrets.txt file and re-create the secret using kubectl create secret generic <service_name>-secret --from-file=secrets.txt --namespace=isecl , else update the variables in configMap.yml Users need to add SETUP_TASK: \"<setup task name>/<comma separated setup task name>\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f - Configuration Update Flow Configuration Update flows have been updated to have K8s native flow to be more agnostic to K8s workflows using configMap only. Following would be the flow for configuration update User would create a new configMap object using existing one and update the new values. The list of config variables would be documented in the Product Guide Users can update variables for more than one Users need to add SETUP_TASK: \"update-service-config\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f - Note Incase of agents, setup tasks or configuration updates done through above flows will be applied for all the agents running on different BMs. In order to run setup task or update configuration for individual agents, then user need to perform kubectl exec -it <pod_name> /bin/bash into a particular agent pod and run the specific setup task. Cleanup workflows Single-node In order to cleanup and setup fresh again on single node without data, config from previous deployment #Purge all data and pods,deploy,cm,secrets ./skc-bootstrap.sh purge <all/usecase of choice> #Purge all db data,pods,deploy,cm,secrets ./skc-bootstrap-db-services.sh purge #Comment/Remove the following lines from /var/snap/microk8s/current/args/kube-scheduler --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service #Setup fresh ./skc-bootstrap-db-services.sh up ./skc-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service In order to cleanup and setup fresh again on single node with data, config from previous deployment #Down all data and pods,deploy,cm,secrets with deleting config,data,logs ./skc-bootstrap.sh down <all/usecase of choice> #Comment/Remove the following lines from /var/snap/microk8s/current/args/kube-scheduler --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service #Setup fresh ./skc-bootstrap-db-services.sh up ./skc-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service In order to clean single service and bring up again on single node without data, config from previous deployment ./skc-bootstrap.sh down <service-name> rm -rf /etc/<service-name> rm -rf /var/log/<service-name> #Only in case of KBS, perform one more step along with above 2 steps rm -rf /opt/kbs ./skc-bootstrap.sh up <service-name> In order to redeploy again on single node with data, config from previous deployment ./skc-bootstrap.sh down <service-name> ./skc-bootstrap.sh up <service-name> Multi-node In order to cleanup and setup fresh again on multi-node with data,config from previous deployment #Purge all data and pods,deploy,cm,secrets ./skc-bootstrap.sh down <all/usecase of choice> #Delete 'scheduler-policy.json' rm -rf /opt/isecl-k8s-extensions #Comment/Remove '- --policy-config-file=...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Comment/Remove '- mountPath: ...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true #Comment/Remove '- hostPath:...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet #Purge all db data,pods,deploy,cm,secrets ./skc-bootstrap-db-services.sh purge #Cleanup all data from NFS share --> User controlled #Cleanup data from each worker node --> User controlled rm -rf /etc/sgx_agent rm -rf /var/log/sgx_agent #Setup fresh ./skc-bootstrap-db-services.sh up ./skc-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet In order to cleanup and setup fresh again on multi-node without removing data,config from previous deployment #Down all pods,deploy,cm,secrets with removing persistent data ./skc-bootstrap.sh down <all/usecase of choice> #Delete 'scheduler-policy.json' rm -rf /opt/isecl-k8s-extensions #Comment/Remove '--policy-config-file=...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Comment/Remove '- mountPath: ...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true #Comment/Remove '-hostPath:...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet #Setup fresh ./skc-bootstrap-db-services.sh up ./skc-bootstrap.sh up <all/usecase of choice> #Setup fresh ./skc-bootstrap-db-services.sh up ./skc-bootstrap.sh up <all/usecase of choice> #Copy ihub_public_key.pem from <NFS-PATH>/ihub/config/ to K8s control-plane and update IHUB_PUB_KEY_PATH in isecl-skc-k8s.env #Bootstrap isecl-scheduler ./skc-bootstrap.sh up isecl-scheduler #Reconfigure K8s-scheduler containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet In order to clean single service and bring up again on multi node without data, config from previous deployment ./skc-bootstrap.sh down <service-name> log into nfs system rm -rf /<nfs-mount-path>/isecl/<service-name>/config rm -rf /<nfs-mount-path>/isecl/<service-name>/logs #Only in case of KBS, perform one more step along with above 2 steps rm -rf /<nfs-mount-path>/isecl/<service-name>/opt log into K8s control-plane ./skc-bootstrap.sh up <service-name> In order to redeploy again on multi node with data, config from previous deployment ./skc-bootstrap.sh down <service-name> ./skc-bootstrap.sh up <service-name>","title":"Appendix"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#appendix","text":"","title":"Appendix"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#running-behind-proxy","text":"#Set proxy in ~/.bash_profile export http_proxy = <proxy-url> export https_proxy = <proxy-url> export no_proxy = <ip_address/hostname>","title":"Running behind Proxy"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#git-config-sample-gitconfig","text":"[user] name = <username> email = <email-id> [color] ui = auto [push] default = matching","title":"Git Config Sample (~/.gitconfig)"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#rebuilding-repos","text":"In order to rebuild repos, ensure the following steps are followed as a pre-requisite # Clean all go-mod packages rm -rf ~/go/pkg/mod/* #Navigate to specific folder where repos are built, example below cd /root/isecl/build/skc-k8s-single-node rm -rf .repo * #Rebuild as before repo init ... repo sync #To rebuild single node make k8s-aio #To rebuild multi node make k8s","title":"Rebuilding Repos"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#sgx-attestation-flow","text":"To Build and obtain the sample_apps tar: cd into the repo folder (For single node 'cd /root/intel-secl/build/skc-k8s-single-node' and for multi node 'cd /root/intel-secl/build/skc-k8s-multi-node') make sample_apps mkdir -p binaries/ cp utils/build/skc-tools/sample_apps/build_scripts/sample_apps.* binaries/ cp utils/build/skc-tools/sample_apps/sampleapps_untar.sh binaries/ To Deploy SampleApp: Copy sample_apps.tar, sample_apps.sha2 and sampleapps_untar.sh from binaries directory to a directory in SGX compute node and untar it using './sample_apps_untar.sh' Install Intel\u00ae SGX SDK for Linux*OS into /opt/intel/sgxsdk using './install_sgxsdk.sh' Install SGX dependencies using './deploy_sgx_dependencies.sh' Note: Make sure to deploy SQVS with includetoken configuration as false. To Verify the SampleApp flow: Update sample_apps.conf with the following - IP address for SQVS services deployed on Enterprise system - IP address for SCS services deployed on CSP system - ENTERPRISE_CMS_IP should point to the IP of CMS service deployed on Enterprise system - Network Port numbers for SCS services deployed on CSP system - Network Port numbers for SQVS and CMS services deployed on Enterprise system - Set RUN_ATTESTING_APP to yes if user wants to run both apps in same machine Run SampleApp using './run_sample_apps.sh' Check the output of attestedApp and attestingApp under out/attested_app_console_out.log and out/attesting_app_console_out.log files","title":"SGX Attestation Flow"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#skc-key-transfer-flow","text":"Below steps to be followed post successful deployment with Single-Node/Multi-Node deployment","title":"SKC Key Transfer Flow"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#generating-keys","text":"From cluster node, copy k8s/manifests/kbs/rsa_create.py to KMIP server, update KMIP server IP in rsa_create.py inside single quote and execute it using python3 rsa_create.py . It will generate the KMIP KEY ID and server.crt. Copy server.crt to cluster node. On cluster node, navigate to k8s/manifests/kbs/ Update kbs.conf with SYSTEM_IP , AAS_PORT , KBS_PORT , CMS_PORT , KMIP_KEY_ID and SERVER_CERT SYSTEM_IP : K8s control-plane IP AAS_PORT : k8s exposed port for AAS (default 30444) KBS_PORT : k8s exposed port for KBS (default 30448) CMS_PORT : k8s exposed port for CMS (default 30445) KMIP_KEY_ID : KMIP KEY ID obtained by running rsa_create.py on KMIP server. SERVER_CERT : Complete path of copied server.crt file obtained by running rsa_create.py on KMIP server. Generate the key using ./run.sh reg Note Before generating new key, every time we have to follow above 4 steps. rsa_create.py script will generate new KMIP_KEY_ID and SERVER_CERT everytime.","title":"Generating keys"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#setup-configurations","text":"Copy generated key to k8s/manifests/skc_library/resources Update create_roles.conf under k8s/manifests/skc_library/resources for AAS_IP to K8s control-plane & AAS_PORT to K8s pod exposed service port (Default: 30444 ) Execute skc_library_create_roles.sh to create skc token Update below files available under k8s/manifests/skc_library/resources with required values hosts Update K8s control-plane IP and K8s control-plane Host Name where we generate KBS key keys.txt Update KBS Key ID nginx.conf Update KBS Key ID in both ssl_certificate and ssl_certificate_key lines. sgx_default_qcnl.conf Update K8s control-plane IP and SCS K8s Service Port kms_npm.ini Update K8s control-plane IP and KBS K8s Service Port skc_library.conf Update KBS_HOSTNAME , KBS_IP and KBS_PORT with K8s control-plane Hostname, K8s control-plane IP and KBS K8s Service Port. Update CMS_IP and CMS_PORT with K8s control-plane IP and CMS K8s Service Port. Update CSP_SCS_IP and CSP_SCS_PORT with K8s control-plane IP and SCS K8s Service Port. Note Update skc token in the skc_library.conf generated in previous step Update mountPath and subPath with generated key id, along with image name in k8s/manifests/skc_library/deployment.yml Update isecl-skc-k8s.env by uncommenting KBS_PUBLIC_CERTIFICATE=<key id>.crt and update the <key id> value","title":"Setup configurations"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#initiate-key-transfer-flow","text":"Deploy the SKC Library using ./skc-bootstrap.sh up skclib It will initiate the key transfer Note To enable debug log, make debug=true in pkcs11-apimodule.ini, kms_npm.ini and sgx_stm.ini files available under k8s/manifests/skc_library/resources Establish tls session with the nginx using the key transferred inside the enclave wget https://<K8s control-plane IP>:30443 --no-check-certificate","title":"Initiate Key Transfer Flow"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#sgx-discovery-flow","text":"Create below sample pod.yml file for nginx workload and add SGX labels as node affinity to it. apiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: SGX-Enabled operator: In values: - \"true\" - key: EPC-Memory operator: In values: - \"2.0GB\" containers: - name: nginx image: nginx ports: - containerPort: 80 Launch the pod and validate if pod can be launched on the node. Run following commands: kubectl apply -f pod.yml kubectl get pods kubectl describe pod nginx Pod should be in running state and launched on the host as per values in yml. Validate by running below command on worker node: docker ps","title":"SGX Discovery Flow"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#skc-virtualization-flow","text":"Update the populate-users.env under aas/scripts directory for SCS_CERT_SAN_LIST and SHVS_CERT_SAN_LIST to add the SGX enabled node IP's Deploy all services on SGX Virtualization control plane Ensure to update agent.conf for deploying SGX agent service and run ./deploy_sgx_agent.sh to deploy SGX_AGENT as binary deployment Deploy SKC LIBRARY on SGX Host, update skc_library.conf and execute ./deploy_skc_library.sh Follow the Generating keys section to generate the keys Copy the generated Key to SGX Host Update keys.txt and nginx.conf Initiate key transfer","title":"SKC Virtualization Flow"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#configuration-for-key-transfer-on-sgx-virtualization-setup","text":"Note Below mentioned OpenSSL and NGINX configuration updates are provided as patches (nginx.patch and openssl.patch) as part of skc_library deployment script.","title":"Configuration for Key Transfer On SGX Virtualization Setup"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#openssl","text":"Update openssl configuration file /etc/pki/tls/openssl.cnf with below changes [openssl_def] engines = engine_section [engine_section] pkcs11 = pkcs11_section [pkcs11_section] engine_id = pkcs11 dynamic_path = /usr/lib64/engines-1.1/pkcs11.so MODULE_PATH = /opt/skc/lib/libpkcs11-api.so init = 0","title":"OpenSSL"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#nginx","text":"Update nginx configuration file /etc/nginx/nginx.conf with below changes ssl_engine pkcs11; Update the location of certificate with the loaction where it was copied into the skc_library machine. ssl_certificate \"add absolute path of crt file\"; Update the KeyID with the KeyID received when RSA key was generated in KBS ssl_certificate_key \"engine:pkcs11:pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234\";","title":"Nginx"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#skc-configuration","text":"Create keys.txt in /root folder. This provides key preloading functionality in skc_library. Note Any number of keys can be added in keys.txt. Each PKCS11 URL should contain different Key ID which need to be transferred from KBS along with respective object tag for each key id specified Sample PKCS11 url is as below pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234; Note Last PKCS11 url entry in keys.txt should match with the one in nginx.conf The keyID should match the keyID of RSA key created in KBS. Other contents should match with nginx.conf . File location should match with preload_keys directive in pkcs11-apimodule.ini Sample /opt/skc/etc/pkcs11-apimodule.ini file [core] preload_keys = /root/keys.txt keyagent_conf = /opt/skc/etc/key-agent.ini mode = SGX debug = true [SW] module = /usr/lib64/pkcs11/libsofthsm2.so [SGX] module = /opt/intel/cryptoapitoolkit/lib/libp11sgx.so","title":"SKC Configuration"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#key-transfer-flow-validation","text":"On SGX Compute node, Execute below commands for KBS key-transfer: pkill nginx Remove any existing pkcs11 token rm -rf /opt/intel/cryptoapitoolkit/tokens/* Initiate Key transfer from KBS systemctl restart nginx Changing group ownership and permissions of pkcs11 token chown -R root:intel /opt/intel/cryptoapitoolkit/tokens/ chmod -R 770 /opt/intel/cryptoapitoolkit/tokens/ Establish a tls session with the nginx using the key transferred inside the enclave wget https://localhost:2443 --no-check-certificate","title":"Key-transfer flow validation"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#note-on-key-transfer-policy","text":"Key Transfer Policy is used to enforce a set of policies which need to be complied before the secret can be securely provisioned onto a sgx enclave Sample Key Transfer Policy: \"sgx_enclave_issuer_anyof\" :[\"cd171c56941c6ce49690b455f691d9c8a04c2e43e0a4d30f752fa5285c7ee57f\"], \"sgx_enclave_issuer_product_id_anyof\":[0], \"sgx_enclave_measurement_anyof\":[\"7df0b7e815bd4b4af41239038d04a740daccf0beb412a2056c8d900b45b621fd\"], \"tls_client_certificate_issuer_cn_anyof\":[\"CMSCA\", \"CMS TLS Client CA\"], \"client_permissions_allof\":[\"nginx\",\"USA\"], \"sgx_enforce_tcb_up_to_date\":false a. sgx_enclave_issuer_anyof - Establishes the signing identity provided by an authority who has signed the SGX enclave. In other words the owner of the enclave. b. sgx_enclave_measurement_anyof - Represents the cryptographic hash of the enclave log (enclave code, data) c. sgx_enforce_tcb_up_to_date - If set to true, Key Broker service will provision the key only of the platform generating the quote conforms to the latest Trusted Computing Base d. client_permissions_allof - Special permission embedded into the skc_library client TLS certificate which can enforce additional restrictions on who can get access to the key. In the above example, key is provisioned only to the nginx workload and platform which is tagged with value for ex: USA","title":"Note on Key Transfer Policy"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#note-on-skc-library-deployment","text":"SKC Library Deployment needs to performed with root privilege Each container instance of workload gets its own private SKC Client Library config information The SKC Client Library TLS client certificate private key is stored in the configuration directories and can be read only with elevated root privileges keys.txt (set of PKCS11 URIs for the keys to be securely provisioned into an SGX enclave) can only be modified with elevated privileges","title":"Note on SKC Library Deployment"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#extracting-sgx-enclave-values-for-key-transfer-policy","text":"Values that are specific to the enclave such as sgx_enclave_issuer_anyof , sgx_enclave_measurement_anyof and sgx_enclave_issuer_product_id_anyof can be retrieved using sgx_sign utility that is available as part of Intel SGX SDK. Run sgx_sign utility on the signed enclave (This command should be run on the build system). /opt/intel/sgxsdk/bin/x64/sgx_sign dump -enclave <path to the signed enclave> -dumpfile info.txt For sgx_enclave_issuer_anyof , in info.txt, search for mrsigner->value . E.g.. mrsigner->value : mrsigner->value: \"0x83 0xd7 0x19 0xe7 0x7d 0xea 0xca 0x14 0x70 0xf6 0xba 0xf6 0x2a 0x4d 0x77 0x43 0x03 0xc8 0x99 0xdb 0x69 0x02 0x0f 0x9c 0x70 0xee 0x1d 0xfc 0x08 0xc7 0xce 0x9e\" Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g.. : \"sgx_enclave_issuer_anyof\" :[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"] For sgx_enclave_measurement_anyof , in info.txt, search for metadata->enclave_css.body.enclave_hash.m . E.g. metadata->enclave_css.body.enclave_hash.m : metadata->enclave_css.body.enclave_hash.m: 0xad 0x46 0x74 0x9e 0xd4 0x1e 0xba 0xa2 0x32 0x72 0x52 0x04 0x1e 0xe7 0x46 0xd3 0x79 0x1a 0x9f 0x24 0x31 0x83 0x0f 0xee 0x08 0x83 0xf7 0x99 0x3c 0xaf 0x31 0x6a Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_measurement_anyof\" : [ \"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\" ] Please note that the SGX Enclave measurement value will depend on the toolchain used to build and link the SGX enclave. Hence the SGX Enclave measurement value would differ across OS flavours. For more details please refer https://github.com/intel/linux-sgx/tree/master/linux/reproducibility","title":"Extracting SGX Enclave values for Key Transfer Policy"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#setup-task-flow","text":"Setup tasks flows have been updated to have K8s native flow to be more agnostic to K8s workflows. Following would be the flow for setup task User would create a new configMap object with the environment variables specific to the setup task. The Setup task variables would be documented in the Product Guide Users can provide variables for more than one setup task If variables involve BEARER_TOKEN then user need to delete the corresponding secret using kubectl delete secret <service_name>-secret -n isecl , update the new token inside secrets.txt file and re-create the secret using kubectl create secret generic <service_name>-secret --from-file=secrets.txt --namespace=isecl , else update the variables in configMap.yml Users need to add SETUP_TASK: \"<setup task name>/<comma separated setup task name>\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f -","title":"Setup Task Flow"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#configuration-update-flow","text":"Configuration Update flows have been updated to have K8s native flow to be more agnostic to K8s workflows using configMap only. Following would be the flow for configuration update User would create a new configMap object using existing one and update the new values. The list of config variables would be documented in the Product Guide Users can update variables for more than one Users need to add SETUP_TASK: \"update-service-config\" in the same configMap Provide a unique name to the new configMap Provide the same name in the deployment.yaml under configMapRef section Deploy the specific service again with kubectl kustomize . | kubectl apply -f - Note Incase of agents, setup tasks or configuration updates done through above flows will be applied for all the agents running on different BMs. In order to run setup task or update configuration for individual agents, then user need to perform kubectl exec -it <pod_name> /bin/bash into a particular agent pod and run the specific setup task.","title":"Configuration Update Flow"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#cleanup-workflows","text":"","title":"Cleanup workflows"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#single-node","text":"In order to cleanup and setup fresh again on single node without data, config from previous deployment #Purge all data and pods,deploy,cm,secrets ./skc-bootstrap.sh purge <all/usecase of choice> #Purge all db data,pods,deploy,cm,secrets ./skc-bootstrap-db-services.sh purge #Comment/Remove the following lines from /var/snap/microk8s/current/args/kube-scheduler --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service #Setup fresh ./skc-bootstrap-db-services.sh up ./skc-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service In order to cleanup and setup fresh again on single node with data, config from previous deployment #Down all data and pods,deploy,cm,secrets with deleting config,data,logs ./skc-bootstrap.sh down <all/usecase of choice> #Comment/Remove the following lines from /var/snap/microk8s/current/args/kube-scheduler --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service #Setup fresh ./skc-bootstrap-db-services.sh up ./skc-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service In order to clean single service and bring up again on single node without data, config from previous deployment ./skc-bootstrap.sh down <service-name> rm -rf /etc/<service-name> rm -rf /var/log/<service-name> #Only in case of KBS, perform one more step along with above 2 steps rm -rf /opt/kbs ./skc-bootstrap.sh up <service-name> In order to redeploy again on single node with data, config from previous deployment ./skc-bootstrap.sh down <service-name> ./skc-bootstrap.sh up <service-name>","title":"Single-node"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/10Appendix/#multi-node","text":"In order to cleanup and setup fresh again on multi-node with data,config from previous deployment #Purge all data and pods,deploy,cm,secrets ./skc-bootstrap.sh down <all/usecase of choice> #Delete 'scheduler-policy.json' rm -rf /opt/isecl-k8s-extensions #Comment/Remove '- --policy-config-file=...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Comment/Remove '- mountPath: ...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true #Comment/Remove '- hostPath:...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet #Purge all db data,pods,deploy,cm,secrets ./skc-bootstrap-db-services.sh purge #Cleanup all data from NFS share --> User controlled #Cleanup data from each worker node --> User controlled rm -rf /etc/sgx_agent rm -rf /var/log/sgx_agent #Setup fresh ./skc-bootstrap-db-services.sh up ./skc-bootstrap.sh up <all/usecase of choice> #Reconfigure K8s-scheduler containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet In order to cleanup and setup fresh again on multi-node without removing data,config from previous deployment #Down all pods,deploy,cm,secrets with removing persistent data ./skc-bootstrap.sh down <all/usecase of choice> #Delete 'scheduler-policy.json' rm -rf /opt/isecl-k8s-extensions #Comment/Remove '--policy-config-file=...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Comment/Remove '- mountPath: ...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true #Comment/Remove '-hostPath:...' from /etc/kubernetes/manifests/kube-scheduler.yaml as below volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet #Setup fresh ./skc-bootstrap-db-services.sh up ./skc-bootstrap.sh up <all/usecase of choice> #Setup fresh ./skc-bootstrap-db-services.sh up ./skc-bootstrap.sh up <all/usecase of choice> #Copy ihub_public_key.pem from <NFS-PATH>/ihub/config/ to K8s control-plane and update IHUB_PUB_KEY_PATH in isecl-skc-k8s.env #Bootstrap isecl-scheduler ./skc-bootstrap.sh up isecl-scheduler #Reconfigure K8s-scheduler containers: - command: - kube-scheduler - --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched #Restart kubelet systemctl restart kubelet In order to clean single service and bring up again on multi node without data, config from previous deployment ./skc-bootstrap.sh down <service-name> log into nfs system rm -rf /<nfs-mount-path>/isecl/<service-name>/config rm -rf /<nfs-mount-path>/isecl/<service-name>/logs #Only in case of KBS, perform one more step along with above 2 steps rm -rf /<nfs-mount-path>/isecl/<service-name>/opt log into K8s control-plane ./skc-bootstrap.sh up <service-name> In order to redeploy again on multi node with data, config from previous deployment ./skc-bootstrap.sh down <service-name> ./skc-bootstrap.sh up <service-name>","title":"Multi-node"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/","text":"Hardware & OS Requirements Machines RHEL 8.2 Build Machine K8s control-plane Node Setup on CSP (VMs/Physical Nodes + SGX enabled Physical Nodes) K8s control-plane Setup on Enterprise (VMs/Physical Nodes) OS Requirements RHEL 8.2 for build RHEL 8.2 or Ubuntu 18.04 for K8s cluster deployments Note SKC Solution is built, installed and tested with root privileges. Please ensure that all the following instructions are executed with root privileges Container Runtime Docker K8s Distributions Single Node: microk8s Multi Node: kubeadm Storage hostPath in case of single-node microk8s for all services and agents NFS in case of multi-node kubeadm for services and hostPath for sgx_agent and skc_library","title":"Hardware & OS Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#hardware-os-requirements","text":"","title":"Hardware &amp; OS Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#machines","text":"RHEL 8.2 Build Machine K8s control-plane Node Setup on CSP (VMs/Physical Nodes + SGX enabled Physical Nodes) K8s control-plane Setup on Enterprise (VMs/Physical Nodes)","title":"Machines"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#os-requirements","text":"RHEL 8.2 for build RHEL 8.2 or Ubuntu 18.04 for K8s cluster deployments Note SKC Solution is built, installed and tested with root privileges. Please ensure that all the following instructions are executed with root privileges","title":"OS Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#container-runtime","text":"Docker","title":"Container Runtime"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#k8s-distributions","text":"Single Node: microk8s Multi Node: kubeadm","title":"K8s Distributions"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/1Hardware%20%26%20OS%20Requirements/#storage","text":"hostPath in case of single-node microk8s for all services and agents NFS in case of multi-node kubeadm for services and hostPath for sgx_agent and skc_library","title":"Storage"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/2Network%20Requirements/","text":"Network Requirements Build System Internet access required CSP Managed Services Internet access required for SGX Caching Service deployed on CSP system/SGX Compute Node; Enterprise Managed Services Internet access required for SGX Caching Service deployed on Enterprise system; SGX Enabled Host Internet access required to access KBS running on Enterprise environment Firewall Settings Ensure that all the SKC service ports are accessible with firewall","title":"Network Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/2Network%20Requirements/#network-requirements","text":"","title":"Network Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/2Network%20Requirements/#build-system","text":"Internet access required","title":"Build System"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/2Network%20Requirements/#csp-managed-services","text":"Internet access required for SGX Caching Service deployed on CSP system/SGX Compute Node;","title":"CSP Managed Services"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/2Network%20Requirements/#enterprise-managed-services","text":"Internet access required for SGX Caching Service deployed on Enterprise system;","title":"Enterprise Managed Services"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/2Network%20Requirements/#sgx-enabled-host","text":"Internet access required to access KBS running on Enterprise environment","title":"SGX Enabled Host"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/2Network%20Requirements/#firewall-settings","text":"Ensure that all the SKC service ports are accessible with firewall","title":"Firewall Settings"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/3RHEL%20RPMs%20Requirements/","text":"RHEL RPMs Requirements Access required for the following rpms in all systems BaseOS Appstream CodeReady","title":"RHEL RPMs Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/3RHEL%20RPMs%20Requirements/#rhel-rpms-requirements","text":"Access required for the following rpms in all systems BaseOS Appstream CodeReady","title":"RHEL RPMs Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/4Deployment%20Model/","text":"Deployment Model Single Node The single Node uses microk8s as a supported K8s distribution Multi Node The multi node supports kubeadm as a supported K8s distribution","title":"Deployment Model"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/4Deployment%20Model/#deployment-model","text":"","title":"Deployment Model"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/4Deployment%20Model/#single-node","text":"The single Node uses microk8s as a supported K8s distribution","title":"Single Node"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/4Deployment%20Model/#multi-node","text":"The multi node supports kubeadm as a supported K8s distribution","title":"Multi Node"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/5Build/","text":"Build Pre-requisites The below steps need to be done on RHEL 8.2 Build machine (VM/Physical Node) Development Tools and Utilities dnf install -y git wget tar python3 gcc gcc-c++ zip tar make yum-utils openssl-devel dnf install -y https://dl.fedoraproject.org/pub/fedora/linux/releases/32/Everything/x86_64/os/Packages/m/makeself-2.4.0-5.fc32.noarch.rpm ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip Repo tool tmpdir = $( mktemp -d ) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir /repo /usr/local/bin rm -rf $tmpdir Golang wget https://dl.google.com/go/go1.14.4.linux-amd64.tar.gz tar -xzf go1.14.4.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT = /usr/local/go export PATH = $GOROOT /bin: $PATH rm -rf go1.14.4.linux-amd64.tar.gz Docker dnf module enable -y container-tools dnf install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo dnf install -y docker-ce-19.03.13 docker-ce-cli-19.03.13 systemctl enable docker systemctl start docker #Ignore the below steps if not running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" systemctl daemon-reload systemctl restart docker Skopeo dnf install -y skopeo Build OCI Container images and K8s Manifests The build process for OCI containers images and K8s manifests for RHEL 8.2 & Ubuntu 18.04 deployments must be done on RHEL 8.2 machine only Single Node Sync the repos mkdir -p /root/intel-secl/build/skc-k8s-single-node && cd /root/intel-secl/build/skc-k8s-single-node repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/skc.xml repo sync Build make k8s-aio Built Container images,K8s manifests and deployment scripts /root/intel-secl/build/skc-k8s-single-node/k8s/ Multi Node Sync the repos mkdir -p /root/intel-secl/build/skc-k8s-multi-node && cd /root/intel-secl/build/skc-k8s-multi-node repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/skc.xml repo sync Build make k8s Built Container images,K8s manifests and deployment scripts /root/intel-secl/build/skc-k8s-multi-node/k8s/","title":"Build"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/5Build/#build","text":"","title":"Build"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/5Build/#pre-requisites","text":"The below steps need to be done on RHEL 8.2 Build machine (VM/Physical Node)","title":"Pre-requisites"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/5Build/#development-tools-and-utilities","text":"dnf install -y git wget tar python3 gcc gcc-c++ zip tar make yum-utils openssl-devel dnf install -y https://dl.fedoraproject.org/pub/fedora/linux/releases/32/Everything/x86_64/os/Packages/m/makeself-2.4.0-5.fc32.noarch.rpm ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip","title":"Development Tools and Utilities"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/5Build/#repo-tool","text":"tmpdir = $( mktemp -d ) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir /repo /usr/local/bin rm -rf $tmpdir","title":"Repo tool"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/5Build/#golang","text":"wget https://dl.google.com/go/go1.14.4.linux-amd64.tar.gz tar -xzf go1.14.4.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT = /usr/local/go export PATH = $GOROOT /bin: $PATH rm -rf go1.14.4.linux-amd64.tar.gz","title":"Golang"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/5Build/#docker","text":"dnf module enable -y container-tools dnf install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo dnf install -y docker-ce-19.03.13 docker-ce-cli-19.03.13 systemctl enable docker systemctl start docker #Ignore the below steps if not running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" systemctl daemon-reload systemctl restart docker","title":"Docker"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/5Build/#skopeo","text":"dnf install -y skopeo","title":"Skopeo"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/5Build/#build-oci-container-images-and-k8s-manifests","text":"The build process for OCI containers images and K8s manifests for RHEL 8.2 & Ubuntu 18.04 deployments must be done on RHEL 8.2 machine only","title":"Build OCI Container images and K8s Manifests"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/5Build/#single-node","text":"Sync the repos mkdir -p /root/intel-secl/build/skc-k8s-single-node && cd /root/intel-secl/build/skc-k8s-single-node repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/skc.xml repo sync Build make k8s-aio Built Container images,K8s manifests and deployment scripts /root/intel-secl/build/skc-k8s-single-node/k8s/","title":"Single Node"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/5Build/#multi-node","text":"Sync the repos mkdir -p /root/intel-secl/build/skc-k8s-multi-node && cd /root/intel-secl/build/skc-k8s-multi-node repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/skc.xml repo sync Build make k8s Built Container images,K8s manifests and deployment scripts /root/intel-secl/build/skc-k8s-multi-node/k8s/","title":"Multi Node"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/","text":"Deployment Pre-requisites Install openssl on K8s control-plane Note For Ubuntu OS, comment RANDFILE = $ENV::HOME/.rnd line in /etc/ssl/openssl.cnf Ensure a docker registry is running locally or remotely. Note For single node microk8s deployment, a registry can be brought up by using microk8s add-ons. More details present in Microk8s documentation. This is not mandatory, if a remote registry already exists, the same can be used as well for single-node Note For multi-node kubeadm deployment, a docker registry needs to be setup by the user Push all container images to docker registry. Example below # Without TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/<image-name>:<image-tag> --dest-tls-verify = false # With TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/image-name>:<image-tag> Note In case of microk8s deployment, when docker registry is enabled locally, the OCI container images need to be copied to the node where registry is enabled and then the above example command can be run. The same would not be required when registry is remotely installed On each worker node with SGX enabled and registered to K8s control-plane, the following pre-req needs to be done RHEL 8.2 enabled K8s worker node with SGX: Pre requisite scripts will be available under k8s/platform-dependencies/ on build machine Copy the platform-dependencies script to SGX enabled worker nodes on K8s Execute ./agent_untar.sh Execute ./agent_container_prereq.sh for deploying all pre-reqs required for agent Ubuntu 18.04 enabled K8s worker node with SGX: Pre requisite scripts will be available under k8s/platform-dependencies/ on build machine Copy the platform-dependencies script to SGX enabled worker nodes Execute ./agent_untar.sh Execute ./agent_container_prereq.sh for deploying all pre-reqs required for agent Ensure a backend KMIP-2.0 compliant server like pykmip is up and running. Retrieve KMIP server's key and certificates i.e. client_certificate.pem, client_key.pem and root_certificate.pem files from /etc/pykmip (default path) to K8s control-plane node under k8s/manifests/kbs/kmip-secrets/ path. Note Under k8s/manifests/kbs/ , if kmip-secrets folder not available then please create it before copying above key and certs Ensure that system date and time are in sync across all servers (K8s control-plane node, worker node, nfs server, kmip server) Deploy Single-Node Pre-requisites Setup microk8s being the default supported single node K8s distribution, users would need to install microk8s on a Physical server Copy all manifests and OCI container images as required to K8s control-plane Ensure docker registry is running locally or remotely The K8s cluster admin configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: SGX-ENABLED can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the K8s manifests. This can be edited in k8s/manifests/sgx_agent/daemonset.yml , k8s/manifests/skc_library/deployment.yml #Label node kubectl label node <node-name> node.type = SGX-ENABLED In case of microk8s cluster, the --allow-privileged=true flag needs to be added to the kube-apiserver under /var/snap/microk8s/current/args/kube-apiserver and restart kube-apiserver with systemctl restart snap.microk8s.daemon-apiserver to allow running of privileged containers like SGX-AGENT and SKC-LIBRARY Enable external API communication for microk8s to talk to external Intel PCS server kubectl get configmap -n kube-system kubectl edit configmap -n kube-system coredns #edit coredns config map and change the name server as: \"forward . /etc/resolv.conf\" #Delete coredns pod, it will relaunch Manifests Update all the K8s manifests with the image names to be pulled from the registry The key field in tolerations and nodeAffinity in case of isecl-scheduler and isecl-controller needs to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to microk8s.io/cluster based on k8s distributions of kubeadm and microk8s respectively Deploy steps The bootstrap script would facilitate the deployment of all SGX components at a usecase level. Sample one given below. Update .env file #Kubernetes Distribution microk8s or kubeadm K8S_DISTRIBUTION = microk8s K8S_CONTROL_PLANE_IP = <K8s control-plane IP> K8S_CONTROL_PLANE_HOSTNAME = <K8s control-plane Hostname> # cms CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # authservice AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # SGX Caching Service SCS_ADMIN_USERNAME = scsuser@scs SCS_ADMIN_PASSWORD = scspassword SCS_DB_USERNAME = scsdbuser SCS_DB_PASSWORD = scsdbpassword # To be generated by user via Intel PCS server in case of new users, else the exisiting Provisioning key can be used INTEL_PROVISIONING_SERVER_API_KEY = <provisioning server key> SCS_CERT_SAN_LIST = scs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # SGX host verification service SHVS_ADMIN_USERNAME = shvsuser@shvs SHVS_ADMIN_PASSWORD = shvspassword SHVS_DB_USERNAME = shvsdbuser SHVS_DB_PASSWORD = shvsdbpassword SHVS_CERT_SAN_LIST = shvs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP> # For microk8s # K8S_API_SERVER_CERT=/var/snap/microk8s/current/certs/server.crt K8S_API_SERVER_CERT = /var/snap/microk8s/current/certs/server.crt IHUB_PUB_KEY_PATH = /etc/ihub/ihub_public_key.pem # KBS bootstrap credentials KBS_SERVICE_USERNAME = admin@kbs KBS_SERVICE_PASSWORD = kbsAdminPass # For SKC Virtualization use case set ENDPOINT_URL=https://<K8s control-plane IP>>:30448/v1 ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_SERVER_IP = <KMIP Server IP> KMIP_SERVER_PORT = <KMIP Server Port> KMIP_HOSTNAME = <KMIP IP/Hostname> #Provide either FQDN or KMIP SERVER IP for kmip hostname #Default port for kmip server is 5696 # ISecl Scheduler # For microk8s # K8S_CA_KEY=/var/snap/microk8s/current/certs/ca.key # K8S_CA_CERT=/var/snap/microk8s/current/certs/ca.crt K8S_CA_KEY = /var/snap/microk8s/current/certs/ca.key K8S_CA_CERT = /var/snap/microk8s/current/certs/ca.crt #Skc Library #Comment during fresh deploy,Uncomment this and update key id when deploying SKC Library #KBS_PUBLIC_CERTIFICATE=<key id>.crt SCS_SERVICE_USERNAME = scsuser@scs SCS_SERVICE_PASSWORD = scspassword SHVS_SERVICE_USERNAME = shvsuser@shvs SHVS_SERVICE_PASSWORD = shvspassword SQVS_SERVICE_USERNAME = sqvsuser@sqvs SQVS_SERVICE_PASSWORD = sqvspassword CCC_ADMIN_USERNAME = ccc_admin CCC_ADMIN_PASSWORD = password GLOBAL_ADMIN_USERNAME = global_admin_user GLOBAL_ADMIN_PASSWORD = globalAdminPass INSTALL_ADMIN_USERNAME = superadmin INSTALL_ADMIN_PASSWORD = superAdminPass Run scripts on K8s control-plane The bootstrap scripts are sample scripts to allow for a quick start of SKC services and agents. Users are free to modify the script or directly use the K8s manifests as per their deployment model requirements #Pre-reqs.sh ./pre-requisites.sh #skc-bootstrap-db-services #Reference #Usage: ./skc-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, SGX Caching Service and SGX Host verification Service # purge Delete Database Services for Authservice, SGX Caching Service and SGX Host verification Service ./skc-bootstrap-db-services.sh up #skc-bootstrap #Reference #Usage: ./skc-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap SKC K8s environment for specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete SKC K8s environment for specified agent/service/usecase [will not delete data,config,logs] # purge Delete SKC K8s environment with data,config,logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of sagent,skclib # service Can be one of cms,authservice,scs,shvs,ihub,sqvs,kbs,isecl-controller,isecl-scheduler # usecase Can be one of secure-key-caching,sgx-attestation,sgx-orchestration-k8s,sgx-virtualization,csp,enterprise ./skc-bootstrap.sh up <all/usecase of choice> Create and update scheduler-policy.json path mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions Configure kube-scheduler to establish communication with isecl-scheduler. vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service Multi-Node Pre-requisites Setup kubeadm being the default supported multi-node K8s distribution, users would need to install a kubeadm K8s control-plane node setup Copy all manifests and OCI container images as required to K8s control-plane Ensure images are pushed to registry locally or remotely The K8s cluster admin configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: SGX-ENABLED can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the K8s manifests. This can be edited in k8s/manifests/sgx_agent/daemonset.yml , k8s/manifests/skc_library/deployment.yml #Label node kubectl label node <node-name> node.type = SGX-ENABLED NFS storage class is used in kubernetes environment for data persistence and supported in SKC. User needs to setup NFS server and create directory structure along with granting permission for a given user id. From security point of view, its been recommended to create a separate user id and grant the permission for all isecl directories for this user id. Below are some samples for reference Snapshot showing directory structure for which user needs to create on NFS volumes manually or using custom scripts. Snapshot showing ownership and permissions for directories for which user needs to manually grant the ownership. Snapshot for configuring PV and PVC , user need to provide the NFS server IP or hostname and paths for each of the service directories. Sample manifest for creating config-pv for cms service --- apiVersion: v1 kind: PersistentVolume metadata: name: cms-config-pv spec: capacity: storage: 128Mi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: /<NFS-vol-base-path>/isecl/cms/config server: <NFS Server IP/Hostname> Sample manifest for creating config-pvc for cms service --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cms-config-pvc namespace: isecl spec: storageClassName: nfs accessModes: - ReadWriteMany resources: requests: storage: 128Mi Note The user id specified in security context in deployment.yml for a given service and owner of the service related directories in NFS must be same Manifests Update all the K8s manifests with the image names to be pulled from the registry The key field in tolerations and nodeAffinity in case of isecl-scheduler and isecl-controller needs to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to node-role.kubernetes.io/master All NFS PV yaml files needs to be updated with the path: /<NFS-vol-path> and server: <NFS Server IP/Hostname> under each service manifest file for config , logs , db-data Deploy steps Update .env file #Kubernetes Distribution microk8s or kubeadm K8S_DISTRIBUTION = kubeadm K8S_CONTROL_PLANE_IP = <K8s control-plane IP> K8S_CONTROL_PLANE_HOSTNAME = <K8s control-plane Hostname> # cms CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # authservice AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # SGX Caching Service SCS_ADMIN_USERNAME = scsuser@scs SCS_ADMIN_PASSWORD = scspassword SCS_DB_USERNAME = scsdbuser SCS_DB_PASSWORD = scsdbpassword # To be generated by user via Intel PCS server in case of new users, else the exisiting Provisioning key can be used INTEL_PROVISIONING_SERVER_API_KEY = <provisioning server key> SCS_CERT_SAN_LIST = scs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # SGX host verification service SHVS_ADMIN_USERNAME = shvsuser@shvs SHVS_ADMIN_PASSWORD = shvspassword SHVS_DB_USERNAME = shvsdbuser SHVS_DB_PASSWORD = shvsdbpassword SHVS_CERT_SAN_LIST = shvs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # For Kubeadm # K8S_API_SERVER_CERT=/etc/kubernetes/pki/apiserver.crt K8S_API_SERVER_CERT = /etc/kubernetes/pki/apiserver.crt # This is not valid for multinode deployment, should be populated once ihub is deployed successfully IHUB_PUB_KEY_PATH = <skip this during initial deploy of K8s multi-node> # KBS bootstrap credentials KBS_SERVICE_USERNAME = admin@kbs KBS_SERVICE_PASSWORD = kbsAdminPass # For SKC Virtualization use case set ENDPOINT_URL=https://<K8s control-plane IP>:30448/v1 ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_SERVER_IP = <KMIP Server IP> KMIP_SERVER_PORT = <KMIP Server Port> KMIP_HOSTNAME = <KMIP IP/Hostname> #Provide either FQDN or KMIP SERVER IP for kmip hostname #Default port for kmip server is 5696 # ISecl Scheduler # For Kubeadm # K8S_CA_KEY=/etc/kubernetes/pki/ca.key # K8S_CA_CERT=/etc/kubernetes/pki/ca.crt K8S_CA_KEY = /etc/kubernetes/pki/ca.key K8S_CA_CERT = /etc/kubernetes/pki/ca.crt #Skc Library #Comment during fresh deploy,Uncomment this and update key id when deploying SKC Library #KBS_PUBLIC_CERTIFICATE=<key id>.crt SCS_SERVICE_USERNAME = scsuser@scs SCS_SERVICE_PASSWORD = scspassword SHVS_SERVICE_USERNAME = shvsuser@shvs SHVS_SERVICE_PASSWORD = shvspassword SQVS_SERVICE_USERNAME = sqvsuser@sqvs SQVS_SERVICE_PASSWORD = sqvspassword CCC_ADMIN_USERNAME = ccc_admin CCC_ADMIN_PASSWORD = password GLOBAL_ADMIN_USERNAME = global_admin_user GLOBAL_ADMIN_PASSWORD = globalAdminPass INSTALL_ADMIN_USERNAME = superadmin INSTALL_ADMIN_PASSWORD = superAdminPass Run scripts on K8s control-plane The bootstrap scripts are sample scripts to allow for a quick start of SKC services and agents. Users are free to modify the script or directly use the K8s manifests as per their deployment model requirements #If using sample script provided for creating nfs directories #Copy the script to the base path of the NFS location configured chmod +x create-skc-dirs.nfs.sh ./create-skc-dirs.nfs.sh #Pre-reqs.sh ./pre-requisites.sh #skc-bootstrap-db-services #Reference #Usage: ./skc-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, SGX Caching Service and SGX Host verification Service # purge Delete Database Services for Authservice, SGX Caching Service and SGX Host verification Service ./skc-bootstrap-db-services.sh up #skc-bootstrap #Reference #Usage: ./skc-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap SKC K8s environment for specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete SKC K8s environment for specified agent/service/usecase [will not delete data,config,logs] # purge Delete SKC K8s environment with data,config,logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of sagent,skclib # service Can be one of cms,authservice,scs,shvs,ihub,sqvs,kbs,isecl-controller,isecl-scheduler # usecase Can be one of secure-key-caching,sgx-attestation,sgx-orchestration-k8s,sgx-virtualization,csp,enterprise ./skc-bootstrap.sh up <all/usecase of choice> #NOTE: The isecl-scheduler will not be deployed in case of multi-node K8s as there is dependency on the NFS server IHUB public key to be copied from NFS to K8s control-plane to allow the successful installation of isecl-scheduler. Post update of the isecl-k8s-skc.env for IHUB_PUB_KEY_PATH on K8s control-plane, user needs to run the following ./skc-bootstrap.sh up isecl-scheduler Create and update scheduler-policy.json path mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions Configure kube-scheduler to establish communication with isecl-scheduler. Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec : containers : - command : - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers : volumeMounts : - mountPath : /opt/isecl-k8s-extensions/ name : extendedsched readOnly : true volumes : - hostPath : path : /opt/isecl-k8s-extensions/ type : name : extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml Restart kubelet which restart all the k8s services including kube-scheduler systemctl restart kubelet","title":"Deployment"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#deployment","text":"","title":"Deployment"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#pre-requisites","text":"Install openssl on K8s control-plane Note For Ubuntu OS, comment RANDFILE = $ENV::HOME/.rnd line in /etc/ssl/openssl.cnf Ensure a docker registry is running locally or remotely. Note For single node microk8s deployment, a registry can be brought up by using microk8s add-ons. More details present in Microk8s documentation. This is not mandatory, if a remote registry already exists, the same can be used as well for single-node Note For multi-node kubeadm deployment, a docker registry needs to be setup by the user Push all container images to docker registry. Example below # Without TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/<image-name>:<image-tag> --dest-tls-verify = false # With TLS enabled skopeo copy oci-archive:<oci-image-tar-name> docker://<registry-ip/hostname>:<registry-port>/image-name>:<image-tag> Note In case of microk8s deployment, when docker registry is enabled locally, the OCI container images need to be copied to the node where registry is enabled and then the above example command can be run. The same would not be required when registry is remotely installed On each worker node with SGX enabled and registered to K8s control-plane, the following pre-req needs to be done RHEL 8.2 enabled K8s worker node with SGX: Pre requisite scripts will be available under k8s/platform-dependencies/ on build machine Copy the platform-dependencies script to SGX enabled worker nodes on K8s Execute ./agent_untar.sh Execute ./agent_container_prereq.sh for deploying all pre-reqs required for agent Ubuntu 18.04 enabled K8s worker node with SGX: Pre requisite scripts will be available under k8s/platform-dependencies/ on build machine Copy the platform-dependencies script to SGX enabled worker nodes Execute ./agent_untar.sh Execute ./agent_container_prereq.sh for deploying all pre-reqs required for agent Ensure a backend KMIP-2.0 compliant server like pykmip is up and running. Retrieve KMIP server's key and certificates i.e. client_certificate.pem, client_key.pem and root_certificate.pem files from /etc/pykmip (default path) to K8s control-plane node under k8s/manifests/kbs/kmip-secrets/ path. Note Under k8s/manifests/kbs/ , if kmip-secrets folder not available then please create it before copying above key and certs Ensure that system date and time are in sync across all servers (K8s control-plane node, worker node, nfs server, kmip server)","title":"Pre-requisites"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#deploy","text":"","title":"Deploy"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#single-node","text":"","title":"Single-Node"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#pre-requisites_1","text":"","title":"Pre-requisites"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#setup","text":"microk8s being the default supported single node K8s distribution, users would need to install microk8s on a Physical server Copy all manifests and OCI container images as required to K8s control-plane Ensure docker registry is running locally or remotely The K8s cluster admin configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: SGX-ENABLED can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the K8s manifests. This can be edited in k8s/manifests/sgx_agent/daemonset.yml , k8s/manifests/skc_library/deployment.yml #Label node kubectl label node <node-name> node.type = SGX-ENABLED In case of microk8s cluster, the --allow-privileged=true flag needs to be added to the kube-apiserver under /var/snap/microk8s/current/args/kube-apiserver and restart kube-apiserver with systemctl restart snap.microk8s.daemon-apiserver to allow running of privileged containers like SGX-AGENT and SKC-LIBRARY Enable external API communication for microk8s to talk to external Intel PCS server kubectl get configmap -n kube-system kubectl edit configmap -n kube-system coredns #edit coredns config map and change the name server as: \"forward . /etc/resolv.conf\" #Delete coredns pod, it will relaunch","title":"Setup"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#manifests","text":"Update all the K8s manifests with the image names to be pulled from the registry The key field in tolerations and nodeAffinity in case of isecl-scheduler and isecl-controller needs to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to microk8s.io/cluster based on k8s distributions of kubeadm and microk8s respectively","title":"Manifests"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#deploy-steps","text":"The bootstrap script would facilitate the deployment of all SGX components at a usecase level. Sample one given below.","title":"Deploy steps"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#update-env-file","text":"#Kubernetes Distribution microk8s or kubeadm K8S_DISTRIBUTION = microk8s K8S_CONTROL_PLANE_IP = <K8s control-plane IP> K8S_CONTROL_PLANE_HOSTNAME = <K8s control-plane Hostname> # cms CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # authservice AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # SGX Caching Service SCS_ADMIN_USERNAME = scsuser@scs SCS_ADMIN_PASSWORD = scspassword SCS_DB_USERNAME = scsdbuser SCS_DB_PASSWORD = scsdbpassword # To be generated by user via Intel PCS server in case of new users, else the exisiting Provisioning key can be used INTEL_PROVISIONING_SERVER_API_KEY = <provisioning server key> SCS_CERT_SAN_LIST = scs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # SGX host verification service SHVS_ADMIN_USERNAME = shvsuser@shvs SHVS_ADMIN_PASSWORD = shvspassword SHVS_DB_USERNAME = shvsdbuser SHVS_DB_PASSWORD = shvsdbpassword SHVS_CERT_SAN_LIST = shvs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP> # For microk8s # K8S_API_SERVER_CERT=/var/snap/microk8s/current/certs/server.crt K8S_API_SERVER_CERT = /var/snap/microk8s/current/certs/server.crt IHUB_PUB_KEY_PATH = /etc/ihub/ihub_public_key.pem # KBS bootstrap credentials KBS_SERVICE_USERNAME = admin@kbs KBS_SERVICE_PASSWORD = kbsAdminPass # For SKC Virtualization use case set ENDPOINT_URL=https://<K8s control-plane IP>>:30448/v1 ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_SERVER_IP = <KMIP Server IP> KMIP_SERVER_PORT = <KMIP Server Port> KMIP_HOSTNAME = <KMIP IP/Hostname> #Provide either FQDN or KMIP SERVER IP for kmip hostname #Default port for kmip server is 5696 # ISecl Scheduler # For microk8s # K8S_CA_KEY=/var/snap/microk8s/current/certs/ca.key # K8S_CA_CERT=/var/snap/microk8s/current/certs/ca.crt K8S_CA_KEY = /var/snap/microk8s/current/certs/ca.key K8S_CA_CERT = /var/snap/microk8s/current/certs/ca.crt #Skc Library #Comment during fresh deploy,Uncomment this and update key id when deploying SKC Library #KBS_PUBLIC_CERTIFICATE=<key id>.crt SCS_SERVICE_USERNAME = scsuser@scs SCS_SERVICE_PASSWORD = scspassword SHVS_SERVICE_USERNAME = shvsuser@shvs SHVS_SERVICE_PASSWORD = shvspassword SQVS_SERVICE_USERNAME = sqvsuser@sqvs SQVS_SERVICE_PASSWORD = sqvspassword CCC_ADMIN_USERNAME = ccc_admin CCC_ADMIN_PASSWORD = password GLOBAL_ADMIN_USERNAME = global_admin_user GLOBAL_ADMIN_PASSWORD = globalAdminPass INSTALL_ADMIN_USERNAME = superadmin INSTALL_ADMIN_PASSWORD = superAdminPass","title":"Update .env file"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#run-scripts-on-k8s-control-plane","text":"The bootstrap scripts are sample scripts to allow for a quick start of SKC services and agents. Users are free to modify the script or directly use the K8s manifests as per their deployment model requirements #Pre-reqs.sh ./pre-requisites.sh #skc-bootstrap-db-services #Reference #Usage: ./skc-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, SGX Caching Service and SGX Host verification Service # purge Delete Database Services for Authservice, SGX Caching Service and SGX Host verification Service ./skc-bootstrap-db-services.sh up #skc-bootstrap #Reference #Usage: ./skc-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap SKC K8s environment for specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete SKC K8s environment for specified agent/service/usecase [will not delete data,config,logs] # purge Delete SKC K8s environment with data,config,logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of sagent,skclib # service Can be one of cms,authservice,scs,shvs,ihub,sqvs,kbs,isecl-controller,isecl-scheduler # usecase Can be one of secure-key-caching,sgx-attestation,sgx-orchestration-k8s,sgx-virtualization,csp,enterprise ./skc-bootstrap.sh up <all/usecase of choice> Create and update scheduler-policy.json path mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions Configure kube-scheduler to establish communication with isecl-scheduler. vi /var/snap/microk8s/current/args/kube-scheduler #Add the below line --policy-config-file = /opt/isecl-k8s-extensions/scheduler-policy.json #Restart kubelet systemctl restart snap.microk8s.daemon-kubelet.service","title":"Run scripts on K8s control-plane"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#multi-node","text":"","title":"Multi-Node"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#pre-requisites_2","text":"","title":"Pre-requisites"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#setup_1","text":"kubeadm being the default supported multi-node K8s distribution, users would need to install a kubeadm K8s control-plane node setup Copy all manifests and OCI container images as required to K8s control-plane Ensure images are pushed to registry locally or remotely The K8s cluster admin configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: SGX-ENABLED can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. The same label is being used as default in the K8s manifests. This can be edited in k8s/manifests/sgx_agent/daemonset.yml , k8s/manifests/skc_library/deployment.yml #Label node kubectl label node <node-name> node.type = SGX-ENABLED NFS storage class is used in kubernetes environment for data persistence and supported in SKC. User needs to setup NFS server and create directory structure along with granting permission for a given user id. From security point of view, its been recommended to create a separate user id and grant the permission for all isecl directories for this user id. Below are some samples for reference Snapshot showing directory structure for which user needs to create on NFS volumes manually or using custom scripts. Snapshot showing ownership and permissions for directories for which user needs to manually grant the ownership. Snapshot for configuring PV and PVC , user need to provide the NFS server IP or hostname and paths for each of the service directories. Sample manifest for creating config-pv for cms service --- apiVersion: v1 kind: PersistentVolume metadata: name: cms-config-pv spec: capacity: storage: 128Mi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: /<NFS-vol-base-path>/isecl/cms/config server: <NFS Server IP/Hostname> Sample manifest for creating config-pvc for cms service --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cms-config-pvc namespace: isecl spec: storageClassName: nfs accessModes: - ReadWriteMany resources: requests: storage: 128Mi Note The user id specified in security context in deployment.yml for a given service and owner of the service related directories in NFS must be same","title":"Setup"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#manifests_1","text":"Update all the K8s manifests with the image names to be pulled from the registry The key field in tolerations and nodeAffinity in case of isecl-scheduler and isecl-controller needs to be updated in the respective manifests under the manifests/k8s-extensions-controller and manifests/k8s-extensions-scheduler directories to node-role.kubernetes.io/master All NFS PV yaml files needs to be updated with the path: /<NFS-vol-path> and server: <NFS Server IP/Hostname> under each service manifest file for config , logs , db-data","title":"Manifests"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#deploy-steps_1","text":"","title":"Deploy steps"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#update-env-file_1","text":"#Kubernetes Distribution microk8s or kubeadm K8S_DISTRIBUTION = kubeadm K8S_CONTROL_PLANE_IP = <K8s control-plane IP> K8S_CONTROL_PLANE_HOSTNAME = <K8s control-plane Hostname> # cms CMS_SAN_LIST = cms-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # authservice AAS_API_CLUSTER_ENDPOINT_URL = https://<K8s control-plane IP>:30444/aas/v1 AAS_ADMIN_USERNAME = admin@aas AAS_ADMIN_PASSWORD = aasAdminPass AAS_DB_USERNAME = aasdbuser AAS_DB_PASSWORD = aasdbpassword AAS_SAN_LIST = aas-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # SGX Caching Service SCS_ADMIN_USERNAME = scsuser@scs SCS_ADMIN_PASSWORD = scspassword SCS_DB_USERNAME = scsdbuser SCS_DB_PASSWORD = scsdbpassword # To be generated by user via Intel PCS server in case of new users, else the exisiting Provisioning key can be used INTEL_PROVISIONING_SERVER_API_KEY = <provisioning server key> SCS_CERT_SAN_LIST = scs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # SGX host verification service SHVS_ADMIN_USERNAME = shvsuser@shvs SHVS_ADMIN_PASSWORD = shvspassword SHVS_DB_USERNAME = shvsdbuser SHVS_DB_PASSWORD = shvsdbpassword SHVS_CERT_SAN_LIST = shvs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # ihub bootstrap IHUB_SERVICE_USERNAME = admin@hub IHUB_SERVICE_PASSWORD = hubAdminPass IH_CERT_SAN_LIST = ihub-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> # For Kubeadm # K8S_API_SERVER_CERT=/etc/kubernetes/pki/apiserver.crt K8S_API_SERVER_CERT = /etc/kubernetes/pki/apiserver.crt # This is not valid for multinode deployment, should be populated once ihub is deployed successfully IHUB_PUB_KEY_PATH = <skip this during initial deploy of K8s multi-node> # KBS bootstrap credentials KBS_SERVICE_USERNAME = admin@kbs KBS_SERVICE_PASSWORD = kbsAdminPass # For SKC Virtualization use case set ENDPOINT_URL=https://<K8s control-plane IP>:30448/v1 ENDPOINT_URL = https://kbs-svc.isecl.svc.cluster.local:9443/v1 KBS_CERT_SAN_LIST = kbs-svc.isecl.svc.cluster.local,<K8s control-plane IP>,<K8s control-plane Hostname> KMIP_SERVER_IP = <KMIP Server IP> KMIP_SERVER_PORT = <KMIP Server Port> KMIP_HOSTNAME = <KMIP IP/Hostname> #Provide either FQDN or KMIP SERVER IP for kmip hostname #Default port for kmip server is 5696 # ISecl Scheduler # For Kubeadm # K8S_CA_KEY=/etc/kubernetes/pki/ca.key # K8S_CA_CERT=/etc/kubernetes/pki/ca.crt K8S_CA_KEY = /etc/kubernetes/pki/ca.key K8S_CA_CERT = /etc/kubernetes/pki/ca.crt #Skc Library #Comment during fresh deploy,Uncomment this and update key id when deploying SKC Library #KBS_PUBLIC_CERTIFICATE=<key id>.crt SCS_SERVICE_USERNAME = scsuser@scs SCS_SERVICE_PASSWORD = scspassword SHVS_SERVICE_USERNAME = shvsuser@shvs SHVS_SERVICE_PASSWORD = shvspassword SQVS_SERVICE_USERNAME = sqvsuser@sqvs SQVS_SERVICE_PASSWORD = sqvspassword CCC_ADMIN_USERNAME = ccc_admin CCC_ADMIN_PASSWORD = password GLOBAL_ADMIN_USERNAME = global_admin_user GLOBAL_ADMIN_PASSWORD = globalAdminPass INSTALL_ADMIN_USERNAME = superadmin INSTALL_ADMIN_PASSWORD = superAdminPass","title":"Update .env file"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/6Deployment/#run-scripts-on-k8s-control-plane_1","text":"The bootstrap scripts are sample scripts to allow for a quick start of SKC services and agents. Users are free to modify the script or directly use the K8s manifests as per their deployment model requirements #If using sample script provided for creating nfs directories #Copy the script to the base path of the NFS location configured chmod +x create-skc-dirs.nfs.sh ./create-skc-dirs.nfs.sh #Pre-reqs.sh ./pre-requisites.sh #skc-bootstrap-db-services #Reference #Usage: ./skc-bootstrap-db-services.sh [-help/up/purge] # -help print help and exit # up Bootstrap Database Services for Authservice, SGX Caching Service and SGX Host verification Service # purge Delete Database Services for Authservice, SGX Caching Service and SGX Host verification Service ./skc-bootstrap-db-services.sh up #skc-bootstrap #Reference #Usage: ./skc-bootstrap.sh [-help/up/down/purge] # -help Print help and exit # up [all/<agent>/<service>/<usecase>] Bootstrap SKC K8s environment for specified agent/service/usecase # down [all/<agent>/<service>/<usecase>] Delete SKC K8s environment for specified agent/service/usecase [will not delete data,config,logs] # purge Delete SKC K8s environment with data,config,logs [only supported for single node deployments] # Available Options for up/down command: # agent Can be one of sagent,skclib # service Can be one of cms,authservice,scs,shvs,ihub,sqvs,kbs,isecl-controller,isecl-scheduler # usecase Can be one of secure-key-caching,sgx-attestation,sgx-orchestration-k8s,sgx-virtualization,csp,enterprise ./skc-bootstrap.sh up <all/usecase of choice> #NOTE: The isecl-scheduler will not be deployed in case of multi-node K8s as there is dependency on the NFS server IHUB public key to be copied from NFS to K8s control-plane to allow the successful installation of isecl-scheduler. Post update of the isecl-k8s-skc.env for IHUB_PUB_KEY_PATH on K8s control-plane, user needs to run the following ./skc-bootstrap.sh up isecl-scheduler Create and update scheduler-policy.json path mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions Configure kube-scheduler to establish communication with isecl-scheduler. Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec : containers : - command : - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers : volumeMounts : - mountPath : /opt/isecl-k8s-extensions/ name : extendedsched readOnly : true volumes : - hostPath : path : /opt/isecl-k8s-extensions/ type : name : extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml Restart kubelet which restart all the k8s services including kube-scheduler systemctl restart kubelet","title":"Run scripts on K8s control-plane"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/7Default%20Service%20and%20Agent%20Mount%20Paths/","text":"Default Service and Agent Mount Paths Single Node Deployments Single node Deployments use hostPath mounting pod(container) files directly on host. Following is the complete list of the files being mounted on host #Certificate-Management-Service Config : /etc/cms Logs : /var/log/cms #Authentication Authorization Service Config : /etc/authservice Logs : /var/log/authservice Pg-data : /usr/local/kube/data/authservice/pgdata #SGX Host Attestation Service Config : /etc/shvs Logs : /var/log/shvs Pg-data : /usr/local/kube/data/shvs #SGX Caching Service Config : /etc/scs Logs : /var/log/scs Pg-data : /usr/local/kube/data/scs #Integration-Hub Config : /etc/ihub Log : /var/log/ihub #SGX Quote Verificaton Service Config : /etc/sqvs Logs : /var/log/sqvs #Key-Broker-Service Config : /etc/kbs Log : /var/log/kbs Opt : /opt/kbs #SGX Library: Config : /root/lib/resources/sgx_default_qcnl.conf openssl-config : /etc/pki/tls/openssl.cnf pkcs11-config : /opt/skc/etc/pkcs11-apimodule.ini kms-npm-config : /opt/skc/etc/kms_npm.ini sgx-stm-config : /opt/skc/etc/sgx_stm.ini kms-cert : /root/9e9db4b5-5893-40fe-b6c4-d54ec6609c55.crt haproxy-hosts : /etc/hosts nginx-config : /etc/nginx/nginx.conf skc-lib-config : /root/lib/skc_library.conf kms-key : /tmp/keys.txt nginx-logs : /var/log/sqvs/ #SGX Agent: Config : /etc/sgx-agent Logs : /var/log/sgx-agent EFI : /sys/firmware/efi/efivars Multi Node Deployments Multi node Deployments use k8s persistent volume and persistent volume claims for mounting pod(container) files on NFS volumes for all services, agents will continue to use hostPath . Following is a sample list of the files being mounted on NFS base volumes #Certificate-Management-Service Config : <NFS-vol-base-path>/isecl/cms/config Logs : <NFS-vol-base-path>/isecl/cms/logs #Authentication Authorization Service Config : <NFS-vol-base-path>/isecl/aas/config Logs : <NFS-vol-base-path>/isecl/aas/logs Pg-data : <NFS-vol-base-path>/isecl/aas/db #SGX Host Attestation Service Config : <NFS-vol-base-path>/isecl/shvs/config Logs : <NFS-vol-base-path>/isecl/shvs/logs Pg-data : <NFS-vol-base-path>/usr/local/kube/data/shvs #Integration-Hub Config : <NFS-vol-base-path>/isecl/ihub/config Log : <NFS-vol-base-path>/isecl/ihub/logs #SGX Quote Verificaton Service Config : <NFS-vol-base-path>/isecl/sqvs/config Logs : <NFS-vol-base-path>/isecl/sqvs/log #Key-Broker-Service Config : <NFS-vol-base-path>/isecl/kbs/config Log : <NFS-vol-base-path>/isecl/kbs/logs Opt : <NFS-vol-base-path>/isecl/kbs/kbs/opt #SGX Agent: Config : /etc/sgx-agent Logs : /var/log/sgx-agent EFI : /sys/firmware/efi/efivars","title":"Default Service and Agent Mount Paths"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/7Default%20Service%20and%20Agent%20Mount%20Paths/#default-service-and-agent-mount-paths","text":"","title":"Default Service and Agent Mount Paths"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/7Default%20Service%20and%20Agent%20Mount%20Paths/#single-node-deployments","text":"Single node Deployments use hostPath mounting pod(container) files directly on host. Following is the complete list of the files being mounted on host #Certificate-Management-Service Config : /etc/cms Logs : /var/log/cms #Authentication Authorization Service Config : /etc/authservice Logs : /var/log/authservice Pg-data : /usr/local/kube/data/authservice/pgdata #SGX Host Attestation Service Config : /etc/shvs Logs : /var/log/shvs Pg-data : /usr/local/kube/data/shvs #SGX Caching Service Config : /etc/scs Logs : /var/log/scs Pg-data : /usr/local/kube/data/scs #Integration-Hub Config : /etc/ihub Log : /var/log/ihub #SGX Quote Verificaton Service Config : /etc/sqvs Logs : /var/log/sqvs #Key-Broker-Service Config : /etc/kbs Log : /var/log/kbs Opt : /opt/kbs #SGX Library: Config : /root/lib/resources/sgx_default_qcnl.conf openssl-config : /etc/pki/tls/openssl.cnf pkcs11-config : /opt/skc/etc/pkcs11-apimodule.ini kms-npm-config : /opt/skc/etc/kms_npm.ini sgx-stm-config : /opt/skc/etc/sgx_stm.ini kms-cert : /root/9e9db4b5-5893-40fe-b6c4-d54ec6609c55.crt haproxy-hosts : /etc/hosts nginx-config : /etc/nginx/nginx.conf skc-lib-config : /root/lib/skc_library.conf kms-key : /tmp/keys.txt nginx-logs : /var/log/sqvs/ #SGX Agent: Config : /etc/sgx-agent Logs : /var/log/sgx-agent EFI : /sys/firmware/efi/efivars","title":"Single Node Deployments"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/7Default%20Service%20and%20Agent%20Mount%20Paths/#multi-node-deployments","text":"Multi node Deployments use k8s persistent volume and persistent volume claims for mounting pod(container) files on NFS volumes for all services, agents will continue to use hostPath . Following is a sample list of the files being mounted on NFS base volumes #Certificate-Management-Service Config : <NFS-vol-base-path>/isecl/cms/config Logs : <NFS-vol-base-path>/isecl/cms/logs #Authentication Authorization Service Config : <NFS-vol-base-path>/isecl/aas/config Logs : <NFS-vol-base-path>/isecl/aas/logs Pg-data : <NFS-vol-base-path>/isecl/aas/db #SGX Host Attestation Service Config : <NFS-vol-base-path>/isecl/shvs/config Logs : <NFS-vol-base-path>/isecl/shvs/logs Pg-data : <NFS-vol-base-path>/usr/local/kube/data/shvs #Integration-Hub Config : <NFS-vol-base-path>/isecl/ihub/config Log : <NFS-vol-base-path>/isecl/ihub/logs #SGX Quote Verificaton Service Config : <NFS-vol-base-path>/isecl/sqvs/config Logs : <NFS-vol-base-path>/isecl/sqvs/log #Key-Broker-Service Config : <NFS-vol-base-path>/isecl/kbs/config Log : <NFS-vol-base-path>/isecl/kbs/logs Opt : <NFS-vol-base-path>/isecl/kbs/kbs/opt #SGX Agent: Config : /etc/sgx-agent Logs : /var/log/sgx-agent EFI : /sys/firmware/efi/efivars","title":"Multi Node Deployments"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/8Default%20Service%20Ports/","text":"Default Service Ports For both Single node and multi-node deployments following ports are used. All services exposing APIs will use the below ports CMS : 30445 AAS : 30444 SCS : 30501 SHVS : 30500 SQVS : 30502 IHUB : None KBS : 30448 K8s-scheduler : 30888 K8s-controller : None SKC Library : None SGXAgent : 31001","title":"Default Service Ports"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/8Default%20Service%20Ports/#default-service-ports","text":"For both Single node and multi-node deployments following ports are used. All services exposing APIs will use the below ports CMS : 30445 AAS : 30444 SCS : 30501 SHVS : 30500 SQVS : 30502 IHUB : None KBS : 30448 K8s-scheduler : 30888 K8s-controller : None SKC Library : None SGXAgent : 31001","title":"Default Service Ports"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/","text":"Usecase Workflows API Collections The below allow to get started with workflows within Intel\u00ae SecL-DC for Foundational and Workload Security Usecases. More details available in API Collections repository Pre-requisites Postman client should be downloaded on supported platforms or on the web to get started with the usecase collections. Note The Postman API Network will always have the latest released version of the API Collections. For all releases, refer the github repository for API Collections Use Case Collections Use case Sub-Usecase API Collection Secure Key Caching - \u2714\ufe0f SGX Discovery, Provisioning and Orchestration - \u2714\ufe0f Downloading API Collections Postman API Network for latest released: https://explore.postman.com/intelsecldc or Github repo for all releases #Clone the github repo for api-collections git clone https://github.com/intel-secl/utils.git #Switch to specific release-version of choice cd utils/ git checkout <release-version of choice> #Import Collections from cd tools/api-collections Note The postman-collections are also available when cloning the repos via build manifest under utils/tools/api-collections Running API Collections Import the collection into Postman API Client Note This step is required only when not using Postman API Network and downloading from Github Update env as per the deployment details for specific usecase View Documentation Run the workflow","title":"Usecase Workflows API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/#usecase-workflows-api-collections","text":"The below allow to get started with workflows within Intel\u00ae SecL-DC for Foundational and Workload Security Usecases. More details available in API Collections repository","title":"Usecase Workflows API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/#pre-requisites","text":"Postman client should be downloaded on supported platforms or on the web to get started with the usecase collections. Note The Postman API Network will always have the latest released version of the API Collections. For all releases, refer the github repository for API Collections","title":"Pre-requisites"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/#use-case-collections","text":"Use case Sub-Usecase API Collection Secure Key Caching - \u2714\ufe0f SGX Discovery, Provisioning and Orchestration - \u2714\ufe0f","title":"Use Case Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/#downloading-api-collections","text":"Postman API Network for latest released: https://explore.postman.com/intelsecldc or Github repo for all releases #Clone the github repo for api-collections git clone https://github.com/intel-secl/utils.git #Switch to specific release-version of choice cd utils/ git checkout <release-version of choice> #Import Collections from cd tools/api-collections Note The postman-collections are also available when cloning the repos via build manifest under utils/tools/api-collections","title":"Downloading API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Containerization/9Usecase%20Workflows%20API%20Collections/#running-api-collections","text":"Import the collection into Postman API Client Note This step is required only when not using Postman API Network and downloading from Github Update env as per the deployment details for specific usecase View Documentation Run the workflow","title":"Running API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/10System%20User%20Configuration/","text":"System User Configuration Build System Setup ~/.gitconfig to update the git user details. A sample config is provided below GIT Configuration** [user] name = John Doe email = john.doe@abc.com [color] ui = auto [push] default = matching + Make sure system date and time of SGX machine and CSP machine both are in sync. Also, if the system is configured to read the RTC time in the local time zone, then use RTC in UTC by running timedatectl set-local-rtc 0 command on both the machine. Otherwise SGX Agent deployment will fail with certificate expiry error.","title":"System User Configuration"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/10System%20User%20Configuration/#system-user-configuration","text":"Build System Setup ~/.gitconfig to update the git user details. A sample config is provided below GIT Configuration** [user] name = John Doe email = john.doe@abc.com [color] ui = auto [push] default = matching + Make sure system date and time of SGX machine and CSP machine both are in sync. Also, if the system is configured to read the RTC time in the local time zone, then use RTC in UTC by running timedatectl set-local-rtc 0 command on both the machine. Otherwise SGX Agent deployment will fail with certificate expiry error.","title":"System User Configuration"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/11Appendix/","text":"Appendix SGX Attestation flow To Deploy SampleApp: Copy sample_apps.tar, sample_apps.sha2 and sampleapps_untar.sh from binaries directory to a directory in SGX compute node and untar it using ./sample_apps_untar.sh Install Intel\u00ae SGX SDK for Linux*OS into /opt/intel/sgxsdk using ./install_sgxsdk.sh Install SGX dependencies using ./deploy_sgx_dependencies.sh Note Make sure to deploy SQVS with includetoken configuration as false. To Verify the SampleApp flow: Update sample_apps.conf with the following IP address for SQVS services deployed on Enterprise system IP address for SCS services deployed on CSP system ENTERPRISE_CMS_IP should point to the IP of CMS service deployed on Enterprise system Network Port numbers for SCS services deployed on CSP system Network Port numbers for SQVS and CMS services deployed on Enterprise system Set RUN_ATTESTING_APP to yes if user wants to run both apps in same machine Run SampleApp using ./run_sample_apps.sh Check the output of attestedApp and attestingApp under out/attested_app_console_out.log and out/attesting_app_console_out.log files Creating RSA Keys in Key Broker Service Steps to run KMIP Server Note Below mentioned steps are provided as script (install_pykmip.sh and pykmip.service) as part of kbs_script folder which will install KMIP Server as daemon. Refer to \u2018Install KMIP Server as daemon\u2019 section. 1. Install python3 and vim-common # dnf -y install python3-pip vim-common ln -s /usr/bin/python3 /usr/bin/python > /dev/null 2>&1 ln -s /usr/bin/pip3 /usr/bin/pip > /dev/null 2>&1 2. Install pykmip # pip3 install pykmip==0.9.1 3. In the /etc/ directory create pykmip and policies folders mkdir -p /etc/pykmip/policies 4. Configure pykmip server using server.conf Update hostname in the server.conf 5. Copy the following to /etc/pykmip/ from kbs_script folder available under binaries directory create_certificates.py, run_server.py, server.conf 6. Create certificates > cd /etc/pykmip > python3 create_certificates.py <KMIP Host IP/KMIP Host FQDN> 7. Kill running KMIP Server processes and wait for 10 seconds until all the KMIP Server processes are killed. > ps -ef | grep run_server.py | grep -v grep | awk '{print $2}' | xargs kill 8. Run pykmip server using run_server.py script > python3 run_server.py & Install KMIP Server as daemon 1. cd into /root/binaries/kbs_script folder 2. Configure pykmip server using server.conf Update hostname in the server.conf 3. Run the install_pykmip.sh script and KMIP server will be installed as daemon process ./install_pykmip.sh Create RSA key in PyKMIP and generate certificate Note This step is required only when PyKMIP script is used as a backend KMIP server. 1. Update Host IP in /root/binaries/kbs_script rsa_create.py script 2. In the kbs_script folder, Run rsa_create.py script > cd /root/binaries/kbs_script > python3 rsa_create.py This script will generate \u201cPrivate Key ID\u201d and \u201cServer certificate\u201d, which should be provided in the kbs.conf file for \u201cKMIP_KEY_ID\u201d and \u201cSERVER_CERT\u201d. Configuration Update to create Keys in KBS cd into /root/binaries/kbs_script folder **To register keys with KBS KMIP** Update the following variables in kbs.conf: KMIP_KEY_ID (Private key ID registered in KMIP server) SERVER_CERT (Server certificate for created private key) Enterprise system IP address where CMS, AAS and KBS services are deployed Port of CMS, AAS and KBS services deployed on enterprise system AAS admin and Enterprise admin credentials Note If KMIP_KEY_ID is not provided then RSA key register will be done with keystring. Update sgx_enclave_measurement_anyof value in transfer_policy_request.json with enclave measurement value obtained using sgx_sign utility. Refer to \"Extracting SGX Enclave values for Key Transfer Policy\" section. Create RSA Key Execute the command ./run.sh reg Copy the generated cert file to SGX Compute node where skc_library is deployed. Also make a note of the key id generated. Configuration for NGINX testing Note Below mentioned OpenSSL and NGINX configuration updates are provided as patches (nginx.patch and openssl.patch) as part of skc_library deployment script. Patch can be applied with default nginx and openssl file. In case nginx/openssl contains any external changes then refer manual step. Apply Patch Execute the command with nginx version - nginx 1.14.1 (Rhel) and openssl version- Openssl 1.1.1g (Rhel) patch -b /etc/nginx/nginx.conf < nginx.patch patch -b /etc/pki/tls/openssl.cnf < openssl.patch OpenSSL Update openssl configuration file /etc/pki/tls/openssl.cnf with below changes: [openssl_def] engines = engine_section [engine_section] pkcs11 = pkcs11_section [pkcs11_section] engine_id = pkcs11 dynamic_path =/usr/lib64/engines-1.1/pkcs11.so MODULE_PATH =/opt/skc/lib/libpkcs11-api.so init = 0 Nginx Update nginx configuration file /etc/nginx/nginx.conf with below changes: ssl_engine pkcs11; Update the location of certificate with the loaction where it was copied into the skc_library machine. ssl_certificate \"add absolute path of crt file\"; Update the fields(token, object and pin-value) with the values given in keys.txt for the KeyID corresponding to the certificate. ssl_certificate_key \"engine:pkcs11:pkcs11:token=KMS;object=RSAKEY;pin-value=1234\"; SKC Configuration Create keys.txt in /root folder. This provides key preloading functionality in skc_library. Any number of keys can be added in keys.txt. Each PKCS11 URL should contain different Key ID which need to be transferred from KBS along with respective object tag for each key id specified Sample PKCS11 url is as below pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234; Token, object and pin-value given in PKCS11 url entry in keys.txt should match with the one in nginx.conf. The keyID should match the keyID of RSA key created in KBS. File location should match with preload_keys directive in pkcs11-apimodule.ini; Sample /opt/skc/etc/pkcs11-apimodule.ini file [core] preload_keys=/root/keys.txt keyagent_conf=/opt/skc/etc/key-agent.ini mode=SGX debug=true [SGX] module=/opt/intel/cryptoapitoolkit/lib/libp11sgx.so KBS key-transfer flow validation On SGX Compute node, Execute below commands for KBS key-transfer: Note: Before initiating key transfer make sure, PYKMIP server is running. pkill nginx Remove any existing pkcs11 token rm -rf /opt/intel/cryptoapitoolkit/tokens/* Initiate Key transfer from KBS systemctl restart nginx Changing group ownership and permissions of pkcs11 token chown -R root:intel /opt/intel/cryptoapitoolkit/tokens/ chmod -R 770 /opt/intel/cryptoapitoolkit/tokens/ Establish a tls session with the nginx using the key transferred inside the enclave wget https://localhost:2443 --no-check-certificate Note on Key Transfer Policy Key transfer policy is used to enforce a set of policies which need to be compiled with before the secret can be securely provisioned onto a sgx enclave A typical Key Transfer Policy would look as below \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"], \"sgx_enclave_issuer_product_id_anyof\":[0], \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"], \"tls_client_certificate_issuer_cn_anyof\":[\"CMSCA\", \"CMS TLS Client CA\"], \"client_permissions_allof\":[\"nginx\",\"USA\"], \"sgx_enforce_tcb_up_to_date\":false sgx_enclave_issuer_anyof establishes the signing identity provided by an authority who has signed the sgx enclave. in other words the owner of the enclave sgx_enclave_measurement_anyof represents the cryptographic hash of the enclave log (enclave code, data) sgx_enforce_tcb_up_to_date - If set to true, Key Broker service will provision the key only of the platform generating the quote conforms to the latest Trusted Computing Base client_permissions_allof - Special permission embedded into the skc_library client TLS certificate which can enforce additional restrictons on who can get access to the key, In above example: the key is provisioned only to the nginx workload and platform which is tagged with value for ex: USA Note on SKC Library Deployment SKC Library Deployment (Binary as well as container) needs to performed with root privilege For binary deployment of SKC client Library, only one instance of Workload can use SKC Client Library. The config information for SKC client library is bound to the workload. In future, Multiple workloads might be supported For container deployment, since configmaps are used, each container instance of workload gets its own private SKC Client Library config information The SKC Client Library TLS client certificate private key is stored in the configuration directories and can be read only with elevated root privileges keys.txt (set of PKCS11 URIs for the keys to be securely provisioned into an SGX enclave) can only be modified with elevated privileges Extracting SGX Enclave values for Key Transfer Policy Values that are specific to the enclave such as sgx_enclave_issuer_anyof, sgx_enclave_measurement_anyof and sgx_enclave_issuer_product_id_anyof can be retrived using sgx_sign utility that is available as part of Intel SGX SDK. Run sgx_sign utility on the signed enclave (This command should be run on the build system). /opt/intel/sgxsdk/bin/x64/sgx_sign dump -enclave <path to the signed enclave> -dumpfile info.txt For sgx_enclave_issuer_anyof , in info.txt, search for \"mrsigner->value\" . E.g mrsigner->value : mrsigner->value: \"0x83 0xd7 0x19 0xe7 0x7d 0xea 0xca 0x14 0x70 0xf6 0xba 0xf6 0x2a 0x4d 0x77 0x43 0x03 0xc8 0x99 0xdb 0x69 0x02 0x0f 0x9c 0x70 0xee 0x1d 0xfc 0x08 0xc7 0xce 0x9e\" Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"] For sgx_enclave_measurement_anyof , in info.txt, search for metadata->enclave_css.body.enclave_hash.m . E.g metadata->enclave_css.body.enclave_hash.m : metadata->enclave_css.body.enclave_hash.m: 0xad 0x46 0x74 0x9e 0xd4 0x1e 0xba 0xa2 0x32 0x72 0x52 0x04 0x1e 0xe7 0x46 0xd3 0x79 0x1a 0x9f 0x24 0x31 0x83 0x0f 0xee 0x08 0x83 0xf7 0x99 0x3c 0xaf 0x31 0x6a Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"] Please note that the SGX Enclave measurement value will depend on the toolchain used to build and link the SGX enclave. Hence the SGX Enclave measurement value would differ across OS flavours. For more details please refer https://github.com/intel/linux-sgx/tree/master/linux/reproducibility","title":"Appendix"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/11Appendix/#appendix","text":"","title":"Appendix"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/11Appendix/#sgx-attestation-flow","text":"To Deploy SampleApp: Copy sample_apps.tar, sample_apps.sha2 and sampleapps_untar.sh from binaries directory to a directory in SGX compute node and untar it using ./sample_apps_untar.sh Install Intel\u00ae SGX SDK for Linux*OS into /opt/intel/sgxsdk using ./install_sgxsdk.sh Install SGX dependencies using ./deploy_sgx_dependencies.sh Note Make sure to deploy SQVS with includetoken configuration as false. To Verify the SampleApp flow: Update sample_apps.conf with the following IP address for SQVS services deployed on Enterprise system IP address for SCS services deployed on CSP system ENTERPRISE_CMS_IP should point to the IP of CMS service deployed on Enterprise system Network Port numbers for SCS services deployed on CSP system Network Port numbers for SQVS and CMS services deployed on Enterprise system Set RUN_ATTESTING_APP to yes if user wants to run both apps in same machine Run SampleApp using ./run_sample_apps.sh Check the output of attestedApp and attestingApp under out/attested_app_console_out.log and out/attesting_app_console_out.log files","title":"SGX Attestation flow"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/11Appendix/#creating-rsa-keys-in-key-broker-service","text":"Steps to run KMIP Server Note Below mentioned steps are provided as script (install_pykmip.sh and pykmip.service) as part of kbs_script folder which will install KMIP Server as daemon. Refer to \u2018Install KMIP Server as daemon\u2019 section. 1. Install python3 and vim-common # dnf -y install python3-pip vim-common ln -s /usr/bin/python3 /usr/bin/python > /dev/null 2>&1 ln -s /usr/bin/pip3 /usr/bin/pip > /dev/null 2>&1 2. Install pykmip # pip3 install pykmip==0.9.1 3. In the /etc/ directory create pykmip and policies folders mkdir -p /etc/pykmip/policies 4. Configure pykmip server using server.conf Update hostname in the server.conf 5. Copy the following to /etc/pykmip/ from kbs_script folder available under binaries directory create_certificates.py, run_server.py, server.conf 6. Create certificates > cd /etc/pykmip > python3 create_certificates.py <KMIP Host IP/KMIP Host FQDN> 7. Kill running KMIP Server processes and wait for 10 seconds until all the KMIP Server processes are killed. > ps -ef | grep run_server.py | grep -v grep | awk '{print $2}' | xargs kill 8. Run pykmip server using run_server.py script > python3 run_server.py & Install KMIP Server as daemon 1. cd into /root/binaries/kbs_script folder 2. Configure pykmip server using server.conf Update hostname in the server.conf 3. Run the install_pykmip.sh script and KMIP server will be installed as daemon process ./install_pykmip.sh Create RSA key in PyKMIP and generate certificate Note This step is required only when PyKMIP script is used as a backend KMIP server. 1. Update Host IP in /root/binaries/kbs_script rsa_create.py script 2. In the kbs_script folder, Run rsa_create.py script > cd /root/binaries/kbs_script > python3 rsa_create.py This script will generate \u201cPrivate Key ID\u201d and \u201cServer certificate\u201d, which should be provided in the kbs.conf file for \u201cKMIP_KEY_ID\u201d and \u201cSERVER_CERT\u201d. Configuration Update to create Keys in KBS cd into /root/binaries/kbs_script folder **To register keys with KBS KMIP** Update the following variables in kbs.conf: KMIP_KEY_ID (Private key ID registered in KMIP server) SERVER_CERT (Server certificate for created private key) Enterprise system IP address where CMS, AAS and KBS services are deployed Port of CMS, AAS and KBS services deployed on enterprise system AAS admin and Enterprise admin credentials Note If KMIP_KEY_ID is not provided then RSA key register will be done with keystring. Update sgx_enclave_measurement_anyof value in transfer_policy_request.json with enclave measurement value obtained using sgx_sign utility. Refer to \"Extracting SGX Enclave values for Key Transfer Policy\" section. Create RSA Key Execute the command ./run.sh reg Copy the generated cert file to SGX Compute node where skc_library is deployed. Also make a note of the key id generated.","title":"Creating RSA Keys in Key Broker Service"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/11Appendix/#configuration-for-nginx-testing","text":"Note Below mentioned OpenSSL and NGINX configuration updates are provided as patches (nginx.patch and openssl.patch) as part of skc_library deployment script. Patch can be applied with default nginx and openssl file. In case nginx/openssl contains any external changes then refer manual step. Apply Patch Execute the command with nginx version - nginx 1.14.1 (Rhel) and openssl version- Openssl 1.1.1g (Rhel) patch -b /etc/nginx/nginx.conf < nginx.patch patch -b /etc/pki/tls/openssl.cnf < openssl.patch OpenSSL Update openssl configuration file /etc/pki/tls/openssl.cnf with below changes: [openssl_def] engines = engine_section [engine_section] pkcs11 = pkcs11_section [pkcs11_section] engine_id = pkcs11 dynamic_path =/usr/lib64/engines-1.1/pkcs11.so MODULE_PATH =/opt/skc/lib/libpkcs11-api.so init = 0 Nginx Update nginx configuration file /etc/nginx/nginx.conf with below changes: ssl_engine pkcs11; Update the location of certificate with the loaction where it was copied into the skc_library machine. ssl_certificate \"add absolute path of crt file\"; Update the fields(token, object and pin-value) with the values given in keys.txt for the KeyID corresponding to the certificate. ssl_certificate_key \"engine:pkcs11:pkcs11:token=KMS;object=RSAKEY;pin-value=1234\"; SKC Configuration Create keys.txt in /root folder. This provides key preloading functionality in skc_library. Any number of keys can be added in keys.txt. Each PKCS11 URL should contain different Key ID which need to be transferred from KBS along with respective object tag for each key id specified Sample PKCS11 url is as below pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234; Token, object and pin-value given in PKCS11 url entry in keys.txt should match with the one in nginx.conf. The keyID should match the keyID of RSA key created in KBS. File location should match with preload_keys directive in pkcs11-apimodule.ini; Sample /opt/skc/etc/pkcs11-apimodule.ini file [core] preload_keys=/root/keys.txt keyagent_conf=/opt/skc/etc/key-agent.ini mode=SGX debug=true [SGX] module=/opt/intel/cryptoapitoolkit/lib/libp11sgx.so","title":"Configuration for NGINX testing"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/11Appendix/#kbs-key-transfer-flow-validation","text":"On SGX Compute node, Execute below commands for KBS key-transfer: Note: Before initiating key transfer make sure, PYKMIP server is running. pkill nginx Remove any existing pkcs11 token rm -rf /opt/intel/cryptoapitoolkit/tokens/* Initiate Key transfer from KBS systemctl restart nginx Changing group ownership and permissions of pkcs11 token chown -R root:intel /opt/intel/cryptoapitoolkit/tokens/ chmod -R 770 /opt/intel/cryptoapitoolkit/tokens/ Establish a tls session with the nginx using the key transferred inside the enclave wget https://localhost:2443 --no-check-certificate","title":"KBS key-transfer flow validation"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/11Appendix/#note-on-key-transfer-policy","text":"Key transfer policy is used to enforce a set of policies which need to be compiled with before the secret can be securely provisioned onto a sgx enclave A typical Key Transfer Policy would look as below \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"], \"sgx_enclave_issuer_product_id_anyof\":[0], \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"], \"tls_client_certificate_issuer_cn_anyof\":[\"CMSCA\", \"CMS TLS Client CA\"], \"client_permissions_allof\":[\"nginx\",\"USA\"], \"sgx_enforce_tcb_up_to_date\":false sgx_enclave_issuer_anyof establishes the signing identity provided by an authority who has signed the sgx enclave. in other words the owner of the enclave sgx_enclave_measurement_anyof represents the cryptographic hash of the enclave log (enclave code, data) sgx_enforce_tcb_up_to_date - If set to true, Key Broker service will provision the key only of the platform generating the quote conforms to the latest Trusted Computing Base client_permissions_allof - Special permission embedded into the skc_library client TLS certificate which can enforce additional restrictons on who can get access to the key, In above example: the key is provisioned only to the nginx workload and platform which is tagged with value for ex: USA","title":"Note on Key Transfer Policy"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/11Appendix/#note-on-skc-library-deployment","text":"SKC Library Deployment (Binary as well as container) needs to performed with root privilege For binary deployment of SKC client Library, only one instance of Workload can use SKC Client Library. The config information for SKC client library is bound to the workload. In future, Multiple workloads might be supported For container deployment, since configmaps are used, each container instance of workload gets its own private SKC Client Library config information The SKC Client Library TLS client certificate private key is stored in the configuration directories and can be read only with elevated root privileges keys.txt (set of PKCS11 URIs for the keys to be securely provisioned into an SGX enclave) can only be modified with elevated privileges","title":"Note on SKC Library Deployment"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/11Appendix/#extracting-sgx-enclave-values-for-key-transfer-policy","text":"Values that are specific to the enclave such as sgx_enclave_issuer_anyof, sgx_enclave_measurement_anyof and sgx_enclave_issuer_product_id_anyof can be retrived using sgx_sign utility that is available as part of Intel SGX SDK. Run sgx_sign utility on the signed enclave (This command should be run on the build system). /opt/intel/sgxsdk/bin/x64/sgx_sign dump -enclave <path to the signed enclave> -dumpfile info.txt For sgx_enclave_issuer_anyof , in info.txt, search for \"mrsigner->value\" . E.g mrsigner->value : mrsigner->value: \"0x83 0xd7 0x19 0xe7 0x7d 0xea 0xca 0x14 0x70 0xf6 0xba 0xf6 0x2a 0x4d 0x77 0x43 0x03 0xc8 0x99 0xdb 0x69 0x02 0x0f 0x9c 0x70 0xee 0x1d 0xfc 0x08 0xc7 0xce 0x9e\" Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"] For sgx_enclave_measurement_anyof , in info.txt, search for metadata->enclave_css.body.enclave_hash.m . E.g metadata->enclave_css.body.enclave_hash.m : metadata->enclave_css.body.enclave_hash.m: 0xad 0x46 0x74 0x9e 0xd4 0x1e 0xba 0xa2 0x32 0x72 0x52 0x04 0x1e 0xe7 0x46 0xd3 0x79 0x1a 0x9f 0x24 0x31 0x83 0x0f 0xee 0x08 0x83 0xf7 0x99 0x3c 0xaf 0x31 0x6a Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"] Please note that the SGX Enclave measurement value will depend on the toolchain used to build and link the SGX enclave. Hence the SGX Enclave measurement value would differ across OS flavours. For more details please refer https://github.com/intel/linux-sgx/tree/master/linux/reproducibility","title":"Extracting SGX Enclave values for Key Transfer Policy"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/1Hardware%20%26%20OS%20Requirements/","text":"Hardware & OS Requirements Four Hosts or VMs Build System CSP managed Services Enterprise Managed Services Orchestrator Node Setup SGX Enabled Host OS Requirements RHEL 8.2. SKC Solution is built, installed and tested with root privileges. Please ensure that all the following instructions are executed with root privileges","title":"Hardware & OS Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/1Hardware%20%26%20OS%20Requirements/#hardware-os-requirements","text":"","title":"Hardware &amp; OS Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/1Hardware%20%26%20OS%20Requirements/#four-hosts-or-vms","text":"Build System CSP managed Services Enterprise Managed Services Orchestrator Node Setup","title":"Four Hosts or VMs"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/1Hardware%20%26%20OS%20Requirements/#sgx-enabled-host","text":"","title":"SGX Enabled Host"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/1Hardware%20%26%20OS%20Requirements/#os-requirements","text":"RHEL 8.2. SKC Solution is built, installed and tested with root privileges. Please ensure that all the following instructions are executed with root privileges","title":"OS Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/2Network%20Requirements/","text":"Network Requirements Internet access is required for the following Build System CSP Managed Services Enterprise Managed Services SGX Enabled Host Setting Proxy and No Proxy export http_proxy=http://<proxy-url>:<proxy-port> export https_proxy=http://<proxy-url>:<proxy-port> export no_proxy=0.0.0.0,127.0.0.1,localhost,<CSP IP>,<Enterprise IP>, <SGX Compute Node IP>, <KBS system Hostname> Firewall Settings Ensure that all the SKC service ports are accessible with firewall","title":"Network Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/2Network%20Requirements/#network-requirements","text":"Internet access is required for the following Build System CSP Managed Services Enterprise Managed Services SGX Enabled Host Setting Proxy and No Proxy export http_proxy=http://<proxy-url>:<proxy-port> export https_proxy=http://<proxy-url>:<proxy-port> export no_proxy=0.0.0.0,127.0.0.1,localhost,<CSP IP>,<Enterprise IP>, <SGX Compute Node IP>, <KBS system Hostname> Firewall Settings Ensure that all the SKC service ports are accessible with firewall","title":"Network Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/3RHEL%20Package%20Requirements/","text":"RHEL Package Requirements Access required for the following packages in all systems BaseOS Appstream CodeReady","title":"RHEL Package Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/3RHEL%20Package%20Requirements/#rhel-package-requirements","text":"Access required for the following packages in all systems BaseOS Appstream CodeReady","title":"RHEL Package Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/4Deployment%20Model/","text":"Deployment Model Build + Deployment Machine CSP - ISecL Services Machine Physical Server as per supported configurations Enterprise - ISecL Services Machine","title":"Deployment Model"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/4Deployment%20Model/#deployment-model","text":"Build + Deployment Machine CSP - ISecL Services Machine Physical Server as per supported configurations Enterprise - ISecL Services Machine","title":"Deployment Model"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/5System%20Tools%20and%20Utilities/","text":"System Tools and Utilities System Tools and utils dnf install git wget tar python3 gcc gcc-c++ zip tar make yum-utils openssl-devel skopeo dnf install https://dl.fedoraproject.org/pub/fedora/linux/releases/32/Everything/x86_64/os/Packages/m/makeself-2.4.0-5.fc32.noarch.rpm ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip export PATH=/usr/local/bin:$PATH Repo Tool tmpdir=$(mktemp -d) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir/repo /usr/local/bin rm -rf $tmpdir Golang Installation wget https://dl.google.com/go/go1.14.1.linux-amd64.tar.gz tar -xzf go1.14.1.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT=/usr/local/go export PATH=$GOROOT/bin:$PATH rm -rf go1.14.1.linux-amd64.tar.gz","title":"System Tools and Utilities"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/5System%20Tools%20and%20Utilities/#system-tools-and-utilities","text":"System Tools and utils dnf install git wget tar python3 gcc gcc-c++ zip tar make yum-utils openssl-devel skopeo dnf install https://dl.fedoraproject.org/pub/fedora/linux/releases/32/Everything/x86_64/os/Packages/m/makeself-2.4.0-5.fc32.noarch.rpm ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip export PATH=/usr/local/bin:$PATH Repo Tool tmpdir=$(mktemp -d) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir/repo /usr/local/bin rm -rf $tmpdir Golang Installation wget https://dl.google.com/go/go1.14.1.linux-amd64.tar.gz tar -xzf go1.14.1.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT=/usr/local/go export PATH=$GOROOT/bin:$PATH rm -rf go1.14.1.linux-amd64.tar.gz","title":"System Tools and Utilities"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/6Build%20Services%2C%20Libraries%20and%20Install%20packages/","text":"Build Services, Libraries and Install packages Note Currently, the repos contain the source code of both the SGX Attestation Infrastructure and SKC. Make will build and package all the binaries and installation scripts but the SGX Attestation Infrastructure can be installed and deployed separately. SKC cannot be installed without the SGX Attestation Infrastructure. The rest of this document will indicate steps that are only needed for SKC. Pulling Source Code mkdir -p /root/workspace && cd /root/workspace repo init -u https://github.com/intel-secl/build-manifest -b refs/tags/v4.0.0 -m manifest/skc.xml repo sync Install, Enable and start the Docker daemon dnf config-manager --add-repo = https://download.docker.com/linux/centos/docker-ce.repo dnf install -y docker-ce-19.03.13 docker-ce-cli-19.03.13 systemctl enable docker systemctl start docker Ignore the below steps if not running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf and set proxy server details if proxy is used [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" #Reload docker systemctl daemon-reload systemctl restart docker Building All SKC Components make Copy Binaries to a clean folder For CSP/Enterprise Deployment Model, copy the generated binaries directory to the /root directory on the CSP/Enterprise system For Single system model, copy the generated binaries directory to the /root directory on the deployment system","title":"Build Services, Libraries and Install packages"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/6Build%20Services%2C%20Libraries%20and%20Install%20packages/#build-services-libraries-and-install-packages","text":"Note Currently, the repos contain the source code of both the SGX Attestation Infrastructure and SKC. Make will build and package all the binaries and installation scripts but the SGX Attestation Infrastructure can be installed and deployed separately. SKC cannot be installed without the SGX Attestation Infrastructure. The rest of this document will indicate steps that are only needed for SKC. Pulling Source Code mkdir -p /root/workspace && cd /root/workspace repo init -u https://github.com/intel-secl/build-manifest -b refs/tags/v4.0.0 -m manifest/skc.xml repo sync Install, Enable and start the Docker daemon dnf config-manager --add-repo = https://download.docker.com/linux/centos/docker-ce.repo dnf install -y docker-ce-19.03.13 docker-ce-cli-19.03.13 systemctl enable docker systemctl start docker Ignore the below steps if not running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf and set proxy server details if proxy is used [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" #Reload docker systemctl daemon-reload systemctl restart docker Building All SKC Components make Copy Binaries to a clean folder For CSP/Enterprise Deployment Model, copy the generated binaries directory to the /root directory on the CSP/Enterprise system For Single system model, copy the generated binaries directory to the /root directory on the deployment system","title":"Build Services, Libraries and Install packages"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/7Deployment%20%26%20Usecase%20Workflow%20Tools%20Installation/","text":"Deployment & Usecase Workflow Tools Installation The below installation is required on the Build & Deployment system only and the Platform(Windows,Linux or MacOS) for Usecase Workflow Tool Installation Deployment Tools Installation Install Ansible on Build Machine pip3 install ansible == 2 .9.10 Install epel-release repository and install sshpass for ansible to connect to remote hosts using SSH dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm dnf install sshpass Create directory for ansible default configuration and hosts file mkdir -p /etc/ansible/ touch /etc/ansible/ansible.cfg Copy the ansible.cfg contents from https://raw.githubusercontent.com/ansible/ansible/v2.9.10/examples/ansible.cfg and paste it under /etc/ansible/ansible.cfg Usecases Workflow Tools Installation Postman client should be downloaded on supported platforms or on the web to get started with the usecase collections. Note The Postman API Network will always have the latest released version of the API Collections. For all releases, refer the github repository for API Collections","title":"Deployment & Usecase Workflow Tools Installation"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/7Deployment%20%26%20Usecase%20Workflow%20Tools%20Installation/#deployment-usecase-workflow-tools-installation","text":"The below installation is required on the Build & Deployment system only and the Platform(Windows,Linux or MacOS) for Usecase Workflow Tool Installation Deployment Tools Installation Install Ansible on Build Machine pip3 install ansible == 2 .9.10 Install epel-release repository and install sshpass for ansible to connect to remote hosts using SSH dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm dnf install sshpass Create directory for ansible default configuration and hosts file mkdir -p /etc/ansible/ touch /etc/ansible/ansible.cfg Copy the ansible.cfg contents from https://raw.githubusercontent.com/ansible/ansible/v2.9.10/examples/ansible.cfg and paste it under /etc/ansible/ansible.cfg","title":"Deployment &amp; Usecase Workflow Tools Installation"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/7Deployment%20%26%20Usecase%20Workflow%20Tools%20Installation/#usecases-workflow-tools-installation","text":"Postman client should be downloaded on supported platforms or on the web to get started with the usecase collections. Note The Postman API Network will always have the latest released version of the API Collections. For all releases, refer the github repository for API Collections","title":"Usecases Workflow Tools Installation"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/","text":"Deployment Deployment Using Ansible The below details would enable the deployment through Ansible Role for Intel\u00ae SecL-DC Secure Key Caching Usecase. However the services can still be installed manually using the Product Guide. More details on Ansible Role for Intel\u00ae SecL-DC in Ansible-Role repository. Download the Ansible Role The role can be cloned locally from git and the contents can be copied to the roles folder used by your ansible server #Create directory for using ansible deployment mkdir -p /root/intel-secl/deploy/ #Clone the repository cd /root/intel-secl/deploy/ && git clone https://github.com/intel-secl/utils.git #Checkout to specific release version cd utils/ git checkout <release-version of choice> cd tools/ansible-role #Update ansible.cfg roles_path to point to path(/root/intel-secl/deploy/utils/tools/ansible-role/roles/) Update Ansible Inventory The following inventory can be used and created under /etc/ansible/hosts [CSP] <machine1_ip/hostname> [Enterprise] <machine2_ip/hostname> [Node] <machine3_ip/hostname> [CSP:vars] isecl_role=csp ansible_user=root ansible_password=<password> [Enterprise:vars] isecl_role=enterprise ansible_user=root ansible_password=<password> [Node:vars] isecl_role=node ansible_user=root ansible_password=<password> Note Ansible requires ssh and root user access to remote machines. The following command can be used to ensure ansible can connect to remote machines with host key check ` ssh-keyscan -H <ip_address> >> /root/.ssh/known_hosts Create and Run Playbook The following are playbook and CLI example for deploying Intel\u00ae SecL-DC binaries based on the supported deployment models and usecases. The below example playbooks can be created as site-bin-isecl.yml Note If running behind a proxy, update the proxy variables under <path to ansible role>/ansible-role/vars/main.yml and run as below Note Go through the Additional Examples and Tips section for specific workflow samples Option 1 Update the PCS Server key with following vars in <path to ansible role>/ansible-role/defaults/main.yml intel_provisioning_server_api_key_sandbox : <pcs server key> Create playbook with following contents - hosts : all gather_facts : yes any_errors_fatal : true vars : setup : <setup var from supported usecases> binaries_path : <path where built binaries are copied to> backend_pykmip : \"<yes/no to install pykmip server along with KMIP KBS>\" roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> OR Option 2: Create playbook with following contents - hosts : all gather_facts : yes any_errors_fatal : true roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> --extra-vars setup = <setup var from supported usecases> --extra-vars binaries_path = <path where built binaries are copied to> --extra-vars intel_provisioning_server_api_key = <pcs server key> --extra-vars backend_pykmip = yes Note If any service installation fails due to any misconfiguration, just uninstall the specific service manually , fix the misconfiguration in ansible and rerun the playbook. The successfully installed services won't be reinstalled. Usecase Setup Options Usecase Variable Secure Key Caching setup: secure-key-caching in playbook or via --extra-vars as setup=secure-key-caching in CLI SGX Orchestration Kubernetes setup: sgx-orchestration-kubernetes in playbook or via --extra-vars as setup=sgx-orchestration-kubernetes in CLI SGX Attestation Kubernetes setup: sgx-attestation-kubernetes in playbook or via --extra-vars as setup=sgx-attestation-kubernetes in CLI SGX Orchestration Openstack setup: sgx-orchestration-openstack in playbook or via --extra-vars as setup=sgx-orchestration-openstack in CLI SGX Attestation Openstack setup: sgx-attestation-openstack in playbook or via --extra-vars as setup=sgx-attestation-openstack in CLI SKC No Orchestration setup: skc-no-orchestration in playbook or via --extra-vars as setup=skc-no-orchestration in CLI SGX Attestation No Orchestration setup: sgx-attestation-no-orchestration in playbook or via --extra-vars as setup=sgx-attestation-no-orchestration in CLI Note Orchestrator installation is not bundled with the role and need to be done independently. Also, components dependent on the orchestrator like isecl-k8s-extensions and integration-hub are installed either partially or not installed Deployment Using Binaries Setup K8S Cluster and Deploy Isecl-k8s-extensions Setup master and worker node for k8s. Worker node should be setup on SGX enabled host machine. Master node can be any system. To setup k8 cluster follow https://phoenixnap.com/kb/how-to-install-kubernetes-on-centos Once the master/worker setup is done, follow below steps on Master Node: Untar packages and push OCI images to registry Copy tar output isecl-k8s-extensions-*.tar.gz from build system's binaries folder to /opt/ directory on the Master Node and extract the contents. cd /opt/ tar -xvzf isecl-k8s-extensions-*.tar.gz cd isecl-k8s-extensions/ Configure private registry Push images to private registry using skopeo command, (this can be done from build vm also) skopeo copy oci-archive:isecl-k8s-controller-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-controller:v4.0.0 skopeo copy oci-archive:isecl-k8s-scheduler-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-scheduler:v4.0.0 Add the image names in isecl-controller.yml and isecl-scheduler.yml in /opt/isecl-k8s-extensions/yamls with full image name including registry IP/hostname (e.g : /isecl-k8s-scheduler:v4.0.0). It will automatically pull the images from registry. Deploy isecl-controller Create hostattributes.crd.isecl.intel.com crd kubectl apply -f yamls/crd-1.17.yaml Check whether the crd is created kubectl get crds Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterrolebinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole=system:node --user=system:serviceaccount:isecl:isecl Fetch token required for ihub installation and follow below IHUB installation steps, kubectl get secrets -n isecl kubectl describe secret default-token-<name> -n isecl For IHUB installation, make sure to update below configuration in /root/binaries/env/ihub.env before installing ihub on CSP system: * Copy /etc/kubernetes/pki/apiserver.crt from master node to /root on CSP system. Update KUBERNETES_CERT_FILE. * Get k8s token in master, using above commands and update KUBERNETES_TOKEN * Update the value of CRD name KUBERNETES_CRD=custom-isecl-sgx Deploy isecl-scheduler The isecl-scheduler default configuration is provided for common cluster support in /opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml. Variables HVS_IHUB_PUBLIC_KEY_PATH and SGX_IHUB_PUBLIC_KEY_PATH are by default set to default paths. Please use and set only required variables based on the use case. For example, if only sgx based attestation is required then remove/comment HVS_IHUB_PUBLIC_KEY_PATH variables. Install cfssl and cfssljson on Kubernetes Control Plane #Download cfssl to /usr/local/bin/ wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl #Download cfssljson to /usr/local/bin wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create tls key pair for isecl-scheduler service, which is signed by k8s apiserver.crt cd /opt/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \"<K8_MASTER_IP>\",\"<K8_MASTER_HOST>\" -c /etc/kubernetes/pki/ca.crt -k /etc/kubernetes/pki/ca.key After iHub deployment, copy /etc/ihub/ihub_public_key.pem from ihub to /opt/isecl-k8s-extensions/ directory on k8 master system. Also, copy tls key pair generated in previous step to secrets directory. mkdir secrets cp /opt/isecl-k8s-extensions/server.key secrets/ cp /opt/isecl-k8s-extensions/server.crt secrets/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem cp /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem secrets/ Note: Prefix the attestation type for ihub_public_key.pem before copying to secrets folder. Create kubernetes secrets scheduler-secret for isecl-scheduler kubectl create secret generic scheduler-certs --namespace isecl --from-file=secrets Deploy isecl-scheduler kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl Configure kube-scheduler to establish communication with isecl-scheduler Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec: containers: - command: - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched Note: Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml. * Restart Kubelet which restart all the k8s services including kube base schedular systemctl restart kubelet Check if CRD data is populated kubectl get -o json hostattributes.crd.isecl.intel.com Deploying SKC Services on Single System Copy the binaries directory generated in the build system to the /root/ directory on the deployment system Update orchestrator.conf with the following - Deployment system IP address - SAN List (a list of ip address and hostname for the deployment system) - Network Port numbers for CMS, AAS, SCS and SHVS - Install Admin and CSP Admin credentials - TENANT as KUBERNETES or OPENSTACK (based on the orchestrator chosen) - System IP address where Kubernetes or Openstack is deployed - Netowrk Port Number of Kubernetes or Openstack Keystone/Placement Service - Database name, Database username and password for SHVS Update enterprise_skc.conf with the following - Deployment system IP address - SAN List (a list of ip address and hostname for the deployment system) - Network Port numbers for CMS, AAS, SCS, SQVS and KBS - Install Admin and CSP Admin credentials - Database name, Database username and password for AAS and SCS services - Intel PCS Server API URL and API Keys - Key Manager can be set to either Directory or KMIP - KMIP server configuration if KMIP is set Save and Close ./install_skc.sh In case ihub installation fails, its recommended to run the following command to clear failed service instance systemctl reset-failed Deploy CSP SKC Services Copy the binaries directory generated in the build system system to the /root/ directory on the CSP system Update csp_skc.conf with the following - CSP system IP Address - SAN List (a list of ip address and hostname for the CSP system) - Network Port numbers for CMS, AAS, SCS and SHVS - Install Admin and CSP Admin credentials - TENANT as KUBERNETES or OPENSTACK (based on the orchestrator chosen) - System IP address where Kubernetes or Openstack is deployed - Netowrk Port Number of Kubernetes or Openstack Keystone/Placement Service - Database name, Database username and password for AAS, SCS and SHVS services - Intel PCS Server API URL and API Keys Save and Close ./install_csp_skc.sh In case installation fails, its recommended to run the following command to clear failed service instance systemctl reset-failed Create sample yml file for nginx workload and add SGX labels to it such as: apiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: SGX-Enabled operator: In values: - \"true\" - key: EPC-Memory operator: In values: - \"2.0GB\" containers: - name: nginx image: nginx ports: - containerPort: 80 Validate if pod can be launched on the node. Run following commands: kubectl apply -f pod.yml kubectl get pods kubectl describe pods nginx Pod should be in running state and launched on the host as per values in pod.yml. Validate by running below command on sgx host: docker ps Openstack Setup and Associate Traits Setup Compute and Controller node for Openstack. Compute node should be setup on SGX host machine, Controller node can be any system. After the compute/controller setup is done, follow the below steps: IHUB should be installed and configured with Openstack Note While using deployment scripts to install the components, in the env directory of the binaries folder comment \"KUBERNETES_TOKEN\" in the ihub.env before installation. Openstack compute node and build VM should have the same OS package repositories, else there will be package mismatch for SKC library. On the openstack controller, if resource provider is not listing the resources then install the \"osc-placement\" pip3 install osc-placement source the admin-openrc credentials to gain access to user-only CLI commands and export the os_placement_API_version source admin-openrc List the set of resources mapped to the Openstack openstack resource provider list Set the required traits for SGX Hosts #For example 'cirros' image can be used for the instances openstack image set --property trait:CUSTOM_ISECL_SGX_ENABLED_TRUE=required <image name> Veiw the Traits that has been set: #The trait should be set and assinged to the respective image successfully. For example 'cirros' image can be used for the instances openstack image show <image name> Verify the trait is enabled for the SGX Host: openstack resource provider trait list <uuid of the host which the openstack resoruce provider lists> #SGX Supported, SGX TCB upto Date, SGX FLC enabled, SGX EPC size attritubes of the SGX host for which the 'required' trait set to TRUE or FALSE is displayed. For example,if required trait is set as TRUE: CUSTOM_ISECL_SGX_ENABLED_TRUE CUSTOM_ISECL_SGX_SUPPORTED_TRUE CUSTOM_ISECL_SGX_TCBUPTODATE_FALSE CUSTOM_ISECL_SGX_FLC_ENABLED_TRUE CUSTOM_ISECL_SGX_EPC_SIZE_2_0_GB For example, if the required trait is set as FALSE CUSTOM_ISECL_SGX_ENABLED_FALSE CUSTOM_ISECL_SGX_SUPPORTED_TRUE CUSTOM_ISECL_SGX_TCBUPTODATE_FALSE CUSTOM_ISECL_SGX_FLC_ENABLED_FALSE CUSTOM_ISECL_SGX_EPC_SIZE_0_B Create the instances openstack server create --flavor tiny --image <image name> --net vmnet <vm instance name> Instances should be created and the status should be \"Active\". Instance should be launched successfully. openstack server list Note To unset the trait, use the following CLI commands: openstack image unset --property trait:CUSTOM_ISECL_SGX_ENABLED_TRUE <image name> openstack image unset --property trait:CUSTOM_ISECL_SGX_ENABLED_FALSE <image name> Deploy Enterprise SKC Services Copy the binaries directory generated in the build system to the /root/ directory on Enterprise system Update enterprise_skc.conf with the following - Enterprise system IP address - SAN List (a list of ip address and hostname for the Enterprise system) - Network Port numbers for CMS, AAS, SCS, SQVS and KBS - Install Admin credentials - Database name, Database username and passwords for AAS and SCS services - Intel PCS Server API URL and API Keys - KMIP server configuration if KMIP is set Save and Close ./install_enterprise_skc.sh Deploy SGX Agent Copy sgx_agent.tar, sgx_agent.sha2 and agent_untar.sh from binaries directoy to a directory in SGX compute node ./agent_untar.sh Edit agent.conf with the following CSP system IP address where CMS, AAS, SHVS and SCS services deployed CSP Admin credentials (same which are provided in service configuration file. for ex: csp_skc.conf, orchestrator.conf or skc.conf) Network Port numbers for CMS, AAS, SCS and SHVS Token validity period in days CMS TLS SHA Value (Run \"cms tlscertsha384\" on CSP system) Save and Close Note In case orchestration support is not needed, please comment/delete SHVS_IP in agent.conf available in same folder ./deploy_sgx_agent.sh Deploy SKC Library Copy skc_library.tar, skc_library.sha2 and skclib_untar.sh from binaries directoy to a directory in SGX compute node ./skclib_untar.sh Update create_roles.conf with the following - IP address of AAS deployed on Enterprise system - Admin account credentials of AAS deployed on Enterprise system. These credentials should match with the AAS admin credentials provided in authservice.env on enterprise side. - Permission string to be embedded into skc_libraty client TLS Certificate - For Each SKC Library installation on a SGX compute node, please change SKC_USER and SKC_USER_PASSWORD Save and Close ./skc_library_create_roles.sh Copy the token printed on console. Update skc_library.conf with the following - IP address for CMS and KBS services deployed on Enterprise system - CSP_CMS_IP should point to the IP of CMS service deployed on CSP system - CSP_SCS_IP should point to the IP of SCS service deployed on CSP system - Hostname of the Enterprise system where KBS is deployed - Network Port numbers for CMS and SCS services deployed on CSP system - Network Port numbers for CMS and KBS services deployed on Enterprise system - For Each SKC Library installation on a SGX compute node, please change SKC_USER (should be same as SKC_USER provided in create_roles.conf) - SKC_TOKEN with the token copied from previous step Save and Close ./deploy_skc_library.sh Deploying SKC Library as a Container Use the following steps to configure SKC library running in a container and to validate key transfer in container on bare metal and inside a VM on SGX enabled hosts. Note: All the configuration files required for SKC Library container are modified in the resources directory only 1. Docker should be installed, enabled and services should be active 2.To get the SKC library tar file, run \"make skc_library_k8s\". In the build System, SKC Library tar file \"<skc-lib*>.tar\" required to load is located in the \"/root/workspace/skc_library\" directory. 3. Copy \"resources\" folder from \"workspace/skc_library/container/resources\" to the \"/root/\" directory of SGX host. Inside the resources folder all the key transfer flow related files will be available. 4. Update sgx_default_qcnl.conf file inside resources folder with SCS IP and SCS port and also update the kms_npm.ini with KBS IP and KBS PORT and update hosts file present in same folder with KBS IP and hostname. 5. To create user and role for skc library, update the create_roles.conf, and run ./skc_library_create_roles.sh, which is inside the resources folder. 6. Generate the RSA key in the kbs host and copy the generated KBS certificate to SGX host under /root/. 7. Refer to openssl and nginx sub sections of Quick Start Guide in the \"Configuration for NGINX testing\" to configure nginx.conf and openssl.conf files which are under resource directory. 8. Update keyID in the keys.txt and nginx.conf. 9. Under [core] section of pkcs11-apimodule.ini in the \"/root/resources/\" directory add preload_keys=/root/keys.txt. 10. Update skc_library.conf with IP addresses where SKC services are deployed. 11. On the SGX Compute node, load the skc library docker image provided in the tar file. docker load < <SKC_Library>.tar 12. Provide valid paramenets in the docker run command and execute the docker run command. Update the genertaed RSA Key ID and <keys>.crt in the resources directory. docker run -p 8080:2443 -p 80:8080 --mount type=bind,source=/root/<KBS_cert>.crt,target=/root/<KBS_cert>.crt --mount type=bind,source=/root/resources/sgx_default_qcnl.conf,target=/etc/sgx_default_qcnl.conf --mount type=bind,source=/root/resources/nginx.conf,target=/etc/nginx/nginx.conf --mount type=bind,source=/root/resources/keys.txt,target=/root/keys.txt,readonly --mount type=bind,source=/root/resources/pkcs11-apimodule.ini,target=/opt/skc/etc/pkcs11-apimodule.ini,readonly --mount type=bind,source=/root/resources/kms_npm.ini,target=/opt/skc/etc/kms_npm.ini,readonly --mount type=bind,source=/root/resources/sgx_stm.ini,target=/opt/skc/etc/sgx_stm.ini,readonly --mount type=bind,source=/root/resources/openssl.cnf,target=/etc/pki/tls/openssl.cnf --mount type=bind,source=/root/resources/skc_library.conf,target=/skc_library.conf --add-host=<SGX_HOSTNAME>:<SGX_HOST_IP> --add-host=<KBS_Hostname>:<KBS host IP> --mount type=bind,source=/dev/sgx,target=/dev/sgx --cap-add=SYS_MODULE --privileged=true <SKC_LIBRARY_IMAGE_NAME> Note: In the above docker run command, source refers to the actual path of the files located on the host and the target always refers to the files which would be mounted inside the container 13. Establish a tls session with the nginx using the key transferred inside the enclave Get the container id using \"docker ps\" command docker exec -it <container_id> /bin/sh #Follow the steps only if proxy setup is required export http_proxy=http://<proxy-url>:<proxy-port> export https_proxy=http://<proxy-url>:<proxy-port> export no_proxy=0.0.0.0,127.0.0.1,localhost,<CSP IP>,<Enterprise IP>, <SGX Compute Node IP>, <KBS system Hostname> #Install wget dnf install wget wget https://localhost:2443 --no-check-certificate","title":"Deployment"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#deployment","text":"","title":"Deployment"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#deployment-using-ansible","text":"The below details would enable the deployment through Ansible Role for Intel\u00ae SecL-DC Secure Key Caching Usecase. However the services can still be installed manually using the Product Guide. More details on Ansible Role for Intel\u00ae SecL-DC in Ansible-Role repository.","title":"Deployment Using Ansible"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#download-the-ansible-role","text":"The role can be cloned locally from git and the contents can be copied to the roles folder used by your ansible server #Create directory for using ansible deployment mkdir -p /root/intel-secl/deploy/ #Clone the repository cd /root/intel-secl/deploy/ && git clone https://github.com/intel-secl/utils.git #Checkout to specific release version cd utils/ git checkout <release-version of choice> cd tools/ansible-role #Update ansible.cfg roles_path to point to path(/root/intel-secl/deploy/utils/tools/ansible-role/roles/)","title":"Download the Ansible Role"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#update-ansible-inventory","text":"The following inventory can be used and created under /etc/ansible/hosts [CSP] <machine1_ip/hostname> [Enterprise] <machine2_ip/hostname> [Node] <machine3_ip/hostname> [CSP:vars] isecl_role=csp ansible_user=root ansible_password=<password> [Enterprise:vars] isecl_role=enterprise ansible_user=root ansible_password=<password> [Node:vars] isecl_role=node ansible_user=root ansible_password=<password> Note Ansible requires ssh and root user access to remote machines. The following command can be used to ensure ansible can connect to remote machines with host key check ` ssh-keyscan -H <ip_address> >> /root/.ssh/known_hosts","title":"Update Ansible Inventory"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#create-and-run-playbook","text":"The following are playbook and CLI example for deploying Intel\u00ae SecL-DC binaries based on the supported deployment models and usecases. The below example playbooks can be created as site-bin-isecl.yml Note If running behind a proxy, update the proxy variables under <path to ansible role>/ansible-role/vars/main.yml and run as below Note Go through the Additional Examples and Tips section for specific workflow samples Option 1 Update the PCS Server key with following vars in <path to ansible role>/ansible-role/defaults/main.yml intel_provisioning_server_api_key_sandbox : <pcs server key> Create playbook with following contents - hosts : all gather_facts : yes any_errors_fatal : true vars : setup : <setup var from supported usecases> binaries_path : <path where built binaries are copied to> backend_pykmip : \"<yes/no to install pykmip server along with KMIP KBS>\" roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> OR Option 2: Create playbook with following contents - hosts : all gather_facts : yes any_errors_fatal : true roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> --extra-vars setup = <setup var from supported usecases> --extra-vars binaries_path = <path where built binaries are copied to> --extra-vars intel_provisioning_server_api_key = <pcs server key> --extra-vars backend_pykmip = yes Note If any service installation fails due to any misconfiguration, just uninstall the specific service manually , fix the misconfiguration in ansible and rerun the playbook. The successfully installed services won't be reinstalled.","title":"Create and Run Playbook"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#usecase-setup-options","text":"Usecase Variable Secure Key Caching setup: secure-key-caching in playbook or via --extra-vars as setup=secure-key-caching in CLI SGX Orchestration Kubernetes setup: sgx-orchestration-kubernetes in playbook or via --extra-vars as setup=sgx-orchestration-kubernetes in CLI SGX Attestation Kubernetes setup: sgx-attestation-kubernetes in playbook or via --extra-vars as setup=sgx-attestation-kubernetes in CLI SGX Orchestration Openstack setup: sgx-orchestration-openstack in playbook or via --extra-vars as setup=sgx-orchestration-openstack in CLI SGX Attestation Openstack setup: sgx-attestation-openstack in playbook or via --extra-vars as setup=sgx-attestation-openstack in CLI SKC No Orchestration setup: skc-no-orchestration in playbook or via --extra-vars as setup=skc-no-orchestration in CLI SGX Attestation No Orchestration setup: sgx-attestation-no-orchestration in playbook or via --extra-vars as setup=sgx-attestation-no-orchestration in CLI Note Orchestrator installation is not bundled with the role and need to be done independently. Also, components dependent on the orchestrator like isecl-k8s-extensions and integration-hub are installed either partially or not installed","title":"Usecase Setup Options"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#deployment-using-binaries","text":"","title":"Deployment Using Binaries"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#setup-k8s-cluster-and-deploy-isecl-k8s-extensions","text":"Setup master and worker node for k8s. Worker node should be setup on SGX enabled host machine. Master node can be any system. To setup k8 cluster follow https://phoenixnap.com/kb/how-to-install-kubernetes-on-centos Once the master/worker setup is done, follow below steps on Master Node:","title":"Setup K8S Cluster and Deploy Isecl-k8s-extensions"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#untar-packages-and-push-oci-images-to-registry","text":"Copy tar output isecl-k8s-extensions-*.tar.gz from build system's binaries folder to /opt/ directory on the Master Node and extract the contents. cd /opt/ tar -xvzf isecl-k8s-extensions-*.tar.gz cd isecl-k8s-extensions/ Configure private registry Push images to private registry using skopeo command, (this can be done from build vm also) skopeo copy oci-archive:isecl-k8s-controller-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-controller:v4.0.0 skopeo copy oci-archive:isecl-k8s-scheduler-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-scheduler:v4.0.0 Add the image names in isecl-controller.yml and isecl-scheduler.yml in /opt/isecl-k8s-extensions/yamls with full image name including registry IP/hostname (e.g : /isecl-k8s-scheduler:v4.0.0). It will automatically pull the images from registry.","title":"Untar packages and push OCI images to registry"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#deploy-isecl-controller","text":"Create hostattributes.crd.isecl.intel.com crd kubectl apply -f yamls/crd-1.17.yaml Check whether the crd is created kubectl get crds Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterrolebinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole=system:node --user=system:serviceaccount:isecl:isecl Fetch token required for ihub installation and follow below IHUB installation steps, kubectl get secrets -n isecl kubectl describe secret default-token-<name> -n isecl For IHUB installation, make sure to update below configuration in /root/binaries/env/ihub.env before installing ihub on CSP system: * Copy /etc/kubernetes/pki/apiserver.crt from master node to /root on CSP system. Update KUBERNETES_CERT_FILE. * Get k8s token in master, using above commands and update KUBERNETES_TOKEN * Update the value of CRD name KUBERNETES_CRD=custom-isecl-sgx","title":"Deploy isecl-controller"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#deploy-isecl-scheduler","text":"The isecl-scheduler default configuration is provided for common cluster support in /opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml. Variables HVS_IHUB_PUBLIC_KEY_PATH and SGX_IHUB_PUBLIC_KEY_PATH are by default set to default paths. Please use and set only required variables based on the use case. For example, if only sgx based attestation is required then remove/comment HVS_IHUB_PUBLIC_KEY_PATH variables. Install cfssl and cfssljson on Kubernetes Control Plane #Download cfssl to /usr/local/bin/ wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl #Download cfssljson to /usr/local/bin wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create tls key pair for isecl-scheduler service, which is signed by k8s apiserver.crt cd /opt/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \"<K8_MASTER_IP>\",\"<K8_MASTER_HOST>\" -c /etc/kubernetes/pki/ca.crt -k /etc/kubernetes/pki/ca.key After iHub deployment, copy /etc/ihub/ihub_public_key.pem from ihub to /opt/isecl-k8s-extensions/ directory on k8 master system. Also, copy tls key pair generated in previous step to secrets directory. mkdir secrets cp /opt/isecl-k8s-extensions/server.key secrets/ cp /opt/isecl-k8s-extensions/server.crt secrets/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem cp /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem secrets/ Note: Prefix the attestation type for ihub_public_key.pem before copying to secrets folder. Create kubernetes secrets scheduler-secret for isecl-scheduler kubectl create secret generic scheduler-certs --namespace isecl --from-file=secrets Deploy isecl-scheduler kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl","title":"Deploy isecl-scheduler"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#configure-kube-scheduler-to-establish-communication-with-isecl-scheduler","text":"Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec: containers: - command: - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched Note: Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml. * Restart Kubelet which restart all the k8s services including kube base schedular systemctl restart kubelet Check if CRD data is populated kubectl get -o json hostattributes.crd.isecl.intel.com","title":"Configure kube-scheduler to establish communication with isecl-scheduler"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#deploying-skc-services-on-single-system","text":"Copy the binaries directory generated in the build system to the /root/ directory on the deployment system Update orchestrator.conf with the following - Deployment system IP address - SAN List (a list of ip address and hostname for the deployment system) - Network Port numbers for CMS, AAS, SCS and SHVS - Install Admin and CSP Admin credentials - TENANT as KUBERNETES or OPENSTACK (based on the orchestrator chosen) - System IP address where Kubernetes or Openstack is deployed - Netowrk Port Number of Kubernetes or Openstack Keystone/Placement Service - Database name, Database username and password for SHVS Update enterprise_skc.conf with the following - Deployment system IP address - SAN List (a list of ip address and hostname for the deployment system) - Network Port numbers for CMS, AAS, SCS, SQVS and KBS - Install Admin and CSP Admin credentials - Database name, Database username and password for AAS and SCS services - Intel PCS Server API URL and API Keys - Key Manager can be set to either Directory or KMIP - KMIP server configuration if KMIP is set Save and Close ./install_skc.sh In case ihub installation fails, its recommended to run the following command to clear failed service instance systemctl reset-failed","title":"Deploying SKC Services on Single System"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#deploy-csp-skc-services","text":"Copy the binaries directory generated in the build system system to the /root/ directory on the CSP system Update csp_skc.conf with the following - CSP system IP Address - SAN List (a list of ip address and hostname for the CSP system) - Network Port numbers for CMS, AAS, SCS and SHVS - Install Admin and CSP Admin credentials - TENANT as KUBERNETES or OPENSTACK (based on the orchestrator chosen) - System IP address where Kubernetes or Openstack is deployed - Netowrk Port Number of Kubernetes or Openstack Keystone/Placement Service - Database name, Database username and password for AAS, SCS and SHVS services - Intel PCS Server API URL and API Keys Save and Close ./install_csp_skc.sh In case installation fails, its recommended to run the following command to clear failed service instance systemctl reset-failed Create sample yml file for nginx workload and add SGX labels to it such as: apiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: SGX-Enabled operator: In values: - \"true\" - key: EPC-Memory operator: In values: - \"2.0GB\" containers: - name: nginx image: nginx ports: - containerPort: 80 Validate if pod can be launched on the node. Run following commands: kubectl apply -f pod.yml kubectl get pods kubectl describe pods nginx Pod should be in running state and launched on the host as per values in pod.yml. Validate by running below command on sgx host: docker ps","title":"Deploy CSP SKC Services"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#openstack-setup-and-associate-traits","text":"Setup Compute and Controller node for Openstack. Compute node should be setup on SGX host machine, Controller node can be any system. After the compute/controller setup is done, follow the below steps: IHUB should be installed and configured with Openstack Note While using deployment scripts to install the components, in the env directory of the binaries folder comment \"KUBERNETES_TOKEN\" in the ihub.env before installation. Openstack compute node and build VM should have the same OS package repositories, else there will be package mismatch for SKC library. On the openstack controller, if resource provider is not listing the resources then install the \"osc-placement\" pip3 install osc-placement source the admin-openrc credentials to gain access to user-only CLI commands and export the os_placement_API_version source admin-openrc List the set of resources mapped to the Openstack openstack resource provider list Set the required traits for SGX Hosts #For example 'cirros' image can be used for the instances openstack image set --property trait:CUSTOM_ISECL_SGX_ENABLED_TRUE=required <image name> Veiw the Traits that has been set: #The trait should be set and assinged to the respective image successfully. For example 'cirros' image can be used for the instances openstack image show <image name> Verify the trait is enabled for the SGX Host: openstack resource provider trait list <uuid of the host which the openstack resoruce provider lists> #SGX Supported, SGX TCB upto Date, SGX FLC enabled, SGX EPC size attritubes of the SGX host for which the 'required' trait set to TRUE or FALSE is displayed. For example,if required trait is set as TRUE: CUSTOM_ISECL_SGX_ENABLED_TRUE CUSTOM_ISECL_SGX_SUPPORTED_TRUE CUSTOM_ISECL_SGX_TCBUPTODATE_FALSE CUSTOM_ISECL_SGX_FLC_ENABLED_TRUE CUSTOM_ISECL_SGX_EPC_SIZE_2_0_GB For example, if the required trait is set as FALSE CUSTOM_ISECL_SGX_ENABLED_FALSE CUSTOM_ISECL_SGX_SUPPORTED_TRUE CUSTOM_ISECL_SGX_TCBUPTODATE_FALSE CUSTOM_ISECL_SGX_FLC_ENABLED_FALSE CUSTOM_ISECL_SGX_EPC_SIZE_0_B Create the instances openstack server create --flavor tiny --image <image name> --net vmnet <vm instance name> Instances should be created and the status should be \"Active\". Instance should be launched successfully. openstack server list Note To unset the trait, use the following CLI commands: openstack image unset --property trait:CUSTOM_ISECL_SGX_ENABLED_TRUE <image name> openstack image unset --property trait:CUSTOM_ISECL_SGX_ENABLED_FALSE <image name>","title":"Openstack Setup and Associate Traits"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#deploy-enterprise-skc-services","text":"Copy the binaries directory generated in the build system to the /root/ directory on Enterprise system Update enterprise_skc.conf with the following - Enterprise system IP address - SAN List (a list of ip address and hostname for the Enterprise system) - Network Port numbers for CMS, AAS, SCS, SQVS and KBS - Install Admin credentials - Database name, Database username and passwords for AAS and SCS services - Intel PCS Server API URL and API Keys - KMIP server configuration if KMIP is set Save and Close ./install_enterprise_skc.sh","title":"Deploy Enterprise SKC Services"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#deploy-sgx-agent","text":"Copy sgx_agent.tar, sgx_agent.sha2 and agent_untar.sh from binaries directoy to a directory in SGX compute node ./agent_untar.sh Edit agent.conf with the following CSP system IP address where CMS, AAS, SHVS and SCS services deployed CSP Admin credentials (same which are provided in service configuration file. for ex: csp_skc.conf, orchestrator.conf or skc.conf) Network Port numbers for CMS, AAS, SCS and SHVS Token validity period in days CMS TLS SHA Value (Run \"cms tlscertsha384\" on CSP system) Save and Close Note In case orchestration support is not needed, please comment/delete SHVS_IP in agent.conf available in same folder ./deploy_sgx_agent.sh","title":"Deploy SGX Agent"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#deploy-skc-library","text":"Copy skc_library.tar, skc_library.sha2 and skclib_untar.sh from binaries directoy to a directory in SGX compute node ./skclib_untar.sh Update create_roles.conf with the following - IP address of AAS deployed on Enterprise system - Admin account credentials of AAS deployed on Enterprise system. These credentials should match with the AAS admin credentials provided in authservice.env on enterprise side. - Permission string to be embedded into skc_libraty client TLS Certificate - For Each SKC Library installation on a SGX compute node, please change SKC_USER and SKC_USER_PASSWORD Save and Close ./skc_library_create_roles.sh Copy the token printed on console. Update skc_library.conf with the following - IP address for CMS and KBS services deployed on Enterprise system - CSP_CMS_IP should point to the IP of CMS service deployed on CSP system - CSP_SCS_IP should point to the IP of SCS service deployed on CSP system - Hostname of the Enterprise system where KBS is deployed - Network Port numbers for CMS and SCS services deployed on CSP system - Network Port numbers for CMS and KBS services deployed on Enterprise system - For Each SKC Library installation on a SGX compute node, please change SKC_USER (should be same as SKC_USER provided in create_roles.conf) - SKC_TOKEN with the token copied from previous step Save and Close ./deploy_skc_library.sh","title":"Deploy SKC Library"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/8Deployment/#deploying-skc-library-as-a-container","text":"Use the following steps to configure SKC library running in a container and to validate key transfer in container on bare metal and inside a VM on SGX enabled hosts. Note: All the configuration files required for SKC Library container are modified in the resources directory only 1. Docker should be installed, enabled and services should be active 2.To get the SKC library tar file, run \"make skc_library_k8s\". In the build System, SKC Library tar file \"<skc-lib*>.tar\" required to load is located in the \"/root/workspace/skc_library\" directory. 3. Copy \"resources\" folder from \"workspace/skc_library/container/resources\" to the \"/root/\" directory of SGX host. Inside the resources folder all the key transfer flow related files will be available. 4. Update sgx_default_qcnl.conf file inside resources folder with SCS IP and SCS port and also update the kms_npm.ini with KBS IP and KBS PORT and update hosts file present in same folder with KBS IP and hostname. 5. To create user and role for skc library, update the create_roles.conf, and run ./skc_library_create_roles.sh, which is inside the resources folder. 6. Generate the RSA key in the kbs host and copy the generated KBS certificate to SGX host under /root/. 7. Refer to openssl and nginx sub sections of Quick Start Guide in the \"Configuration for NGINX testing\" to configure nginx.conf and openssl.conf files which are under resource directory. 8. Update keyID in the keys.txt and nginx.conf. 9. Under [core] section of pkcs11-apimodule.ini in the \"/root/resources/\" directory add preload_keys=/root/keys.txt. 10. Update skc_library.conf with IP addresses where SKC services are deployed. 11. On the SGX Compute node, load the skc library docker image provided in the tar file. docker load < <SKC_Library>.tar 12. Provide valid paramenets in the docker run command and execute the docker run command. Update the genertaed RSA Key ID and <keys>.crt in the resources directory. docker run -p 8080:2443 -p 80:8080 --mount type=bind,source=/root/<KBS_cert>.crt,target=/root/<KBS_cert>.crt --mount type=bind,source=/root/resources/sgx_default_qcnl.conf,target=/etc/sgx_default_qcnl.conf --mount type=bind,source=/root/resources/nginx.conf,target=/etc/nginx/nginx.conf --mount type=bind,source=/root/resources/keys.txt,target=/root/keys.txt,readonly --mount type=bind,source=/root/resources/pkcs11-apimodule.ini,target=/opt/skc/etc/pkcs11-apimodule.ini,readonly --mount type=bind,source=/root/resources/kms_npm.ini,target=/opt/skc/etc/kms_npm.ini,readonly --mount type=bind,source=/root/resources/sgx_stm.ini,target=/opt/skc/etc/sgx_stm.ini,readonly --mount type=bind,source=/root/resources/openssl.cnf,target=/etc/pki/tls/openssl.cnf --mount type=bind,source=/root/resources/skc_library.conf,target=/skc_library.conf --add-host=<SGX_HOSTNAME>:<SGX_HOST_IP> --add-host=<KBS_Hostname>:<KBS host IP> --mount type=bind,source=/dev/sgx,target=/dev/sgx --cap-add=SYS_MODULE --privileged=true <SKC_LIBRARY_IMAGE_NAME> Note: In the above docker run command, source refers to the actual path of the files located on the host and the target always refers to the files which would be mounted inside the container 13. Establish a tls session with the nginx using the key transferred inside the enclave Get the container id using \"docker ps\" command docker exec -it <container_id> /bin/sh #Follow the steps only if proxy setup is required export http_proxy=http://<proxy-url>:<proxy-port> export https_proxy=http://<proxy-url>:<proxy-port> export no_proxy=0.0.0.0,127.0.0.1,localhost,<CSP IP>,<Enterprise IP>, <SGX Compute Node IP>, <KBS system Hostname> #Install wget dnf install wget wget https://localhost:2443 --no-check-certificate","title":"Deploying SKC Library as a Container"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/9Usecase%20Workflows%20with%20Postman%20API%20Collections/","text":"Usecase Workflows with Postman API Collections The below allow to get started with workflows within Intel\u00ae SecL-DC for Foundational and Workload Security Usecases. More details available in API Collections repository Use Case Collections Use case Sub-Usecase API Collection Secure Key Caching - \u2714\ufe0f SGX Discovery, Provisioning and Orchestration - \u2714\ufe0f SGX Discovery and Provisioning - \u2714\ufe0f Download Postman API Collections Postman API Network for latest released collections: https://explore.postman.com/intelsecldc or Github repo for allreleases #Clone the github repo for api-collections git clone https://github.com/intel-secl/utils.git #Switch to specific release-version of choice cd utils/ git checkout <release-version of choice> #Import Collections from cd tools/api-collections Note The postman-collections are also available when cloning the repos via build manifest under utils/tools/api-collections Running API Collections Import the collection into Postman API Client Note This step is required only when not using Postman API Network and downloading from Github Update env as per the deployment details for specific usecase View Documentation Run the workflow","title":"Usecase Workflows with Postman API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/9Usecase%20Workflows%20with%20Postman%20API%20Collections/#usecase-workflows-with-postman-api-collections","text":"The below allow to get started with workflows within Intel\u00ae SecL-DC for Foundational and Workload Security Usecases. More details available in API Collections repository","title":"Usecase Workflows with Postman API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/9Usecase%20Workflows%20with%20Postman%20API%20Collections/#use-case-collections","text":"Use case Sub-Usecase API Collection Secure Key Caching - \u2714\ufe0f SGX Discovery, Provisioning and Orchestration - \u2714\ufe0f SGX Discovery and Provisioning - \u2714\ufe0f","title":"Use Case Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/9Usecase%20Workflows%20with%20Postman%20API%20Collections/#download-postman-api-collections","text":"Postman API Network for latest released collections: https://explore.postman.com/intelsecldc or Github repo for allreleases #Clone the github repo for api-collections git clone https://github.com/intel-secl/utils.git #Switch to specific release-version of choice cd utils/ git checkout <release-version of choice> #Import Collections from cd tools/api-collections Note The postman-collections are also available when cloning the repos via build manifest under utils/tools/api-collections","title":"Download Postman API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20RHEL/9Usecase%20Workflows%20with%20Postman%20API%20Collections/#running-api-collections","text":"Import the collection into Postman API Client Note This step is required only when not using Postman API Network and downloading from Github Update env as per the deployment details for specific usecase View Documentation Run the workflow","title":"Running API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/10System%20User%20Configuration/","text":"System User Configuration Build System Setup ~/.gitconfig to update the git user details. A sample config is provided below GIT Configuration** [user] name = John Doe email = john.doe@abc.com [color] ui = auto [push] default = matching Make sure system date and time of SGX machine and CSP machine both are in sync. Also, if the system is configured to read the RTC time in the local time zone, then use RTC in UTC by running timedatectl set-local-rtc 0 command on both the machine. Otherwise SGX Agent deployment will fail with certificate expiry error.","title":"System User Configuration"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/10System%20User%20Configuration/#system-user-configuration","text":"Build System Setup ~/.gitconfig to update the git user details. A sample config is provided below GIT Configuration** [user] name = John Doe email = john.doe@abc.com [color] ui = auto [push] default = matching Make sure system date and time of SGX machine and CSP machine both are in sync. Also, if the system is configured to read the RTC time in the local time zone, then use RTC in UTC by running timedatectl set-local-rtc 0 command on both the machine. Otherwise SGX Agent deployment will fail with certificate expiry error.","title":"System User Configuration"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/11Appendix/","text":"Appendix SGX Attestation flow To Deploy SampleApp: Copy sample_apps.tar, sample_apps.sha2 and sampleapps_untar.sh from binaries directory to a directory in SGX compute node and untar it using './sample_apps_untar.sh' Install Intel\u00ae SGX SDK for Linux*OS into /opt/intel/sgxsdk using './install_sgxsdk.sh' Install SGX dependencies using './deploy_sgx_dependencies.sh' Note: Make sure to deploy SQVS with includetoken configuration as false. To Verify the SampleApp flow: Update sample_apps.conf with the following - IP address for SQVS services deployed on Enterprise system - IP address for SCS services deployed on CSP system - ENTERPRISE_CMS_IP should point to the IP of CMS service deployed on Enterprise system - Network Port numbers for SCS services deployed on CSP system - Network Port numbers for SQVS and CMS services deployed on Enterprise system - Set RUN_ATTESTING_APP to yes if user wants to run both apps in same machine Run SampleApp using './run_sample_apps.sh' Check the output of attestedApp and attestingApp under out/attested_app_console_out.log and out/attesting_app_console_out.log files Creating RSA Keys in Key Broker Service Steps to run KMIP Server Note Below mentioned steps are provided as script (install_pykmip.sh and pykmip.service) as part of kbs_script folder which will install KMIP Server as daemon. Refer to \u2018Install KMIP Server as daemon\u2019 section. 1. Install python3 and vim-common # apt -y install python3-pip vim-common ln -s /usr/bin/python3 /usr/bin/python > /dev/null 2>&1 ln -s /usr/bin/pip3 /usr/bin/pip > /dev/null 2>&1 2. Install pykmip # pip3 install pykmip==0.9.1 3. In the /etc/ directory create pykmip and policies folders mkdir -p /etc/pykmip/policies 4. Configure pykmip server using server.conf Update hostname in the server.conf 5. Copy the following to /etc/pykmip/ from kbs_script folder available under binaries directory create_certificates.py, run_server.py, server.conf 6. Create certificates > cd /etc/pykmip > python3 create_certificates.py <KMIP Host IP/KMIP Host FQDN> 7. Kill running KMIP Server processes and wait for 10 seconds until all the KMIP Server processes are killed. > ps -ef | grep run_server.py | grep -v grep | awk '{print $2}' | xargs kill 8. Run pykmip server using run_server.py script > python3 run_server.py & Install KMIP Server as daemon 1. cd into /root/binaries/kbs_script folder 2. Configure pykmip server using server.conf Update hostname in the server.conf 3. Run the install_pykmip.sh script and KMIP server will be installed as daemon process ./install_pykmip.sh Create RSA key in PyKMIP and generate certificate Note This step is required only when PyKMIP script is used as a backend KMIP server. 1. Update Host IP in /root/binaries/kbs_script rsa_create.py script 2. In the kbs_script folder, Run rsa_create.py script > cd /root/binaries/kbs_script > python3 rsa_create.py This script will generate \u201cPrivate Key ID\u201d and \u201cServer certificate\u201d, which should be provided in the kbs.conf file for \u201cKMIP_KEY_ID\u201d and \u201cSERVER_CERT\u201d. Configuration Update to create Keys in KBS cd into /root/binaries/kbs_script folder **To register keys with KBS KMIP** Update the following variables in kbs.conf: KMIP_KEY_ID (Private key ID registered in KMIP server) SERVER_CERT (Server certificate for created private key) Enterprise system IP address where CMS, AAS and KBS services are deployed Port of CMS, AAS and KBS services deployed on enterprise system AAS admin and Enterprise admin credentials Note If KMIP_KEY_ID is not provided then RSA key register will be done with keystring. Update sgx_enclave_measurement_anyof value in transfer_policy_request.json with enclave measurement value obtained using sgx_sign utility. Refer to \"Extracting SGX Enclave values for Key Transfer Policy\" section. Create RSA Key Execute the command ./run.sh reg copy the generated cert file to SGX Compute node where skc_library is deployed. Also make a note of the key id generated Configuration for NGINX testing Note Below mentioned OpenSSL and NGINX configuration updates are provided as patches (nginx.patch and openssl.patch) as part of skc_library deployment script. Patch can be applied with default nginx and openssl file. In case nginx/openssl contains any external changes then refer manual step. Apply Patch Execute the command with nginx version - nginx 1.14.0 (Ubuntu) and openssl version- Openssl 1.1.1 (Ubuntu) patch -b /etc/nginx/nginx.conf < nginx_ubuntu.patch patch -b /etc/ssl/openssl.cnf < openssl_ubuntu.patch OpenSSL In the /etc/ssl/openssl.cnf file, look for the below line: [ new_oids ] Just before the line [ new_oids ], add the below section: openssl_conf = openssl_def [openssl_def] engines = engine_section oid_section = new_oids [engine_section] pkcs11 = pkcs11_section [pkcs11_section] engine_id = pkcs11 dynamic_path =/usr/lib/x86_64-linux-gnu/engines-1.1/pkcs11.so MODULE_PATH =/opt/skc/lib/libpkcs11-api.so init = 0 Nginx Update nginx configuration file /etc/nginx/nginx.conf with below changes: ssl_engine pkcs11; Update the location of certificate with the loaction where it was copied into the skc_library machine. ssl_certificate \"add absolute path of crt file\"; Update the fields(token, object and pin-value) with the values given in keys.txt for the KeyID corresponding to the certificate. ssl_certificate_key \"engine:pkcs11:pkcs11:token=KMS;object=RSAKEY;pin-value=1234\"; SKC Configuration Create keys.txt in /root folder. This provides key preloading functionality in skc_library. Any number of keys can be added in keys.txt. Each PKCS11 URL should contain different Key ID which need to be transferred from KBS along with respective object tag for each key id specified Sample PKCS11 url is as below pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234; Token, object and pin-value given in PKCS11 url entry in keys.txt should match with the one in nginx.conf. The keyID should match the keyID of RSA key created in KBS. File location should match with preload_keys directive in pkcs11-apimodule.ini; Sample /opt/skc/etc/pkcs11-apimodule.ini file [core] preload_keys=/root/keys.txt keyagent_conf=/opt/skc/etc/key-agent.ini mode=SGX debug=true [SGX] module=/opt/intel/cryptoapitoolkit/lib/libp11sgx.so KBS key-transfer flow validation On SGX Compute node, Execute below commands for KBS key-transfer: Note Before initiating key transfer make sure, PYKMIP server is running. pkill nginx Remove any existing pkcs11 token rm -rf /opt/intel/cryptoapitoolkit/tokens/* Initiate Key transfer from KBS systemctl restart nginx Changing group ownership and permissions of pkcs11 token chown -R root:intel /opt/intel/cryptoapitoolkit/tokens/ chmod -R 770 /opt/intel/cryptoapitoolkit/tokens/ Establish a tls session with the nginx using the key transferred inside the enclave wget https://localhost:2443 --no-check-certificate Note on Key Transfer Policy Key transfer policy is used to enforce a set of policies which need to be compiled with before the secret can be securely provisioned onto a sgx enclave A typical Key Transfer Policy would look as below \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"], \"sgx_enclave_issuer_product_id_anyof\":[0], \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"], \"tls_client_certificate_issuer_cn_anyof\":[\"CMSCA\", \"CMS TLS Client CA\"], \"client_permissions_allof\":[\"nginx\",\"USA\"], \"sgx_enforce_tcb_up_to_date\":false sgx_enclave_issuer_anyof establishes the signing identity provided by an authority who has signed the sgx enclave. in other words the owner of the enclave sgx_enclave_measurement_anyof represents the cryptographic hash of the enclave log (enclave code, data) sgx_enforce_tcb_up_to_date - If set to true, Key Broker service will provision the key only of the platform generating the quote conforms to the latest Trusted Computing Base client_permissions_allof - Special permission embedded into the skc_library client TLS certificate which can enforce additional restrictons on who can get access to the key, In above example: the key is provisioned only to the nginx workload and platform which is tagged with value for ex: USA Note on SKC Library Deployment SKC Library Deployment (Binary as well as container) needs to performed with root privilege For binary deployment of SKC client Library, only one instance of Workload can use SKC Client Library. The config information for SKC client library is bound to the workload. In future, Multiple workloads might be supported For container deployment, since configmaps are used, each container instance of workload gets its own private SKC Client Library config information The SKC Client Library TLS client certificate private key is stored in the configuration directories and can be read only with elevated root privileges keys.txt (set of PKCS11 URIs for the keys to be securely provisioned into an SGX enclave) can only be modified with elevated privileges Extracting SGX Enclave values for Key Transfer Policy Values that are specific to the enclave such as sgx_enclave_issuer_anyof, sgx_enclave_measurement_anyof and sgx_enclave_issuer_product_id_anyof can be retrived using sgx_sign utility that is available as part of Intel SGX SDK. Run sgx_sign utility on the signed enclave (This command should be run on the build system). /opt/intel/sgxsdk/bin/x64/sgx_sign dump -enclave <path to the signed enclave> -dumpfile info.txt For sgx_enclave_issuer_anyof , in info.txt, search for \"mrsigner->value\" . E.g mrsigner->value : mrsigner->value: \"0x83 0xd7 0x19 0xe7 0x7d 0xea 0xca 0x14 0x70 0xf6 0xba 0xf6 0x2a 0x4d 0x77 0x43 0x03 0xc8 0x99 0xdb 0x69 0x02 0x0f 0x9c 0x70 0xee 0x1d 0xfc 0x08 0xc7 0xce 0x9e\" Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"] For sgx_enclave_measurement_anyof , in info.txt, search for metadata->enclave_css.body.enclave_hash.m . E.g metadata->enclave_css.body.enclave_hash.m : metadata->enclave_css.body.enclave_hash.m: 0xad 0x46 0x74 0x9e 0xd4 0x1e 0xba 0xa2 0x32 0x72 0x52 0x04 0x1e 0xe7 0x46 0xd3 0x79 0x1a 0x9f 0x24 0x31 0x83 0x0f 0xee 0x08 0x83 0xf7 0x99 0x3c 0xaf 0x31 0x6a Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"] Please note that the SGX Enclave measurement value will depend on the toolchain used to build and link the SGX enclave. Hence the SGX Enclave measurement value would differ across OS flavours. For more details please refer https://github.com/intel/linux-sgx/tree/master/linux/reproducibility","title":"Appendix"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/11Appendix/#appendix","text":"","title":"Appendix"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/11Appendix/#sgx-attestation-flow","text":"To Deploy SampleApp: Copy sample_apps.tar, sample_apps.sha2 and sampleapps_untar.sh from binaries directory to a directory in SGX compute node and untar it using './sample_apps_untar.sh' Install Intel\u00ae SGX SDK for Linux*OS into /opt/intel/sgxsdk using './install_sgxsdk.sh' Install SGX dependencies using './deploy_sgx_dependencies.sh' Note: Make sure to deploy SQVS with includetoken configuration as false. To Verify the SampleApp flow: Update sample_apps.conf with the following - IP address for SQVS services deployed on Enterprise system - IP address for SCS services deployed on CSP system - ENTERPRISE_CMS_IP should point to the IP of CMS service deployed on Enterprise system - Network Port numbers for SCS services deployed on CSP system - Network Port numbers for SQVS and CMS services deployed on Enterprise system - Set RUN_ATTESTING_APP to yes if user wants to run both apps in same machine Run SampleApp using './run_sample_apps.sh' Check the output of attestedApp and attestingApp under out/attested_app_console_out.log and out/attesting_app_console_out.log files","title":"SGX Attestation flow"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/11Appendix/#creating-rsa-keys-in-key-broker-service","text":"Steps to run KMIP Server Note Below mentioned steps are provided as script (install_pykmip.sh and pykmip.service) as part of kbs_script folder which will install KMIP Server as daemon. Refer to \u2018Install KMIP Server as daemon\u2019 section. 1. Install python3 and vim-common # apt -y install python3-pip vim-common ln -s /usr/bin/python3 /usr/bin/python > /dev/null 2>&1 ln -s /usr/bin/pip3 /usr/bin/pip > /dev/null 2>&1 2. Install pykmip # pip3 install pykmip==0.9.1 3. In the /etc/ directory create pykmip and policies folders mkdir -p /etc/pykmip/policies 4. Configure pykmip server using server.conf Update hostname in the server.conf 5. Copy the following to /etc/pykmip/ from kbs_script folder available under binaries directory create_certificates.py, run_server.py, server.conf 6. Create certificates > cd /etc/pykmip > python3 create_certificates.py <KMIP Host IP/KMIP Host FQDN> 7. Kill running KMIP Server processes and wait for 10 seconds until all the KMIP Server processes are killed. > ps -ef | grep run_server.py | grep -v grep | awk '{print $2}' | xargs kill 8. Run pykmip server using run_server.py script > python3 run_server.py & Install KMIP Server as daemon 1. cd into /root/binaries/kbs_script folder 2. Configure pykmip server using server.conf Update hostname in the server.conf 3. Run the install_pykmip.sh script and KMIP server will be installed as daemon process ./install_pykmip.sh Create RSA key in PyKMIP and generate certificate Note This step is required only when PyKMIP script is used as a backend KMIP server. 1. Update Host IP in /root/binaries/kbs_script rsa_create.py script 2. In the kbs_script folder, Run rsa_create.py script > cd /root/binaries/kbs_script > python3 rsa_create.py This script will generate \u201cPrivate Key ID\u201d and \u201cServer certificate\u201d, which should be provided in the kbs.conf file for \u201cKMIP_KEY_ID\u201d and \u201cSERVER_CERT\u201d. Configuration Update to create Keys in KBS cd into /root/binaries/kbs_script folder **To register keys with KBS KMIP** Update the following variables in kbs.conf: KMIP_KEY_ID (Private key ID registered in KMIP server) SERVER_CERT (Server certificate for created private key) Enterprise system IP address where CMS, AAS and KBS services are deployed Port of CMS, AAS and KBS services deployed on enterprise system AAS admin and Enterprise admin credentials Note If KMIP_KEY_ID is not provided then RSA key register will be done with keystring. Update sgx_enclave_measurement_anyof value in transfer_policy_request.json with enclave measurement value obtained using sgx_sign utility. Refer to \"Extracting SGX Enclave values for Key Transfer Policy\" section. Create RSA Key Execute the command ./run.sh reg copy the generated cert file to SGX Compute node where skc_library is deployed. Also make a note of the key id generated","title":"Creating RSA Keys in Key Broker Service"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/11Appendix/#configuration-for-nginx-testing","text":"Note Below mentioned OpenSSL and NGINX configuration updates are provided as patches (nginx.patch and openssl.patch) as part of skc_library deployment script. Patch can be applied with default nginx and openssl file. In case nginx/openssl contains any external changes then refer manual step. Apply Patch Execute the command with nginx version - nginx 1.14.0 (Ubuntu) and openssl version- Openssl 1.1.1 (Ubuntu) patch -b /etc/nginx/nginx.conf < nginx_ubuntu.patch patch -b /etc/ssl/openssl.cnf < openssl_ubuntu.patch OpenSSL In the /etc/ssl/openssl.cnf file, look for the below line: [ new_oids ] Just before the line [ new_oids ], add the below section: openssl_conf = openssl_def [openssl_def] engines = engine_section oid_section = new_oids [engine_section] pkcs11 = pkcs11_section [pkcs11_section] engine_id = pkcs11 dynamic_path =/usr/lib/x86_64-linux-gnu/engines-1.1/pkcs11.so MODULE_PATH =/opt/skc/lib/libpkcs11-api.so init = 0 Nginx Update nginx configuration file /etc/nginx/nginx.conf with below changes: ssl_engine pkcs11; Update the location of certificate with the loaction where it was copied into the skc_library machine. ssl_certificate \"add absolute path of crt file\"; Update the fields(token, object and pin-value) with the values given in keys.txt for the KeyID corresponding to the certificate. ssl_certificate_key \"engine:pkcs11:pkcs11:token=KMS;object=RSAKEY;pin-value=1234\"; SKC Configuration Create keys.txt in /root folder. This provides key preloading functionality in skc_library. Any number of keys can be added in keys.txt. Each PKCS11 URL should contain different Key ID which need to be transferred from KBS along with respective object tag for each key id specified Sample PKCS11 url is as below pkcs11:token=KMS;id=164b41ae-be61-4c7c-a027-4a2ab1e5e4c4;object=RSAKEY;type=private;pin-value=1234; Token, object and pin-value given in PKCS11 url entry in keys.txt should match with the one in nginx.conf. The keyID should match the keyID of RSA key created in KBS. File location should match with preload_keys directive in pkcs11-apimodule.ini; Sample /opt/skc/etc/pkcs11-apimodule.ini file [core] preload_keys=/root/keys.txt keyagent_conf=/opt/skc/etc/key-agent.ini mode=SGX debug=true [SGX] module=/opt/intel/cryptoapitoolkit/lib/libp11sgx.so","title":"Configuration for NGINX testing"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/11Appendix/#kbs-key-transfer-flow-validation","text":"On SGX Compute node, Execute below commands for KBS key-transfer: Note Before initiating key transfer make sure, PYKMIP server is running. pkill nginx Remove any existing pkcs11 token rm -rf /opt/intel/cryptoapitoolkit/tokens/* Initiate Key transfer from KBS systemctl restart nginx Changing group ownership and permissions of pkcs11 token chown -R root:intel /opt/intel/cryptoapitoolkit/tokens/ chmod -R 770 /opt/intel/cryptoapitoolkit/tokens/ Establish a tls session with the nginx using the key transferred inside the enclave wget https://localhost:2443 --no-check-certificate","title":"KBS key-transfer flow validation"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/11Appendix/#note-on-key-transfer-policy","text":"Key transfer policy is used to enforce a set of policies which need to be compiled with before the secret can be securely provisioned onto a sgx enclave A typical Key Transfer Policy would look as below \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"], \"sgx_enclave_issuer_product_id_anyof\":[0], \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"], \"tls_client_certificate_issuer_cn_anyof\":[\"CMSCA\", \"CMS TLS Client CA\"], \"client_permissions_allof\":[\"nginx\",\"USA\"], \"sgx_enforce_tcb_up_to_date\":false sgx_enclave_issuer_anyof establishes the signing identity provided by an authority who has signed the sgx enclave. in other words the owner of the enclave sgx_enclave_measurement_anyof represents the cryptographic hash of the enclave log (enclave code, data) sgx_enforce_tcb_up_to_date - If set to true, Key Broker service will provision the key only of the platform generating the quote conforms to the latest Trusted Computing Base client_permissions_allof - Special permission embedded into the skc_library client TLS certificate which can enforce additional restrictons on who can get access to the key, In above example: the key is provisioned only to the nginx workload and platform which is tagged with value for ex: USA","title":"Note on Key Transfer Policy"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/11Appendix/#note-on-skc-library-deployment","text":"SKC Library Deployment (Binary as well as container) needs to performed with root privilege For binary deployment of SKC client Library, only one instance of Workload can use SKC Client Library. The config information for SKC client library is bound to the workload. In future, Multiple workloads might be supported For container deployment, since configmaps are used, each container instance of workload gets its own private SKC Client Library config information The SKC Client Library TLS client certificate private key is stored in the configuration directories and can be read only with elevated root privileges keys.txt (set of PKCS11 URIs for the keys to be securely provisioned into an SGX enclave) can only be modified with elevated privileges","title":"Note on SKC Library Deployment"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/11Appendix/#extracting-sgx-enclave-values-for-key-transfer-policy","text":"Values that are specific to the enclave such as sgx_enclave_issuer_anyof, sgx_enclave_measurement_anyof and sgx_enclave_issuer_product_id_anyof can be retrived using sgx_sign utility that is available as part of Intel SGX SDK. Run sgx_sign utility on the signed enclave (This command should be run on the build system). /opt/intel/sgxsdk/bin/x64/sgx_sign dump -enclave <path to the signed enclave> -dumpfile info.txt For sgx_enclave_issuer_anyof , in info.txt, search for \"mrsigner->value\" . E.g mrsigner->value : mrsigner->value: \"0x83 0xd7 0x19 0xe7 0x7d 0xea 0xca 0x14 0x70 0xf6 0xba 0xf6 0x2a 0x4d 0x77 0x43 0x03 0xc8 0x99 0xdb 0x69 0x02 0x0f 0x9c 0x70 0xee 0x1d 0xfc 0x08 0xc7 0xce 0x9e\" Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_issuer_anyof\":[\"83d719e77deaca1470f6baf62a4d774303c899db69020f9c70ee1dfc08c7ce9e\"] For sgx_enclave_measurement_anyof , in info.txt, search for metadata->enclave_css.body.enclave_hash.m . E.g metadata->enclave_css.body.enclave_hash.m : metadata->enclave_css.body.enclave_hash.m: 0xad 0x46 0x74 0x9e 0xd4 0x1e 0xba 0xa2 0x32 0x72 0x52 0x04 0x1e 0xe7 0x46 0xd3 0x79 0x1a 0x9f 0x24 0x31 0x83 0x0f 0xee 0x08 0x83 0xf7 0x99 0x3c 0xaf 0x31 0x6a Remove the whitespace and 0x characters from the above string and add it to the policy file. E.g : \"sgx_enclave_measurement_anyof\":[\"ad46749ed41ebaa2327252041ee746d3791a9f2431830fee0883f7993caf316a\"] Please note that the SGX Enclave measurement value will depend on the toolchain used to build and link the SGX enclave. Hence the SGX Enclave measurement value would differ across OS flavours. For more details please refer https://github.com/intel/linux-sgx/tree/master/linux/reproducibility","title":"Extracting SGX Enclave values for Key Transfer Policy"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/1Hardware%20%26%20OS%20Requirements/","text":"Hardware & OS Requirements Four Hosts or VMs Build System CSP managed Services Enterprise Managed Services Orchestrator Node Setup SGX Enabled Host OS Requirements UBUNTU 18.04. SKC Solution is built, installed and tested with root privileges. Please ensure that all the following instructions are executed with root privileges","title":"Hardware & OS Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/1Hardware%20%26%20OS%20Requirements/#hardware-os-requirements","text":"","title":"Hardware &amp; OS Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/1Hardware%20%26%20OS%20Requirements/#four-hosts-or-vms","text":"Build System CSP managed Services Enterprise Managed Services Orchestrator Node Setup","title":"Four Hosts or VMs"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/1Hardware%20%26%20OS%20Requirements/#sgx-enabled-host","text":"","title":"SGX Enabled Host"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/1Hardware%20%26%20OS%20Requirements/#os-requirements","text":"UBUNTU 18.04. SKC Solution is built, installed and tested with root privileges. Please ensure that all the following instructions are executed with root privileges","title":"OS Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/2Network%20Requirements/","text":"Network Requirements Internet access is required for the following Build System CSP Managed Services Enterprise Managed Services SGX Enabled Host Setting Proxy and No Proxy export http_proxy=http://<proxy-url>:<proxy-port> export https_proxy=http://<proxy-url>:<proxy-port> export no_proxy=0.0.0.0,127.0.0.1,localhost,<CSP IP>,<Enterprise IP>, <SGX Compute Node IP>, <KBS system Hostname> Firewall Settings Ensure that all the SKC service ports are accessible with firewall","title":"Network Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/2Network%20Requirements/#network-requirements","text":"Internet access is required for the following Build System CSP Managed Services Enterprise Managed Services SGX Enabled Host Setting Proxy and No Proxy export http_proxy=http://<proxy-url>:<proxy-port> export https_proxy=http://<proxy-url>:<proxy-port> export no_proxy=0.0.0.0,127.0.0.1,localhost,<CSP IP>,<Enterprise IP>, <SGX Compute Node IP>, <KBS system Hostname> Firewall Settings Ensure that all the SKC service ports are accessible with firewall","title":"Network Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/3Ubuntu%20Package%20Requirements/","text":"Ubuntu Package Requirements Access required for the postgresql repo in all systems, through below steps: Create the file repository configuration sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list' Import the repository signing key wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - Update the package lists apt-get update","title":"Ubuntu Package Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/3Ubuntu%20Package%20Requirements/#ubuntu-package-requirements","text":"Access required for the postgresql repo in all systems, through below steps: Create the file repository configuration sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list' Import the repository signing key wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - Update the package lists apt-get update","title":"Ubuntu Package Requirements"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/4Deployment%20Model/","text":"Deployment Model Build + Deployment Machine CSP - ISecL Services Machine Physical Server as per supported configurations Enterprise - ISecL Services Machine","title":"Deployment Model"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/4Deployment%20Model/#deployment-model","text":"Build + Deployment Machine CSP - ISecL Services Machine Physical Server as per supported configurations Enterprise - ISecL Services Machine","title":"Deployment Model"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/5Ubuntu%20System%20Tools%20and%20Utilities/","text":"Ubuntu System Tools and Utilities Ubuntu System Tools and utils apt-get install -y software-properties-common git gcc zip wget make python3 python3-yaml python3-pip tar lsof jq nginx curl libssl-dev ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip add-apt-repository ppa:projectatomic/ppa apt-get update apt-get install skopeo apt-get install makeself export PATH=/usr/local/bin:$PATH Note After skopeo installation, if /etc/containers/policy.json file not available then follow below steps: mkdir /etc/containers touch /etc/containers/policy.json #Add below contents in policy.json { \"default\": [ { \"type\": \"insecureAcceptAnything\" } ], \"transports\": { \"docker-daemon\": { \"\": [{\"type\":\"insecureAcceptAnything\"}] } } } Repo Tool tmpdir=$(mktemp -d) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir/repo /usr/local/bin rm -rf $tmpdir Golang Installation wget https://dl.google.com/go/go1.14.1.linux-amd64.tar.gz tar -xzf go1.14.1.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT=/usr/local/go export PATH=$GOROOT/bin:$PATH rm -rf go1.14.1.linux-amd64.tar.gz To Generate Swagger Documentation for components, Java Runtime needs to be installed apt-get install -y default-jre","title":"Ubuntu System Tools and Utilities"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/5Ubuntu%20System%20Tools%20and%20Utilities/#ubuntu-system-tools-and-utilities","text":"Ubuntu System Tools and utils apt-get install -y software-properties-common git gcc zip wget make python3 python3-yaml python3-pip tar lsof jq nginx curl libssl-dev ln -s /usr/bin/python3 /usr/bin/python ln -s /usr/bin/pip3 /usr/bin/pip add-apt-repository ppa:projectatomic/ppa apt-get update apt-get install skopeo apt-get install makeself export PATH=/usr/local/bin:$PATH Note After skopeo installation, if /etc/containers/policy.json file not available then follow below steps: mkdir /etc/containers touch /etc/containers/policy.json #Add below contents in policy.json { \"default\": [ { \"type\": \"insecureAcceptAnything\" } ], \"transports\": { \"docker-daemon\": { \"\": [{\"type\":\"insecureAcceptAnything\"}] } } } Repo Tool tmpdir=$(mktemp -d) git clone https://gerrit.googlesource.com/git-repo $tmpdir install -m 755 $tmpdir/repo /usr/local/bin rm -rf $tmpdir Golang Installation wget https://dl.google.com/go/go1.14.1.linux-amd64.tar.gz tar -xzf go1.14.1.linux-amd64.tar.gz sudo mv go /usr/local export GOROOT=/usr/local/go export PATH=$GOROOT/bin:$PATH rm -rf go1.14.1.linux-amd64.tar.gz To Generate Swagger Documentation for components, Java Runtime needs to be installed apt-get install -y default-jre","title":"Ubuntu System Tools and Utilities"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/6Build%20Services%2C%20Libraries%20and%20Install%20packages/","text":"Build Services, Libraries and Install packages Note Currently, the repos contain the source code of both the SGX Attestation Infrastructure and SKC. Make will build and package all the binaries and installation scripts but the SGX Attestation Infrastructure can be installed and deployed separately. SKC cannot be installed without the SGX Attestation Infrastructure. The rest of this document will indicate steps that are only needed for SKC. Pulling Source Code mkdir -p /root/workspace && cd /root/workspace repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/skc.xml repo sync Install, Enable and start the Docker daemon wget https://download.docker.com/linux/ubuntu/dists/bionic/pool/stable/amd64/containerd.io_1.2.10-3_amd64.deb dpkg -i containerd.io_1.2.10-3_amd64.deb wget \"https://download.docker.com/linux/ubuntu/dists/bionic/pool/stable/amd64/docker-ce-cli_19.03.5~3-0~ubuntu-bionic_amd64.deb\" dpkg -i docker-ce-cli_19.03.5~3-0~ubuntu-bionic_amd64.deb wget \"https://download.docker.com/linux/ubuntu/dists/bionic/pool/stable/amd64/docker-ce_19.03.5~3-0~ubuntu-bionic_amd64.deb\" dpkg -i docker-ce_19.03.5~3-0~ubuntu-bionic_amd64.deb systemctl enable docker systemctl start docker Ignore the below steps if not running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf and set proxy server details if proxy is used [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" #Reload docker systemctl daemon-reload systemctl restart docker Building All SKC Components make Copy Binaries to a clean folder For CSP/Enterprise Deployment Model, copy the generated binaries directory to the /root directory on the CSP/Enterprise system For Single system model, copy the generated binaries directory to the /root directory on the deployment system","title":"Build Services, Libraries and Install packages"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/6Build%20Services%2C%20Libraries%20and%20Install%20packages/#build-services-libraries-and-install-packages","text":"Note Currently, the repos contain the source code of both the SGX Attestation Infrastructure and SKC. Make will build and package all the binaries and installation scripts but the SGX Attestation Infrastructure can be installed and deployed separately. SKC cannot be installed without the SGX Attestation Infrastructure. The rest of this document will indicate steps that are only needed for SKC. Pulling Source Code mkdir -p /root/workspace && cd /root/workspace repo init -u https://github.com/intel-secl/build-manifest.git -b refs/tags/v4.0.0 -m manifest/skc.xml repo sync Install, Enable and start the Docker daemon wget https://download.docker.com/linux/ubuntu/dists/bionic/pool/stable/amd64/containerd.io_1.2.10-3_amd64.deb dpkg -i containerd.io_1.2.10-3_amd64.deb wget \"https://download.docker.com/linux/ubuntu/dists/bionic/pool/stable/amd64/docker-ce-cli_19.03.5~3-0~ubuntu-bionic_amd64.deb\" dpkg -i docker-ce-cli_19.03.5~3-0~ubuntu-bionic_amd64.deb wget \"https://download.docker.com/linux/ubuntu/dists/bionic/pool/stable/amd64/docker-ce_19.03.5~3-0~ubuntu-bionic_amd64.deb\" dpkg -i docker-ce_19.03.5~3-0~ubuntu-bionic_amd64.deb systemctl enable docker systemctl start docker Ignore the below steps if not running behind a proxy mkdir -p /etc/systemd/system/docker.service.d touch /etc/systemd/system/docker.service.d/proxy.conf #Add the below lines in proxy.conf and set proxy server details if proxy is used [ Service ] Environment = \"HTTP_PROXY=<http_proxy>\" Environment = \"HTTPS_PROXY=<https_proxy>\" Environment = \"NO_PROXY=<no_proxy>\" #Reload docker systemctl daemon-reload systemctl restart docker Building All SKC Components make Copy Binaries to a clean folder For CSP/Enterprise Deployment Model, copy the generated binaries directory to the /root directory on the CSP/Enterprise system For Single system model, copy the generated binaries directory to the /root directory on the deployment system","title":"Build Services, Libraries and Install packages"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/7Deployment%20%26%20Usecase%20Workflow%20Tools%20Installation/","text":"Deployment & Usecase Workflow Tools Installation The below installation is required on the Build & Deployment system only and the Platform(Windows,Linux or MacOS) for Usecase Workflow Tool Installation Deployment Tools Installation Install Ansible on Build Machine pip3 install ansible == 2 .9.10 Install sshpass for ansible to connect to remote hosts using SSH apt-get install sshpass Create directory for ansible default configuration and hosts file mkdir -p /etc/ansible/ touch /etc/ansible/ansible.cfg Copy the ansible.cfg contents from https://raw.githubusercontent.com/ansible/ansible/v2.9.10/examples/ansible.cfg and paste it under /etc/ansible/ansible.cfg Usecases Workflow Tools Installation Postman client should be downloaded on supported platforms or on the web to get started with the usecase collections. Note The Postman API Network will always have the latest released version of the API Collections. For all releases, refer the github repository for API Collections","title":"Deployment & Usecase Workflow Tools Installation"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/7Deployment%20%26%20Usecase%20Workflow%20Tools%20Installation/#deployment-usecase-workflow-tools-installation","text":"The below installation is required on the Build & Deployment system only and the Platform(Windows,Linux or MacOS) for Usecase Workflow Tool Installation Deployment Tools Installation Install Ansible on Build Machine pip3 install ansible == 2 .9.10 Install sshpass for ansible to connect to remote hosts using SSH apt-get install sshpass Create directory for ansible default configuration and hosts file mkdir -p /etc/ansible/ touch /etc/ansible/ansible.cfg Copy the ansible.cfg contents from https://raw.githubusercontent.com/ansible/ansible/v2.9.10/examples/ansible.cfg and paste it under /etc/ansible/ansible.cfg","title":"Deployment &amp; Usecase Workflow Tools Installation"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/7Deployment%20%26%20Usecase%20Workflow%20Tools%20Installation/#usecases-workflow-tools-installation","text":"Postman client should be downloaded on supported platforms or on the web to get started with the usecase collections. Note The Postman API Network will always have the latest released version of the API Collections. For all releases, refer the github repository for API Collections","title":"Usecases Workflow Tools Installation"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/","text":"Deployment Deployment Using Ansible The below details would enable the deployment through Ansible Role for Intel\u00ae SecL-DC Secure Key Caching Usecase. However the services can still be installed manually using the Product Guide. More details on Ansible Role for Intel\u00ae SecL-DC in Ansible-Role repository. Download the Ansible Role The role can be cloned locally from git and the contents can be copied to the roles folder used by your ansible server #Create directory for using ansible deployment mkdir -p /root/intel-secl/deploy/ #Clone the repository cd /root/intel-secl/deploy/ && git clone https://github.com/intel-secl/utils.git #Checkout to specific release version cd utils/ git checkout <release-version of choice> cd tools/ansible-role #Update ansible.cfg roles_path to point to path(/root/intel-secl/deploy/utils/tools/ansible-role/roles/) Update Ansible Inventory The following inventory can be used and created under /etc/ansible/hosts [CSP] <machine1_ip/hostname> [Enterprise] <machine2_ip/hostname> [Node] <machine3_ip/hostname> [CSP:vars] isecl_role=csp ansible_user=<ubuntu_user> ansible_sudo_pass=<password> ansible_password=<password> [Enterprise:vars] isecl_role=enterprise ansible_user=<ubuntu_user> ansible_sudo_pass=<password> ansible_password=<password> [Node:vars] isecl_role=node ansible_user=<ubuntu_user> ansible_sudo_pass=<password> ansible_password=<password> Note Ansible requires ssh and root user access to remote machines. The following command can be used to ensure ansible can connect to remote machines with host key check ` ssh-keyscan -H <ip_address> >> /root/.ssh/known_hosts Create and Run Playbook The following are playbook and CLI example for deploying Intel\u00ae SecL-DC binaries based on the supported deployment models and usecases. The below example playbooks can be created as site-bin-isecl.yml Note If running behind a proxy, update the proxy variables under <path to ansible role>/ansible-role/vars/main.yml and run as below Note Go through the Additional Examples and Tips section for specific workflow samples Option 1 Update the PCS Server key with following vars in <path to ansible role>/ansible-role/defaults/main.yml intel_provisioning_server_api_key_sandbox : <pcs server key> Create playbook with following contents - hosts : all remote_user : root become : yes become_user : root gather_facts : yes any_errors_fatal : true vars : setup : <setup var from supported usecases> binaries_path : <path where built binaries are copied to> backend_pykmip : \"<yes/no to install pykmip server along with KMIP KBS>\" roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> OR Option 2: Create playbook with following contents - hosts : all remote_user : root become : yes become_user : root gather_facts : yes any_errors_fatal : true roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> --extra-vars setup = <setup var from supported usecases> --extra-vars binaries_path = <path where built binaries are copied to> --extra-vars intel_provisioning_server_api_key = <pcs server key> --extra-vars backend_pykmip = yes Note If any service installation fails due to any misconfiguration, just uninstall the specific service manually , fix the misconfiguration in ansible and rerun the playbook. The successfully installed services won't be reinstalled. Usecase Setup Options Usecase Variable Secure Key Caching setup: secure-key-caching in playbook or via --extra-vars as setup=secure-key-caching in CLI SGX Orchestration Kubernetes setup: sgx-orchestration-kubernetes in playbook or via --extra-vars as setup=sgx-orchestration-kubernetes in CLI SKC No Orchestration setup: skc-no-orchestration in playbook or via --extra-vars as setup=skc-no-orchestration in CLI SGX Attestation Kubernetes setup: sgx-attestation-kubernetes in playbook or via --extra-vars as setup=sgx-attestation-kubernetes in CLI SGX Attestation No Orchestration setup: sgx-attestation-no-orchestration in playbook or via --extra-vars as setup=sgx-attestation-no-orchestration in CLI Note Orchestrator installation is not bundled with the role and need to be done independently. Also, components dependent on the orchestrator like isecl-k8s-extensions and integration-hub are installed either partially or not installed Deployment Using Binaries Setup K8S Cluster and Deploy Isecl-k8s-extensions Setup master and worker node for k8s. Worker node should be setup on SGX enabled host machine. Master node can be any system. To setup k8 cluster follow https://phoenixnap.com/kb/install-kubernetes-on-ubuntu Once the master/worker setup is done, follow below steps on Master Node: Untar packages and push OCI images to registry Copy tar output isecl-k8s-extensions-*.tar.gz from build system's binaries folder to /opt/ directory on the Master Node and extract the contents. cd /opt/ tar -xvzf isecl-k8s-extensions-*.tar.gz cd isecl-k8s-extensions/ Configure private registry Push images to private registry using skopeo command, (this can be done from build vm also) skopeo copy oci-archive:isecl-k8s-controller-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-controller:v4.0.0 skopeo copy oci-archive:isecl-k8s-scheduler-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-scheduler:v4.0.0 Add the image names in isecl-controller.yml and isecl-scheduler.yml in /opt/isecl-k8s-extensions/yamls with full image name including registry IP/hostname (e.g : /isecl-k8s-scheduler:v4.0.0). It will automatically pull the images from registry. Deploy isecl-controller Create hostattributes.crd.isecl.intel.com crd kubectl apply -f yamls/crd-1.17.yaml Check whether the crd is created kubectl get crds Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterrolebinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole=system:node --user=system:serviceaccount:isecl:isecl Fetch token required for ihub installation and follow below IHUB installation steps, kubectl get secrets -n isecl kubectl describe secret default-token-<name> -n isecl For IHUB installation, make sure to update below configuration in /root/binaries/env/ihub.env before installing ihub on CSP system: * Copy /etc/kubernetes/pki/apiserver.crt from master node to /root on CSP system. Update KUBERNETES_CERT_FILE. * Get k8s token in master, using above commands and update KUBERNETES_TOKEN * Update the value of CRD name KUBERNETES_CRD=custom-isecl-sgx Deploy isecl-scheduler The isecl-scheduler default configuration is provided for common cluster support in /opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml. Variables HVS_IHUB_PUBLIC_KEY_PATH and SGX_IHUB_PUBLIC_KEY_PATH are by default set to default paths. Please use and set only required variables based on the use case. For example, if only sgx based attestation is required then remove/comment HVS_IHUB_PUBLIC_KEY_PATH variables. Install cfssl and cfssljson on Kubernetes Control Plane #Download cfssl to /usr/local/bin/ wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl #Download cfssljson to /usr/local/bin wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create tls key pair for isecl-scheduler service, which is signed by k8s apiserver.crt cd /opt/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \"<K8_MASTER_IP>\",\"<K8_MASTER_HOST>\" -c /etc/kubernetes/pki/ca.crt -k /etc/kubernetes/pki/ca.key After iHub deployment, copy /etc/ihub/ihub_public_key.pem from ihub to /opt/isecl-k8s-extensions/ directory on k8 master system. Also, copy tls key pair generated in previous step to secrets directory. mkdir secrets cp /opt/isecl-k8s-extensions/server.key secrets/ cp /opt/isecl-k8s-extensions/server.crt secrets/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem cp /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem secrets/ Note Prefix the attestation type for ihub_public_key.pem before copying to secrets folder. Create kubernetes secrets scheduler-secret for isecl-scheduler kubectl create secret generic scheduler-certs --namespace isecl --from-file=secrets Deploy isecl-scheduler kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl Configure kube-scheduler to establish communication with isecl-scheduler Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec: containers: - command: - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml. Restart Kubelet which restart all the k8s services including kube base schedular systemctl restart kubelet Check if CRD data is populated kubectl get -o json hostattributes.crd.isecl.intel.com Deploying SKC Services on Single System Copy the binaries directory generated in the build system to the /root/ directory on the deployment system Update orchestrator.conf with the following - Deployment system IP address - SAN List (a list of ip address and hostname for the deployment system) - Network Port numbers for CMS, AAS, SCS and SHVS - Install Admin and CSP Admin credentials - TENANT as KUBERNETES - System IP address where Kubernetes deployed - Netowrk Port Number of Kubernetes - Database name, Database username and password for SHVS Update enterprise_skc.conf with the following - Deployment system IP address - SAN List (a list of ip address and hostname for the deployment system) - Network Port numbers for CMS, AAS, SCS, SQVS and KBS - Install Admin and CSP Admin credentials - Database name, Database username and password for AAS and SCS services - Intel PCS Server API URL and API Keys - Key Manager can be set to either Directory or KMIP - KMIP server configuration if KMIP is set Save and Close ./install_skc.sh In case installation fails, its recommended to run the following command to clear failed service instance systemctl reset-failed Deploy CSP SKC Services Copy the binaries directory generated in the build system system to the /root/ directory on the CSP system Update csp_skc.conf with the following - CSP system IP Address - SAN List (a list of ip address and hostname for the CSP system) - Network Port numbers for CMS, AAS, SCS and SHVS - Install Admin and CSP Admin credentials - TENANT as KUBERNETES - System IP address where Kubernetes is deployed - Netowrk Port Number of Kubernetes - Database name, Database username and password for AAS, SCS and SHVS services - Intel PCS Server API URL and API Keys Save and Close ./install_csp_skc.sh In case installation fails, its recommended to run the following command to clear failed service instance systemctl reset-failed Create sample yml file for nginx workload and add SGX labels to it such as: apiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: SGX-Enabled operator: In values: - \"true\" - key: EPC-Memory operator: In values: - \"2.0GB\" containers: - name: nginx image: nginx ports: - containerPort: 80 Validate if pod can be launched on the node. Run following commands: kubectl apply -f pod.yml kubectl get pods kubectl describe pods nginx Pod should be in running state and launched on the host as per values in pod.yml. Validate by running below command on sgx host: docker ps Deploy Enterprise SKC Services Copy the binaries directory generated in the build system to the /root/ directory on Enterprise system Update enterprise_skc.conf with the following - Enterprise system IP address - SAN List (a list of ip address and hostname for the Enterprise system) - Network Port numbers for CMS, AAS, SCS, SQVS and KBS - Install Admin credentials - Database name, Database username and passwords for AAS and SCS services - Intel PCS Server API URL and API Keys - Key Manager can be set to either Directory or KMIP - KMIP server configuration if KMIP is set Save and Close ./install_enterprise_skc.sh Deploy SGX Agent Copy sgx_agent.tar, sgx_agent.sha2 and agent_untar.sh from binaries directoy to a directory in SGX compute node ./agent_untar.sh Edit agent.conf with the following - CSP system IP address where CMS, AAS, SHVS and SCS services deployed - CSP Admin credentials (same which are provided in service configuration file. for ex: csp_skc.conf, orchestrator.conf or skc.conf) - Network Port numbers for CMS, AAS, SCS and SHVS - Token validity period in days - CMS TLS SHA Value (Run \"cms tlscertsha384\" on CSP system) Save and Close Note: In case orchestration support is not needed, please comment/delete SHVS_IP in agent.conf available in same folder ./deploy_sgx_agent.sh Deploy SKC Library Copy skc_library.tar, skc_library.sha2 and skclib_untar.sh from binaries directoy to a directory in SGX compute node ./skclib_untar.sh Update create_roles.conf with the following - IP address of AAS deployed on Enterprise system - Admin account credentials of AAS deployed on Enterprise system. These credentials should match with the AAS admin credentials provided in authservice.env on enterprise side. - Permission string to be embedded into skc_libraty client TLS Certificate - For Each SKC Library installation on a SGX compute node, please change SKC_USER and SKC_USER_PASSWORD Save and Close ./skc_library_create_roles.sh Copy the token printed on console. Update skc_library.conf with the following - IP address for CMS and KBS services deployed on Enterprise system - CSP_CMS_IP should point to the IP of CMS service deployed on CSP system - CSP_SCS_IP should point to the IP of SCS service deployed on CSP system - Hostname of the Enterprise system where KBS is deployed - Network Port numbers for CMS and SCS services deployed on CSP system - Network Port numbers for CMS and KBS services deployed on Enterprise system - For Each SKC Library installation on a SGX compute node, please change SKC_USER (should be same as SKC_USER provided in create_roles.conf) - SKC_TOKEN with the token copied from previous step Save and Close ./deploy_skc_library.sh","title":"Deployment"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#deployment","text":"","title":"Deployment"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#deployment-using-ansible","text":"The below details would enable the deployment through Ansible Role for Intel\u00ae SecL-DC Secure Key Caching Usecase. However the services can still be installed manually using the Product Guide. More details on Ansible Role for Intel\u00ae SecL-DC in Ansible-Role repository.","title":"Deployment Using Ansible"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#download-the-ansible-role","text":"The role can be cloned locally from git and the contents can be copied to the roles folder used by your ansible server #Create directory for using ansible deployment mkdir -p /root/intel-secl/deploy/ #Clone the repository cd /root/intel-secl/deploy/ && git clone https://github.com/intel-secl/utils.git #Checkout to specific release version cd utils/ git checkout <release-version of choice> cd tools/ansible-role #Update ansible.cfg roles_path to point to path(/root/intel-secl/deploy/utils/tools/ansible-role/roles/)","title":"Download the Ansible Role"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#update-ansible-inventory","text":"The following inventory can be used and created under /etc/ansible/hosts [CSP] <machine1_ip/hostname> [Enterprise] <machine2_ip/hostname> [Node] <machine3_ip/hostname> [CSP:vars] isecl_role=csp ansible_user=<ubuntu_user> ansible_sudo_pass=<password> ansible_password=<password> [Enterprise:vars] isecl_role=enterprise ansible_user=<ubuntu_user> ansible_sudo_pass=<password> ansible_password=<password> [Node:vars] isecl_role=node ansible_user=<ubuntu_user> ansible_sudo_pass=<password> ansible_password=<password> Note Ansible requires ssh and root user access to remote machines. The following command can be used to ensure ansible can connect to remote machines with host key check ` ssh-keyscan -H <ip_address> >> /root/.ssh/known_hosts","title":"Update Ansible Inventory"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#create-and-run-playbook","text":"The following are playbook and CLI example for deploying Intel\u00ae SecL-DC binaries based on the supported deployment models and usecases. The below example playbooks can be created as site-bin-isecl.yml Note If running behind a proxy, update the proxy variables under <path to ansible role>/ansible-role/vars/main.yml and run as below Note Go through the Additional Examples and Tips section for specific workflow samples Option 1 Update the PCS Server key with following vars in <path to ansible role>/ansible-role/defaults/main.yml intel_provisioning_server_api_key_sandbox : <pcs server key> Create playbook with following contents - hosts : all remote_user : root become : yes become_user : root gather_facts : yes any_errors_fatal : true vars : setup : <setup var from supported usecases> binaries_path : <path where built binaries are copied to> backend_pykmip : \"<yes/no to install pykmip server along with KMIP KBS>\" roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> OR Option 2: Create playbook with following contents - hosts : all remote_user : root become : yes become_user : root gather_facts : yes any_errors_fatal : true roles : - ansible-role environment : http_proxy : \"{{http_proxy}}\" https_proxy : \"{{https_proxy}}\" no_proxy : \"{{no_proxy}}\" and ansible-playbook <playbook-name> --extra-vars setup = <setup var from supported usecases> --extra-vars binaries_path = <path where built binaries are copied to> --extra-vars intel_provisioning_server_api_key = <pcs server key> --extra-vars backend_pykmip = yes Note If any service installation fails due to any misconfiguration, just uninstall the specific service manually , fix the misconfiguration in ansible and rerun the playbook. The successfully installed services won't be reinstalled.","title":"Create and Run Playbook"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#usecase-setup-options","text":"Usecase Variable Secure Key Caching setup: secure-key-caching in playbook or via --extra-vars as setup=secure-key-caching in CLI SGX Orchestration Kubernetes setup: sgx-orchestration-kubernetes in playbook or via --extra-vars as setup=sgx-orchestration-kubernetes in CLI SKC No Orchestration setup: skc-no-orchestration in playbook or via --extra-vars as setup=skc-no-orchestration in CLI SGX Attestation Kubernetes setup: sgx-attestation-kubernetes in playbook or via --extra-vars as setup=sgx-attestation-kubernetes in CLI SGX Attestation No Orchestration setup: sgx-attestation-no-orchestration in playbook or via --extra-vars as setup=sgx-attestation-no-orchestration in CLI Note Orchestrator installation is not bundled with the role and need to be done independently. Also, components dependent on the orchestrator like isecl-k8s-extensions and integration-hub are installed either partially or not installed","title":"Usecase Setup Options"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#deployment-using-binaries","text":"","title":"Deployment Using Binaries"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#setup-k8s-cluster-and-deploy-isecl-k8s-extensions","text":"Setup master and worker node for k8s. Worker node should be setup on SGX enabled host machine. Master node can be any system. To setup k8 cluster follow https://phoenixnap.com/kb/install-kubernetes-on-ubuntu Once the master/worker setup is done, follow below steps on Master Node:","title":"Setup K8S Cluster and Deploy Isecl-k8s-extensions"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#untar-packages-and-push-oci-images-to-registry","text":"Copy tar output isecl-k8s-extensions-*.tar.gz from build system's binaries folder to /opt/ directory on the Master Node and extract the contents. cd /opt/ tar -xvzf isecl-k8s-extensions-*.tar.gz cd isecl-k8s-extensions/ Configure private registry Push images to private registry using skopeo command, (this can be done from build vm also) skopeo copy oci-archive:isecl-k8s-controller-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-controller:v4.0.0 skopeo copy oci-archive:isecl-k8s-scheduler-v4.0.0-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-scheduler:v4.0.0 Add the image names in isecl-controller.yml and isecl-scheduler.yml in /opt/isecl-k8s-extensions/yamls with full image name including registry IP/hostname (e.g : /isecl-k8s-scheduler:v4.0.0). It will automatically pull the images from registry.","title":"Untar packages and push OCI images to registry"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#deploy-isecl-controller","text":"Create hostattributes.crd.isecl.intel.com crd kubectl apply -f yamls/crd-1.17.yaml Check whether the crd is created kubectl get crds Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterrolebinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole=system:node --user=system:serviceaccount:isecl:isecl Fetch token required for ihub installation and follow below IHUB installation steps, kubectl get secrets -n isecl kubectl describe secret default-token-<name> -n isecl For IHUB installation, make sure to update below configuration in /root/binaries/env/ihub.env before installing ihub on CSP system: * Copy /etc/kubernetes/pki/apiserver.crt from master node to /root on CSP system. Update KUBERNETES_CERT_FILE. * Get k8s token in master, using above commands and update KUBERNETES_TOKEN * Update the value of CRD name KUBERNETES_CRD=custom-isecl-sgx","title":"Deploy isecl-controller"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#deploy-isecl-scheduler","text":"The isecl-scheduler default configuration is provided for common cluster support in /opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml. Variables HVS_IHUB_PUBLIC_KEY_PATH and SGX_IHUB_PUBLIC_KEY_PATH are by default set to default paths. Please use and set only required variables based on the use case. For example, if only sgx based attestation is required then remove/comment HVS_IHUB_PUBLIC_KEY_PATH variables. Install cfssl and cfssljson on Kubernetes Control Plane #Download cfssl to /usr/local/bin/ wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl #Download cfssljson to /usr/local/bin wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create tls key pair for isecl-scheduler service, which is signed by k8s apiserver.crt cd /opt/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \"<K8_MASTER_IP>\",\"<K8_MASTER_HOST>\" -c /etc/kubernetes/pki/ca.crt -k /etc/kubernetes/pki/ca.key After iHub deployment, copy /etc/ihub/ihub_public_key.pem from ihub to /opt/isecl-k8s-extensions/ directory on k8 master system. Also, copy tls key pair generated in previous step to secrets directory. mkdir secrets cp /opt/isecl-k8s-extensions/server.key secrets/ cp /opt/isecl-k8s-extensions/server.crt secrets/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem cp /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem secrets/ Note Prefix the attestation type for ihub_public_key.pem before copying to secrets folder. Create kubernetes secrets scheduler-secret for isecl-scheduler kubectl create secret generic scheduler-certs --namespace isecl --from-file=secrets Deploy isecl-scheduler kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl","title":"Deploy isecl-scheduler"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#configure-kube-scheduler-to-establish-communication-with-isecl-scheduler","text":"Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec: containers: - command: - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml. Restart Kubelet which restart all the k8s services including kube base schedular systemctl restart kubelet Check if CRD data is populated kubectl get -o json hostattributes.crd.isecl.intel.com","title":"Configure kube-scheduler to establish communication with isecl-scheduler"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#deploying-skc-services-on-single-system","text":"Copy the binaries directory generated in the build system to the /root/ directory on the deployment system Update orchestrator.conf with the following - Deployment system IP address - SAN List (a list of ip address and hostname for the deployment system) - Network Port numbers for CMS, AAS, SCS and SHVS - Install Admin and CSP Admin credentials - TENANT as KUBERNETES - System IP address where Kubernetes deployed - Netowrk Port Number of Kubernetes - Database name, Database username and password for SHVS Update enterprise_skc.conf with the following - Deployment system IP address - SAN List (a list of ip address and hostname for the deployment system) - Network Port numbers for CMS, AAS, SCS, SQVS and KBS - Install Admin and CSP Admin credentials - Database name, Database username and password for AAS and SCS services - Intel PCS Server API URL and API Keys - Key Manager can be set to either Directory or KMIP - KMIP server configuration if KMIP is set Save and Close ./install_skc.sh In case installation fails, its recommended to run the following command to clear failed service instance systemctl reset-failed","title":"Deploying SKC Services on Single System"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#deploy-csp-skc-services","text":"Copy the binaries directory generated in the build system system to the /root/ directory on the CSP system Update csp_skc.conf with the following - CSP system IP Address - SAN List (a list of ip address and hostname for the CSP system) - Network Port numbers for CMS, AAS, SCS and SHVS - Install Admin and CSP Admin credentials - TENANT as KUBERNETES - System IP address where Kubernetes is deployed - Netowrk Port Number of Kubernetes - Database name, Database username and password for AAS, SCS and SHVS services - Intel PCS Server API URL and API Keys Save and Close ./install_csp_skc.sh In case installation fails, its recommended to run the following command to clear failed service instance systemctl reset-failed Create sample yml file for nginx workload and add SGX labels to it such as: apiVersion: v1 kind: Pod metadata: name: nginx labels: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: SGX-Enabled operator: In values: - \"true\" - key: EPC-Memory operator: In values: - \"2.0GB\" containers: - name: nginx image: nginx ports: - containerPort: 80 Validate if pod can be launched on the node. Run following commands: kubectl apply -f pod.yml kubectl get pods kubectl describe pods nginx Pod should be in running state and launched on the host as per values in pod.yml. Validate by running below command on sgx host: docker ps","title":"Deploy CSP SKC Services"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#deploy-enterprise-skc-services","text":"Copy the binaries directory generated in the build system to the /root/ directory on Enterprise system Update enterprise_skc.conf with the following - Enterprise system IP address - SAN List (a list of ip address and hostname for the Enterprise system) - Network Port numbers for CMS, AAS, SCS, SQVS and KBS - Install Admin credentials - Database name, Database username and passwords for AAS and SCS services - Intel PCS Server API URL and API Keys - Key Manager can be set to either Directory or KMIP - KMIP server configuration if KMIP is set Save and Close ./install_enterprise_skc.sh","title":"Deploy Enterprise SKC Services"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#deploy-sgx-agent","text":"Copy sgx_agent.tar, sgx_agent.sha2 and agent_untar.sh from binaries directoy to a directory in SGX compute node ./agent_untar.sh Edit agent.conf with the following - CSP system IP address where CMS, AAS, SHVS and SCS services deployed - CSP Admin credentials (same which are provided in service configuration file. for ex: csp_skc.conf, orchestrator.conf or skc.conf) - Network Port numbers for CMS, AAS, SCS and SHVS - Token validity period in days - CMS TLS SHA Value (Run \"cms tlscertsha384\" on CSP system) Save and Close Note: In case orchestration support is not needed, please comment/delete SHVS_IP in agent.conf available in same folder ./deploy_sgx_agent.sh","title":"Deploy SGX Agent"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/8Deployment/#deploy-skc-library","text":"Copy skc_library.tar, skc_library.sha2 and skclib_untar.sh from binaries directoy to a directory in SGX compute node ./skclib_untar.sh Update create_roles.conf with the following - IP address of AAS deployed on Enterprise system - Admin account credentials of AAS deployed on Enterprise system. These credentials should match with the AAS admin credentials provided in authservice.env on enterprise side. - Permission string to be embedded into skc_libraty client TLS Certificate - For Each SKC Library installation on a SGX compute node, please change SKC_USER and SKC_USER_PASSWORD Save and Close ./skc_library_create_roles.sh Copy the token printed on console. Update skc_library.conf with the following - IP address for CMS and KBS services deployed on Enterprise system - CSP_CMS_IP should point to the IP of CMS service deployed on CSP system - CSP_SCS_IP should point to the IP of SCS service deployed on CSP system - Hostname of the Enterprise system where KBS is deployed - Network Port numbers for CMS and SCS services deployed on CSP system - Network Port numbers for CMS and KBS services deployed on Enterprise system - For Each SKC Library installation on a SGX compute node, please change SKC_USER (should be same as SKC_USER provided in create_roles.conf) - SKC_TOKEN with the token copied from previous step Save and Close ./deploy_skc_library.sh","title":"Deploy SKC Library"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/9Usecase%20Workflows%20with%20Postman%20API%20Collection/","text":"Usecase Workflows with Postman API Collections The below allow to get started with workflows within Intel\u00ae SecL-DC for Foundational and Workload Security Usecases. More details available in API Collections repository Use Case Collections Use case Sub-Usecase API Collection Secure Key Caching - \u2714\ufe0f SGX Discovery, Provisioning and Orchestration - \u2714\ufe0f SGX Discovery and Provisioning - \u2714\ufe0f Download Postman API Collections Postman API Network for latest released collections: https://explore.postman.com/intelsecldc or Github repo for allreleases #Clone the github repo for api-collections git clone https://github.com/intel-secl/utils.git #Switch to specific release-version of choice cd utils/ git checkout <release-version of choice> #Import Collections from cd tools/api-collections Note The postman-collections are also available when cloning the repos via build manifest under utils/tools/api-collections Running API Collections Import the collection into Postman API Client Note This step is required only when not using Postman API Network and downloading from Github Update env as per the deployment details for specific usecase View Documentation Run the workflow","title":"Usecase Workflows with Postman API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/9Usecase%20Workflows%20with%20Postman%20API%20Collection/#usecase-workflows-with-postman-api-collections","text":"The below allow to get started with workflows within Intel\u00ae SecL-DC for Foundational and Workload Security Usecases. More details available in API Collections repository","title":"Usecase Workflows with Postman API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/9Usecase%20Workflows%20with%20Postman%20API%20Collection/#use-case-collections","text":"Use case Sub-Usecase API Collection Secure Key Caching - \u2714\ufe0f SGX Discovery, Provisioning and Orchestration - \u2714\ufe0f SGX Discovery and Provisioning - \u2714\ufe0f","title":"Use Case Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/9Usecase%20Workflows%20with%20Postman%20API%20Collection/#download-postman-api-collections","text":"Postman API Network for latest released collections: https://explore.postman.com/intelsecldc or Github repo for allreleases #Clone the github repo for api-collections git clone https://github.com/intel-secl/utils.git #Switch to specific release-version of choice cd utils/ git checkout <release-version of choice> #Import Collections from cd tools/api-collections Note The postman-collections are also available when cloning the repos via build manifest under utils/tools/api-collections","title":"Download Postman API Collections"},{"location":"quick-start-guides/SGX%20Infrastructure%20-%20Ubuntu/9Usecase%20Workflows%20with%20Postman%20API%20Collection/#running-api-collections","text":"Import the collection into Postman API Client Note This step is required only when not using Postman API Network and downloading from Github Update env as per the deployment details for specific usecase View Documentation Run the workflow","title":"Running API Collections"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/1Pre-requisites/","text":"Common Cluster Deployment Guide Common Cluster deployment is for deploying both Foundational Security and Secure Key Caching components in a common Kubernetes cluster. The worker nodes may be separate with TXT/BTG/SUEFI enabled on a specific worker node and SGX on a separate worker node or all Intel Security hardware features enabled on a single worker node. This guide goes over the deployment steps to enable Common Cluster for Foundational Security and Secure Key Caching Pre-requisites Binary Deployment Binary deployment would enable all components of Foundational Security and Secure Key Caching to be deployed as binary systemd services in a VM and Host for Agent systemd services. Refer the following links for the pre-requisites and requirements All Hardware, OS , Network, RPM's requirements are given the table below Use case Details Foundational & Workload Security Quick Start Guide Secure Key Caching Quick Start Guide All Build steps and pre-requisites are given in the table below Use case Details Foundational & Workload Security Quick Start Guide Secure Key Caching Quick Start Guide Containerized Deployment Containerized deployment would enable all components of Foundational Security and Secure Key Caching to be deployed as containers in a Kubernetes cluster. Refer the following links for the pre-requisites and requirements All Hardware, OS , Network, RPM's requirements are given the table below Use case Details Foundational & Workload Security Quick Start Guide Secure Key Caching Quick Start Guide All Build steps and pre-requisites are given in the table below Use case Details Foundational & Workload Security Quick Start Guide Secure Key Caching Quick Start Guide","title":"Common Cluster Deployment Guide"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/1Pre-requisites/#common-cluster-deployment-guide","text":"Common Cluster deployment is for deploying both Foundational Security and Secure Key Caching components in a common Kubernetes cluster. The worker nodes may be separate with TXT/BTG/SUEFI enabled on a specific worker node and SGX on a separate worker node or all Intel Security hardware features enabled on a single worker node. This guide goes over the deployment steps to enable Common Cluster for Foundational Security and Secure Key Caching","title":"Common Cluster Deployment Guide"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/1Pre-requisites/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/1Pre-requisites/#binary-deployment","text":"Binary deployment would enable all components of Foundational Security and Secure Key Caching to be deployed as binary systemd services in a VM and Host for Agent systemd services. Refer the following links for the pre-requisites and requirements All Hardware, OS , Network, RPM's requirements are given the table below Use case Details Foundational & Workload Security Quick Start Guide Secure Key Caching Quick Start Guide All Build steps and pre-requisites are given in the table below Use case Details Foundational & Workload Security Quick Start Guide Secure Key Caching Quick Start Guide","title":"Binary Deployment"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/1Pre-requisites/#containerized-deployment","text":"Containerized deployment would enable all components of Foundational Security and Secure Key Caching to be deployed as containers in a Kubernetes cluster. Refer the following links for the pre-requisites and requirements All Hardware, OS , Network, RPM's requirements are given the table below Use case Details Foundational & Workload Security Quick Start Guide Secure Key Caching Quick Start Guide All Build steps and pre-requisites are given in the table below Use case Details Foundational & Workload Security Quick Start Guide Secure Key Caching Quick Start Guide","title":"Containerized Deployment"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/2Deployment%20Architecture/","text":"Deployment Architecture Binary Deployment Containerized Deployment","title":"Deployment Architecture"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/2Deployment%20Architecture/#deployment-architecture","text":"","title":"Deployment Architecture"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/2Deployment%20Architecture/#binary-deployment","text":"","title":"Binary Deployment"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/2Deployment%20Architecture/#containerized-deployment","text":"","title":"Containerized Deployment"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/","text":"Deployment Steps Binary Deployment Pre-requisites Setup master and worker node for k8s. Worker node should be setup on SGX and SUEFI enabled host machine. Master node can be any system. To setup k8 cluster follow https://phoenixnap.com/kb/how-to-install-kubernetes-on-centos Follow \"SGX Attestation Infrastructure and Secure Key Caching (SKC) Quick Start Guide\" to build, setup and deploy all SKC services. Follow \"Foundational & Workload Security Quick Start Guide\" to build, setup and deploy all FS and WS services. Note Deployment of isecl-controller , ihub and isecl-scheduler will be manual. SKC services (CMS, AAS, SCS, SHVS) should be running on one VM, FS/WS services (CMS, AAS, HVS, WLS) should be running on other VM on CSP side. SGX-Agent, Trust-Agent and Workload-Agent should be running on worker node. Configure private registry Push images to private registry using skopeo command, (this can be done from build vm also) skopeo copy oci-archive:isecl-k8s-controller-<tag>-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-controller:<tag> skopeo copy oci-archive:isecl-k8s-scheduler-<tag>-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-scheduler:<tag> Deployment isecl-k8s-controller and isecl-k8s-scheduler will deploy on Master Node and ihub will deploy on any CSP side VM Copy tar output isecl-k8s-extensions-*.tar.gz from build system's binaries folder to /opt/ directory on the Master Node and extract the contents. cd /opt/ tar -xvzf isecl-k8s-extensions-*.tar.gz cd isecl-k8s-extensions/ Add the image names in isecl-controller.yml and isecl-scheduler.yml in /opt/isecl-k8s-extensions/yamls with full image name including registry IP/hostname (e.g :/isecl-k8s-scheduler:v3.6.0). Deploy isecl-controller Create hostattributes.crd.isecl.intel.com crd kubectl apply -f yamls/crd-1.17.yaml Check whether the crd is created kubectl get crds Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterrolebinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole=system:node --user=system:serviceaccount:isecl:isecl Fetch k8s token required for ihub installation and follow below IHUB installation steps, kubectl get secrets -n isecl kubectl describe secret default-token-<name> -n isecl Deploy ihub For iHub installation, make sure to update below configuration in /root/ihub.env before installing ihub on ihub VM. Copy /etc/kubernetes/pki/apiserver.crt from master node to /root on ihub systems. Update KUBERNETES_CERT_FILE . Get k8s token in master obtained by running kubectl describe secret default-token-<name> -n isecl command and update KUBERNETES_TOKEN Update KUBERNETES_URL with Master node IP and k8s port (default port 6443) When using 2 different instances of ihub on 2 different VMs: Update the value of CRD name in /root/ihub.env KUBERNETES_CRD=custom-isecl-sgx (On SGX ihub) KUBERNETES_CRD=custom-isecl-hvs (On FS ihub) Update HVS and SHVS url in /root/ihub.env SHVS_BASE_URL=https://<SGX SHVS IP>:13000/sgx-hvs/v2 (On SGX ihub) HVS_BASE_URL=https://<FS HVS IP>:8443/sgx-hvs/v2 (On FS ihub) Deploy ihub for both SGX and FS After iHub deployment, copy /etc/ihub/ihub_public_key.pem from both ihub systems to /opt/isecl-k8s-extensions/ directory on k8 master system. Prefix sgx_ihub_public_key.pem for SGX ihub key and prefix hvs_ihub_public_key.pem for FS ihub key scp <user>@<SGX iHub host>:/etc/ihub/ihub_public_key.pem /opt/isecl-k8s-extensions/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem scp <user>@<FS iHub host>:/etc/ihub/ihub_public_key.pem /opt/isecl-k8s-extensions/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/hvs_ihub_public_key.pem When using 2 different instances of ihub on same VM: Update the below values in /root/ihub.env and deploy ihub instance for SKC. KUBERNETES_CRD=custom-isecl-sgx SHVS_BASE_URL=https://<SGX SHVS IP>:13000/sgx-hvs/v2 INSTANCE_NAME=ihub-sgx Update the below values in /root/ihub.env and deploy ihub instance for FS. KUBERNETES_CRD=custom-isecl-hvs HVS_BASE_URL=https://<FS HVS IP>:8443/sgx-hvs/v2 INSTANCE_NAME=ihub-hvs After iHub deployment, copy both ihub public keys /etc/ihub-sgx/ihub_public_key.pem and /etc/ihub-sgx/ihub_public_key.pem from ihub system to /opt/isecl-k8s-extensions/ directory on k8 master system. Prefix sgx_ihub_public_key.pem for SGX ihub key and prefix hvs_ihub_public_key.pem for FS ihub key scp <user>@<SGX iHub host>:/etc/ihub-sgx/ihub_public_key.pem /opt/isecl-k8s-extensions/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem scp <user>@<FS iHub host>:/etc/ihub-hvs/ihub_public_key.pem /opt/isecl-k8s-extensions/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/hvs_ihub_public_key.pem When using common instance of ihub on same VM: Update the below values in /root/ihub.env and deploy common ihub instance. KUBERNETES_CRD=custom-isecl SHVS_BASE_URL=https://<SGX SHVS IP>:13000/sgx-hvs/v2 HVS_BASE_URL=https://<FS HVS IP>:8443/sgx-hvs/v2 INSTANCE_NAME=ihub After iHub deployment, copy ihub public keys /etc/ihub/ihub_public_key.pem from ihub system to /opt/isecl-k8s-extensions/ directory on k8 master system. Create 2 copy of same ihub public key with prefix sgx_ihub_public_key.pem for SGX and hvs_ihub_public_key.pem for FS. scp <user>@<iHub host>:/etc/ihub/ihub_public_key.pem /opt/isecl-k8s-extensions/ cp /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/hvs_ihub_public_key.pem Deploy isecl-scheduler The isecl-scheduler default configuration is provided for common cluster support in /opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml. Variables HVS_IHUB_PUBLIC_KEY_PATH and SGX_IHUB_PUBLIC_KEY_PATH are by default set to default paths. Install cfssl and cfssljson on Kubernetes Control Plane wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create TLS key pair for isecl-scheduler service, which is signed by k8s apiserver.crt cd /opt/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \"<K8_MASTER_IP>\",\"<K8_MASTER_HOST>\" -c /etc/kubernetes/pki/ca.crt -k /etc/kubernetes/pki/ca.key Copy TLS key pair generated in previous step to secrets directory. mkdir secrets cp /opt/isecl-k8s-extensions/server.key secrets/ cp /opt/isecl-k8s-extensions/server.crt secrets/ mv /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem secrets/ mv /opt/isecl-k8s-extensions/hvs_ihub_public_key.pem secrets/ Create kubernetes secrets scheduler-secret for isecl-scheduler kubectl create secret generic scheduler-certs --namespace isecl --from-file=secrets Deploy isecl-scheduler kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl Configure kube-scheduler to establish communication with isecl-scheduler Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec: containers: - command: - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml. Restart Kubelet which restart all the k8s services including kube base schedular systemctl restart kubelet Check if CRD data is populated kubectl get -o json hostattributes.crd.isecl.intel.com kubectl get nodes --show-labels Containerized Deployment Pre-requisites SGX and SUEFI both HW features should be enabled on Worker node. The K8s cluster admin configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: SGX-SUEFI-ENABLED can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. #Label node , below example is for SGX & SUEFI enabled host kubectl label node <node-name> node.type = SGX-SUEFI-ENABLED #Label node , below example is for SGX & TXT enabled host kubectl label node <node-name> node.type = SGX-TXT-ENABLED Based on the worker node as given the deployment architecture diagram, the respective agent daemonsets should be updated with the node-affinity as per the labels put on the worker nodes User needs to create on NFS volumes manually or using custom scripts for both use cases inside same directory /<NFS-vol-base-path>/isecl/ Note Deployment of isecl-controller , ihub and isecl-scheduler will be manual. SKC Deployment Ensure to have all the container images pushed to the registry of choice. Install pre-reqs using platform-dependencies on respective SGX enabled nodes Copy manifests to K8s-Controller node to /root/skc Update isecl-skc-k8s.env Run the following steps for deployment # Run pre-requisites ./pre-requisites.sh # Bootstrap DB pods ./isecl-bootstrap-db-services.sh up # Deploy SKC Services and Agents ./isecl-bootstrap.sh up secure-key-caching FS,WS Deployment Ensure to have all the container images pushed to the registry of choice. Install pre-reqs using platform-dependencies on respective TXT/SUEFI enabled nodes for Foundational Security use case Install pre-reqs for container-runtime on respective nodes as applicable for Workload Security use case Copy manifests to K8s-Controller node to /root/fs Update isecl-k8s.env with relevant data as applicable Run the following steps for deployment # Run pre-requisites ./pre-requisites.sh # Bootstrap DB pods ./isecl-bootstrap-db-services.sh up # Deploy Foundational Security Services and Agents ./isecl-bootstrap.sh up foundation-security Integration Hub and ISecL-K8s-Extensions Deployment ISecL-K8s-Extensions Custom-Controller Run the following to deploy isecl-controller from manifests folder under /root/fs ./isecl-bootstrap.sh up isecl-controller Integration Hub Run the following to deploy ihub from manifests folder under /root/fs pointing to HVS only ./isecl-bootstrap.sh up ihub Run the following for pointing IHUB to both HVS and SHVS : Open configMap.yml under /root/fs/manifests/ihub/configMap.yml Add setup task SHVS_BASE_URL: https://shvs-svc.isecl.svc.cluster.local:13000/sgx-hvs/v2 in the same configMap inside data section Add SETUP_TASK: \"attestation-service-connection\" inside data section Provide unique name to new configMap inside metadata section Provide the same name in the deployment.yml under configMapRef section Deploy the ihub service again with kubectl kustomize . | kubectl apply -f - ISecL-K8s-Extensions Extended-Scheduler Run the following to deploy isecl-scheduler from manifests folder under /root/fs Copy ihub public key from NFS to K8s Controller and update isecl-k8s.env for IHUB_PUB_KEY_PATH Run below commands: # Set environment variables export K8S_MASTER_IP = <k8s Master IP> export K8S_MASTER_HOSTNAME = <k8s Master Hostname> export K8S_CA_KEY = /etc/kubernetes/pki/ca.key export K8S_CA_CERT = /etc/kubernetes/pki/ca.crt # Create Extended Scheduler Certs cd k8s/manifests/k8s-extensions-scheduler/ chmod +x scripts/create_k8s_extsched_certs.sh cd scripts && echo ./create_k8s_extsched_certs.sh -n \"K8S Extended Scheduler\" -s \" $K8S_MASTER_IP \" , \" $K8S_MASTER_HOSTNAME \" -c \" $K8S_CA_CERT \" -k \" $K8S_CA_KEY \" ./create_k8s_extsched_certs.sh -n \"K8S Extended Scheduler\" -s \" $K8S_MASTER_IP \" , \" $K8S_MASTER_HOSTNAME \" -c \" $K8S_CA_CERT \" -k \" $K8S_CA_KEY \" # Created Extended Scheduler secrets cd .. mkdir -p secrets cp scripts/server.key secrets/ cp scripts/server.crt secrets/ cp /<IHUB_PUB_KEY_PATH>/ihub_public_key.pem secrets/hvs_ihub_public_key.pem cp /<IHUB_PUB_KEY_PATH>/ihub_public_key.pem secrets/sgx_ihub_public_key.pem Update SGX_IHUB_PUBLIC_KEY_PATH and HVS_IHUB_PUBLIC_KEY_PATH inside k8s/manifests/k8s-extensions-scheduler/isecl-scheduler.yml as mentioned below: - name : SGX_IHUB_PUBLIC_KEY_PATH value : /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem - name : HVS_IHUB_PUBLIC_KEY_PATH value : /opt/isecl-k8s-extensions/hvs_ihub_public_key.pem Create Secret kubectl create secret generic scheduler-certs --namespace isecl --from-file = secrets Deploy isecl-scheduler kubectl kustomize . | kubectl apply -f - Create and update scheduler-policy.json path mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions Configure kube-scheduler to establish communication with isecl-scheduler. Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec : containers : - command : - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers : volumeMounts : - mountPath : /opt/isecl-k8s-extensions/ name : extendedsched readOnly : true volumes : - hostPath : path : /opt/isecl-k8s-extensions/ type : name : extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml Restart kubelet which restart all the k8s services including kube-scheduler systemctl restart kubelet","title":"Deployment Steps"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#deployment-steps","text":"","title":"Deployment Steps"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#binary-deployment","text":"","title":"Binary Deployment"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#pre-requisites","text":"Setup master and worker node for k8s. Worker node should be setup on SGX and SUEFI enabled host machine. Master node can be any system. To setup k8 cluster follow https://phoenixnap.com/kb/how-to-install-kubernetes-on-centos Follow \"SGX Attestation Infrastructure and Secure Key Caching (SKC) Quick Start Guide\" to build, setup and deploy all SKC services. Follow \"Foundational & Workload Security Quick Start Guide\" to build, setup and deploy all FS and WS services. Note Deployment of isecl-controller , ihub and isecl-scheduler will be manual. SKC services (CMS, AAS, SCS, SHVS) should be running on one VM, FS/WS services (CMS, AAS, HVS, WLS) should be running on other VM on CSP side. SGX-Agent, Trust-Agent and Workload-Agent should be running on worker node. Configure private registry Push images to private registry using skopeo command, (this can be done from build vm also) skopeo copy oci-archive:isecl-k8s-controller-<tag>-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-controller:<tag> skopeo copy oci-archive:isecl-k8s-scheduler-<tag>-<commitid>.tar docker://<registryIP>:<registryPort>/isecl-k8s-scheduler:<tag>","title":"Pre-requisites"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#deployment","text":"isecl-k8s-controller and isecl-k8s-scheduler will deploy on Master Node and ihub will deploy on any CSP side VM Copy tar output isecl-k8s-extensions-*.tar.gz from build system's binaries folder to /opt/ directory on the Master Node and extract the contents. cd /opt/ tar -xvzf isecl-k8s-extensions-*.tar.gz cd isecl-k8s-extensions/ Add the image names in isecl-controller.yml and isecl-scheduler.yml in /opt/isecl-k8s-extensions/yamls with full image name including registry IP/hostname (e.g :/isecl-k8s-scheduler:v3.6.0).","title":"Deployment"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#deploy-isecl-controller","text":"Create hostattributes.crd.isecl.intel.com crd kubectl apply -f yamls/crd-1.17.yaml Check whether the crd is created kubectl get crds Deploy isecl-controller kubectl apply -f yamls/isecl-controller.yaml Check whether the isecl-controller is up and running kubectl get deploy -n isecl Create clusterrolebinding for ihub to get access to cluster nodes kubectl create clusterrolebinding isecl-clusterrole --clusterrole=system:node --user=system:serviceaccount:isecl:isecl Fetch k8s token required for ihub installation and follow below IHUB installation steps, kubectl get secrets -n isecl kubectl describe secret default-token-<name> -n isecl","title":"Deploy isecl-controller"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#deploy-ihub","text":"For iHub installation, make sure to update below configuration in /root/ihub.env before installing ihub on ihub VM. Copy /etc/kubernetes/pki/apiserver.crt from master node to /root on ihub systems. Update KUBERNETES_CERT_FILE . Get k8s token in master obtained by running kubectl describe secret default-token-<name> -n isecl command and update KUBERNETES_TOKEN Update KUBERNETES_URL with Master node IP and k8s port (default port 6443) When using 2 different instances of ihub on 2 different VMs: Update the value of CRD name in /root/ihub.env KUBERNETES_CRD=custom-isecl-sgx (On SGX ihub) KUBERNETES_CRD=custom-isecl-hvs (On FS ihub) Update HVS and SHVS url in /root/ihub.env SHVS_BASE_URL=https://<SGX SHVS IP>:13000/sgx-hvs/v2 (On SGX ihub) HVS_BASE_URL=https://<FS HVS IP>:8443/sgx-hvs/v2 (On FS ihub) Deploy ihub for both SGX and FS After iHub deployment, copy /etc/ihub/ihub_public_key.pem from both ihub systems to /opt/isecl-k8s-extensions/ directory on k8 master system. Prefix sgx_ihub_public_key.pem for SGX ihub key and prefix hvs_ihub_public_key.pem for FS ihub key scp <user>@<SGX iHub host>:/etc/ihub/ihub_public_key.pem /opt/isecl-k8s-extensions/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem scp <user>@<FS iHub host>:/etc/ihub/ihub_public_key.pem /opt/isecl-k8s-extensions/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/hvs_ihub_public_key.pem When using 2 different instances of ihub on same VM: Update the below values in /root/ihub.env and deploy ihub instance for SKC. KUBERNETES_CRD=custom-isecl-sgx SHVS_BASE_URL=https://<SGX SHVS IP>:13000/sgx-hvs/v2 INSTANCE_NAME=ihub-sgx Update the below values in /root/ihub.env and deploy ihub instance for FS. KUBERNETES_CRD=custom-isecl-hvs HVS_BASE_URL=https://<FS HVS IP>:8443/sgx-hvs/v2 INSTANCE_NAME=ihub-hvs After iHub deployment, copy both ihub public keys /etc/ihub-sgx/ihub_public_key.pem and /etc/ihub-sgx/ihub_public_key.pem from ihub system to /opt/isecl-k8s-extensions/ directory on k8 master system. Prefix sgx_ihub_public_key.pem for SGX ihub key and prefix hvs_ihub_public_key.pem for FS ihub key scp <user>@<SGX iHub host>:/etc/ihub-sgx/ihub_public_key.pem /opt/isecl-k8s-extensions/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem scp <user>@<FS iHub host>:/etc/ihub-hvs/ihub_public_key.pem /opt/isecl-k8s-extensions/ mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/hvs_ihub_public_key.pem When using common instance of ihub on same VM: Update the below values in /root/ihub.env and deploy common ihub instance. KUBERNETES_CRD=custom-isecl SHVS_BASE_URL=https://<SGX SHVS IP>:13000/sgx-hvs/v2 HVS_BASE_URL=https://<FS HVS IP>:8443/sgx-hvs/v2 INSTANCE_NAME=ihub After iHub deployment, copy ihub public keys /etc/ihub/ihub_public_key.pem from ihub system to /opt/isecl-k8s-extensions/ directory on k8 master system. Create 2 copy of same ihub public key with prefix sgx_ihub_public_key.pem for SGX and hvs_ihub_public_key.pem for FS. scp <user>@<iHub host>:/etc/ihub/ihub_public_key.pem /opt/isecl-k8s-extensions/ cp /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem mv /opt/isecl-k8s-extensions/ihub_public_key.pem /opt/isecl-k8s-extensions/hvs_ihub_public_key.pem","title":"Deploy ihub"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#deploy-isecl-scheduler","text":"The isecl-scheduler default configuration is provided for common cluster support in /opt/isecl-k8s-extensions/yamls/isecl-scheduler.yaml. Variables HVS_IHUB_PUBLIC_KEY_PATH and SGX_IHUB_PUBLIC_KEY_PATH are by default set to default paths. Install cfssl and cfssljson on Kubernetes Control Plane wget -O /usr/local/bin/cfssl http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 chmod +x /usr/local/bin/cfssl wget -O /usr/local/bin/cfssljson http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x /usr/local/bin/cfssljson Create TLS key pair for isecl-scheduler service, which is signed by k8s apiserver.crt cd /opt/isecl-k8s-extensions/ chmod +x create_k8s_extsched_cert.sh ./create_k8s_extsched_cert.sh -n \"K8S Extended Scheduler\" -s \"<K8_MASTER_IP>\",\"<K8_MASTER_HOST>\" -c /etc/kubernetes/pki/ca.crt -k /etc/kubernetes/pki/ca.key Copy TLS key pair generated in previous step to secrets directory. mkdir secrets cp /opt/isecl-k8s-extensions/server.key secrets/ cp /opt/isecl-k8s-extensions/server.crt secrets/ mv /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem secrets/ mv /opt/isecl-k8s-extensions/hvs_ihub_public_key.pem secrets/ Create kubernetes secrets scheduler-secret for isecl-scheduler kubectl create secret generic scheduler-certs --namespace isecl --from-file=secrets Deploy isecl-scheduler kubectl apply -f yamls/isecl-scheduler.yaml Check whether the isecl-scheduler is up and running kubectl get deploy -n isecl","title":"Deploy isecl-scheduler"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#configure-kube-scheduler-to-establish-communication-with-isecl-scheduler","text":"Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec: containers: - command: - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers: volumeMounts: - mountPath: /opt/isecl-k8s-extensions/ name: extendedsched readOnly: true volumes: - hostPath: path: /opt/isecl-k8s-extensions/ type: name: extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml. Restart Kubelet which restart all the k8s services including kube base schedular systemctl restart kubelet Check if CRD data is populated kubectl get -o json hostattributes.crd.isecl.intel.com kubectl get nodes --show-labels","title":"Configure kube-scheduler to establish communication with isecl-scheduler"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#containerized-deployment","text":"","title":"Containerized Deployment"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#pre-requisites_1","text":"SGX and SUEFI both HW features should be enabled on Worker node. The K8s cluster admin configure the existing bare metal worker nodes or register fresh bare metal worker nodes with labels. For example, a label like node.type: SGX-SUEFI-ENABLED can be used by the cluster admin to distinguish the baremetal worker node and the same label can be used in ISECL Agent pod configuration to schedule on all worker nodes marked with the label. #Label node , below example is for SGX & SUEFI enabled host kubectl label node <node-name> node.type = SGX-SUEFI-ENABLED #Label node , below example is for SGX & TXT enabled host kubectl label node <node-name> node.type = SGX-TXT-ENABLED Based on the worker node as given the deployment architecture diagram, the respective agent daemonsets should be updated with the node-affinity as per the labels put on the worker nodes User needs to create on NFS volumes manually or using custom scripts for both use cases inside same directory /<NFS-vol-base-path>/isecl/ Note Deployment of isecl-controller , ihub and isecl-scheduler will be manual.","title":"Pre-requisites"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#skc-deployment","text":"Ensure to have all the container images pushed to the registry of choice. Install pre-reqs using platform-dependencies on respective SGX enabled nodes Copy manifests to K8s-Controller node to /root/skc Update isecl-skc-k8s.env Run the following steps for deployment # Run pre-requisites ./pre-requisites.sh # Bootstrap DB pods ./isecl-bootstrap-db-services.sh up # Deploy SKC Services and Agents ./isecl-bootstrap.sh up secure-key-caching","title":"SKC Deployment"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#fsws-deployment","text":"Ensure to have all the container images pushed to the registry of choice. Install pre-reqs using platform-dependencies on respective TXT/SUEFI enabled nodes for Foundational Security use case Install pre-reqs for container-runtime on respective nodes as applicable for Workload Security use case Copy manifests to K8s-Controller node to /root/fs Update isecl-k8s.env with relevant data as applicable Run the following steps for deployment # Run pre-requisites ./pre-requisites.sh # Bootstrap DB pods ./isecl-bootstrap-db-services.sh up # Deploy Foundational Security Services and Agents ./isecl-bootstrap.sh up foundation-security","title":"FS,WS Deployment"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#integration-hub-and-isecl-k8s-extensions-deployment","text":"","title":"Integration Hub and ISecL-K8s-Extensions Deployment"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#isecl-k8s-extensions-custom-controller","text":"Run the following to deploy isecl-controller from manifests folder under /root/fs ./isecl-bootstrap.sh up isecl-controller","title":"ISecL-K8s-Extensions Custom-Controller"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#integration-hub","text":"Run the following to deploy ihub from manifests folder under /root/fs pointing to HVS only ./isecl-bootstrap.sh up ihub Run the following for pointing IHUB to both HVS and SHVS : Open configMap.yml under /root/fs/manifests/ihub/configMap.yml Add setup task SHVS_BASE_URL: https://shvs-svc.isecl.svc.cluster.local:13000/sgx-hvs/v2 in the same configMap inside data section Add SETUP_TASK: \"attestation-service-connection\" inside data section Provide unique name to new configMap inside metadata section Provide the same name in the deployment.yml under configMapRef section Deploy the ihub service again with kubectl kustomize . | kubectl apply -f -","title":"Integration Hub"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/3Deployment%20Steps/#isecl-k8s-extensions-extended-scheduler","text":"Run the following to deploy isecl-scheduler from manifests folder under /root/fs Copy ihub public key from NFS to K8s Controller and update isecl-k8s.env for IHUB_PUB_KEY_PATH Run below commands: # Set environment variables export K8S_MASTER_IP = <k8s Master IP> export K8S_MASTER_HOSTNAME = <k8s Master Hostname> export K8S_CA_KEY = /etc/kubernetes/pki/ca.key export K8S_CA_CERT = /etc/kubernetes/pki/ca.crt # Create Extended Scheduler Certs cd k8s/manifests/k8s-extensions-scheduler/ chmod +x scripts/create_k8s_extsched_certs.sh cd scripts && echo ./create_k8s_extsched_certs.sh -n \"K8S Extended Scheduler\" -s \" $K8S_MASTER_IP \" , \" $K8S_MASTER_HOSTNAME \" -c \" $K8S_CA_CERT \" -k \" $K8S_CA_KEY \" ./create_k8s_extsched_certs.sh -n \"K8S Extended Scheduler\" -s \" $K8S_MASTER_IP \" , \" $K8S_MASTER_HOSTNAME \" -c \" $K8S_CA_CERT \" -k \" $K8S_CA_KEY \" # Created Extended Scheduler secrets cd .. mkdir -p secrets cp scripts/server.key secrets/ cp scripts/server.crt secrets/ cp /<IHUB_PUB_KEY_PATH>/ihub_public_key.pem secrets/hvs_ihub_public_key.pem cp /<IHUB_PUB_KEY_PATH>/ihub_public_key.pem secrets/sgx_ihub_public_key.pem Update SGX_IHUB_PUBLIC_KEY_PATH and HVS_IHUB_PUBLIC_KEY_PATH inside k8s/manifests/k8s-extensions-scheduler/isecl-scheduler.yml as mentioned below: - name : SGX_IHUB_PUBLIC_KEY_PATH value : /opt/isecl-k8s-extensions/sgx_ihub_public_key.pem - name : HVS_IHUB_PUBLIC_KEY_PATH value : /opt/isecl-k8s-extensions/hvs_ihub_public_key.pem Create Secret kubectl create secret generic scheduler-certs --namespace isecl --from-file = secrets Deploy isecl-scheduler kubectl kustomize . | kubectl apply -f - Create and update scheduler-policy.json path mkdir -p /opt/isecl-k8s-extensions cp manifests/k8s-extensions-scheduler/config/scheduler-policy.json /opt/isecl-k8s-extensions Configure kube-scheduler to establish communication with isecl-scheduler. Add scheduler-policy.json under kube-scheduler section, mountPath under container section and hostPath under volumes section in /etc/kubernetes/manifests/kube-scheduler.yaml as mentioned below spec : containers : - command : - kube-scheduler - --policy-config-file=/opt/isecl-k8s-extensions/scheduler-policy.json containers : volumeMounts : - mountPath : /opt/isecl-k8s-extensions/ name : extendedsched readOnly : true volumes : - hostPath : path : /opt/isecl-k8s-extensions/ type : name : extendedsched Note Make sure to use proper indentation and don't delete existing mountPath and hostPath sections in kube-scheduler.yaml Restart kubelet which restart all the k8s services including kube-scheduler systemctl restart kubelet","title":"ISecL-K8s-Extensions Extended-Scheduler"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/4Workload%20POD%20Launch%20Validation/","text":"Workload POD Launch Validation Launch Workload for SGX Create below sample sgx_pod.yml file for nginx workload and add SGX labels as node affinity to it. apiVersion : v1 kind : Pod metadata : name : nginx-sgx labels : name : nginx-sgx spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : SGX-Enabled operator : In values : - \"true\" - key : EPC-Memory operator : In values : - \"2.0GB\" containers : - name : nginx image : nginx ports : - containerPort : 80 Launch the pod by running below command: kubectl apply -f sgx_pod.yml Check pod status using kubectl get pod -l name = nginx-sgx Launch Workload for FS Create below sample fs_pod.yml file for nginx workload and add trusted labels as node affinity to it. apiVersion : v1 kind : Pod metadata : name : nginx-fs labels : name : nginx-fs spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : isecl.trusted operator : In values : - \"true\" containers : - name : nginx image : nginx ports : - containerPort : 80 Launch the pod by running below command: kubectl apply -f fs_pod.yml Check pod status using kubectl get pod -l name = nginx-fs","title":"Workload POD Launch Validation"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/4Workload%20POD%20Launch%20Validation/#workload-pod-launch-validation","text":"","title":"Workload POD Launch Validation"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/4Workload%20POD%20Launch%20Validation/#launch-workload-for-sgx","text":"Create below sample sgx_pod.yml file for nginx workload and add SGX labels as node affinity to it. apiVersion : v1 kind : Pod metadata : name : nginx-sgx labels : name : nginx-sgx spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : SGX-Enabled operator : In values : - \"true\" - key : EPC-Memory operator : In values : - \"2.0GB\" containers : - name : nginx image : nginx ports : - containerPort : 80 Launch the pod by running below command: kubectl apply -f sgx_pod.yml Check pod status using kubectl get pod -l name = nginx-sgx","title":"Launch Workload for SGX"},{"location":"supplementary-guides/Common%20Cluster%20Deployment%20Guide/4Workload%20POD%20Launch%20Validation/#launch-workload-for-fs","text":"Create below sample fs_pod.yml file for nginx workload and add trusted labels as node affinity to it. apiVersion : v1 kind : Pod metadata : name : nginx-fs labels : name : nginx-fs spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : isecl.trusted operator : In values : - \"true\" containers : - name : nginx image : nginx ports : - containerPort : 80 Launch the pod by running below command: kubectl apply -f fs_pod.yml Check pod status using kubectl get pod -l name = nginx-fs","title":"Launch Workload for FS"}]}